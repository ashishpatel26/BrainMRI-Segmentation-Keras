{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MRI_Brain_ResNet.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"qBFduXpDIXz0","colab_type":"code","outputId":"71f465a8-72b8-44e3-c40f-7e1faf7a63c2","executionInfo":{"status":"ok","timestamp":1555931541978,"user_tz":-480,"elapsed":3641,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/My Drive/lib')\n","\n","import keras\n","from keras import layers\n","from keras import models\n","from keras import optimizers\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras import backend\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import math\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"C3LQjheZIqU_","colab_type":"code","colab":{}},"cell_type":"code","source":["dataset = np.load(\"drive/My Drive/MRI_Brain_Segmentation/Dataset_final_1.npy\")\n","x = np.load(\"drive/My Drive/MRI_Brain_Segmentation/Dataset_final_2.npy\") \n","dataset = np.concatenate((dataset,x),axis=0)\n","x = np.load(\"drive/My Drive/MRI_Brain_Segmentation/Dataset_final_3.npy\") \n","dataset = np.concatenate((dataset,x),axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"leu4R-g4KiBB","colab_type":"code","outputId":"8a396878-3a68-42a8-a7a7-78fd5d9e5395","executionInfo":{"status":"ok","timestamp":1555931610179,"user_tz":-480,"elapsed":71800,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}},"colab":{"base_uri":"https://localhost:8080/","height":796}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","import time\n","from numpy import array\n","import keras\n","import sys\n","import copy\n","import matplotlib.pyplot as plt\n","\n","# The two functions for normalization are defined.\n","def whitening(image):\n","    \"\"\"Whitening. Normalises image to zero mean and unit variance.\"\"\"\n","\n","    image = image.astype(np.float32)\n","\n","    mean = np.mean(image)\n","    std = np.std(image)\n","\n","    if std > 0:\n","        ret = (image - mean) / std\n","    else:\n","        ret = image * 0.\n","    return ret\n","\n","def normalize(image):\n","    #Normalize the image \n","    \n","    image = image.astype(np.float32)\n","    tmp = np.shape(image)\n","    print(tmp[0],tmp[1],tmp[2])\n","    #if image > 0:\n","    #  i_min = image.min()\n","    #  i_max = image.max()\n","    #  ret = (image - i_min) * 255/ (i_max - i_min)\n","    #else:\n","    #  ret = 0.     \n","      \n","  \n","    print(\"min \", np.min(image))\n","    print(\"max \", np.max(image))\n","    return image\n","  \n","  \n","def histeq(im,nbr_bins=256):\n","  \"\"\"This is for image equalization\"\"\"\n","  #get image histogram\n","  imhist,bins = np.histogram(im.flatten(),nbr_bins,normed=True)\n","   \n","  cdf = imhist.cumsum() #cumulative distribution function\n","  \n","  cdf_m = np.ma.masked_equal(cdf,0)#mask the background voxels \n","    \n","  # the main step of histogram equalization\n","  cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min()) \n","   \n","  cdf = np.ma.filled(cdf_m,0).astype('uint8') # set the removed background pixels back to 0\n","\n","  #use linear interpolation of cdf to find new pixel values\n","    \n","  # im2 = np.interp(im.flatten(),bins[:-1],cdf)/255 # this line can cause the program to fail\n","  # ############################################## because it returns a float64 and run out of RAM\n","  im2 = (np.interp(im.flatten(),bins[:-1],cdf)/255).astype(np.float32)\n","\n","  return im2.reshape(im.shape), cdf\n","\n","\n","\n","# We load in three .npy files for training. Each file contains 5 patients.\n","#\n","\"\"\"\n","dataset = np.load(\"drive/My Drive/MRI_Brain_Segmentation/Dataset_final_1.npy\")\n","x = np.load(\"drive/My Drive/MRI_Brain_Segmentation/Dataset_final_2.npy\") \n","dataset = np.concatenate((dataset,x),axis=0)\n","x = np.load(\"drive/My Drive/MRI_Brain_Segmentation/Dataset_final_3.npy\") \n","dataset = np.concatenate((dataset,x),axis=0)\n","\"\"\"\n","\n","# padding to all sides of the 3D volume\n","npad = ((43,43), (43,43), (43,43))\n","for i in range (0, len(dataset)):  \n","  dataset[i][0] = np.pad(dataset[i][0], pad_width=npad, mode='constant', constant_values=0)\n","  dataset[i][1] = np.pad(dataset[i][1], pad_width=npad, mode='constant', constant_values=0)\n","  #\n","  #\n","\n","  if i == 0: # Just for the first patient\n","    # plot histogram of the dataset raw values\n","    # Do not display the voxels at 0 for there are too many background voxels\n","    plt.hist(dataset[i][0].ravel(),256,[0.000001,np.amax(dataset[i][0])])\n","    plt.show()\n","    print(\"**** Display some sample values for reference: z=125; x=125; y=[125:140] \")\n","    print(\"raw: \",dataset[i][0][125][125][125:140])\n","    print(\"label: \",dataset[i][1][125][125][125:140])\n","    print(np.shape(dataset[i][0]))\n","    tmp_total = np.size(dataset[i][0])\n","    print(\"Total number of voxels in the Patient = \",tmp_total)\n","    # The following is to count the number of non-zero voxels in the raw values\n","    tmp_xyz=np.shape(dataset[i][0])\n","    countNZ = sum(sum(sum(dataset[i][0] != 0)))\n","    \"\"\"\n","    countNZ = 0\n","    for ii in range(0,tmp_xyz[0]):\n","      for jj in range(0,tmp_xyz[1]):\n","        for kk in range(0,tmp_xyz[2]):\n","          if dataset[i][0][ii][jj][kk] != 0:\n","            countNZ += 1\n","    \"\"\"\n","    print(\"Total number of non-zero voxels = \",countNZ)\n","    \n","    print(\"Next is Image Equalization\")\n","\n","  # Tp carry out image equalization\n","  dataset[i][0],_ = histeq(dataset[i][0])\n","\n","\n","  if i == 0: # Just for the first patient\n","    # Do not display the voxels at 0 for there are too many background voxels\n","    # Hence, the use of 0.000001\n","    plt.hist(dataset[i][0].ravel(),256,[0.000001,np.amax(dataset[i][0])])\n","    plt.show()\n","    print(\"equalize: \",dataset[i][0][125][125][125:140])\n","  #\n","    \n","\n","  # dataset[i][0],_ = histeq(dataset[i][0])\n","  #tmp_data = copy.deepcopy(dataset[i][0])\n","  #dataset[i][0]=image_Norm(tmp_data)\n","\n","\n","print(\"Number of patient (Length of dataset) = \",len(dataset) )\n"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsDAzs0L/Dw7BOBCmz3v0AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["**** Display some sample values for reference: z=125; x=125; y=[125:140] \n","raw:  [   0.       702.4829   827.6591   961.05835 1154.9955  1359.2179\n"," 1464.9822  1397.3713  1203.547    966.5984   772.7163   686.564\n","  597.32086  538.85724  510.1532 ]\n","label:  [  0. 125. 125. 125. 125.  14.  14.  14.  14.  97.  97.  97.  97.  97.\n","  97.]\n","(373, 342, 342)\n","Total number of voxels in the Patient =  43627572\n","Total number of non-zero voxels =  1543898\n","Next is Image Equalization\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6jAcJEkdhoMkqcNwkCR1/H8Pnz+ACGCwlwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["equalize:  [0.         0.07793374 0.1876256  0.36516523 0.5635231  0.70861965\n"," 0.8131826  0.74016535 0.6004598  0.37090978 0.1339547  0.06968046\n"," 0.02745098 0.01176471 0.00566974]\n","Number of patient (Length of dataset) =  15\n"],"name":"stdout"}]},{"metadata":{"id":"vNu5hJMlMNjc","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\n","\"\"\"\n","Residual Block\n","\"\"\"\n","\n","def buildResBlock(inputs, layer_type='cv',\n","                  filter1=16, filter2=16, \n","                  conv_size1=(3,3), conv_size2=(3,3), \n","                  stride1=(1,1), stride2=(1,1), stridec=(2,2),\n","                  input_size_cross=False\n","                  ):\n","  \n","  if layer_type == 'cv':\n","\n","    x = layers.BatchNormalization()(inputs)\n","    x = layers.Activation('relu')(x)\n","    x = layers.Conv2D(filter1, conv_size1, strides=stride1, padding='SAME')(x)\n","    \n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","    x = layers.Conv2D(filter2, conv_size2, strides=stride2, padding='SAME')(x)\n","    \n","    if input_size_cross:\n","      ex = layers.Conv2D(filter2, (1,1), strides=stridec, padding='SAME')(inputs)\n","      x = layers.add([x, ex])\n","    else:\n","      x = layers.add([x, inputs])\n","      \n","    return x\n","\n","  elif layer_type == 'cv3':\n","\n","    x = layers.BatchNormalization()(inputs)\n","    x = layers.Activation('relu')(x)\n","    x = layers.Conv3D(filter1, conv_size1, strides=stride1, padding='SAME')(x)\n","    \n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","    x = layers.Conv3D(filter2, conv_size2, strides=stride2, padding='SAME')(x)\n","    \n","    if input_size_cross:\n","      ex = layers.Conv3D(filter2, (1,1,1), strides=stridec, padding='SAME')(inputs)\n","      x = layers.add([x, ex])\n","    else:\n","      x = layers.add([x, inputs])\n","      \n","    return x\n","  \n","  else:\n","    print(\"Type Error\")\n","    return None\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gwFOAnc7cV_q","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.utils import plot_model\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w6Fh99yxea0A","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"H7PMpqj1M_3u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1394},"outputId":"0451a505-09d8-4fdb-e576-f14b87052a16","executionInfo":{"status":"ok","timestamp":1555943645276,"user_tz":-480,"elapsed":3933,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}}},"cell_type":"code","source":["def buildSingleResNetSmall():\n","  inputs = layers.Input(shape=(16,16,1))\n","  x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='SAME', activation='relu')(inputs)\n","  x = layers.MaxPooling2D((2, 2))(x)\n","  \n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=128, filter2=128, stridec=(1,1), input_size_cross=True)\n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=128, filter2=128)\n","  \n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=256, filter2=256, stride1=(2,2), input_size_cross=True)\n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=256, filter2=256)\n","\n","  #x = buildResBlock(inputs=x, layer_type='cv', filter1=256, filter2=256, stride1=(2,2), input_size_cross=True)\n","  #x = buildResBlock(inputs=x, layer_type='cv', filter1=256, filter2=256)\n","  \n","  x = layers.AveragePooling2D((2, 2))(x)\n","  \n","  x = layers.Flatten()(x)\n","  \n","  network = models.Model(inputs=inputs, outputs=x)\n","  return network\n","\n","  \n","\n","def buildSingleResNetLarge():\n","  inputs = layers.Input(shape=(87,87,1))\n","  x = layers.Conv2D(64, (7, 7), strides=(3, 3), padding='VALID', activation='relu')(inputs)\n","  x = layers.MaxPooling2D((2, 2))(x)\n","  \n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=128, filter2=128, stridec=(1,1), input_size_cross=True)\n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=128, filter2=128)\n","  \n","  x = layers.MaxPooling2D((2, 2))(x)\n","  \n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=256, filter2=256, stride1=(2,2), input_size_cross=True)\n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=256, filter2=256)\n","  \n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=256, filter2=256, stride1=(2,2), input_size_cross=True)\n","  x = buildResBlock(inputs=x, layer_type='cv', filter1=256, filter2=256)\n","  \n","  x = layers.AveragePooling2D((2, 2))(x)\n","  \n","  x = layers.Flatten()(x)\n","  \n","  network = models.Model(inputs=inputs, outputs=x)\n","  return network\n","\n","  \n","\n","def buildSingleResNetCube():\n","  \n","  inputs = layers.Input(shape=(26,26,26,1))\n","  \n","  x = layers.Conv3D(64, (3, 3, 3), strides=(2, 2, 2), padding='SAME', activation='relu')(inputs)\n","  \n","  x = layers.MaxPooling3D((2, 2, 2))(x)\n","  \n","  \n","  x = buildResBlock(inputs=x, layer_type='cv3', conv_size1=(3,3,3), conv_size2=(3,3,3), \n","                    filter1=128, filter2=128, stride1=(1,1,1), stride2=(1,1,1), stridec=(1,1,1), input_size_cross=True)\n","  \n","  x = buildResBlock(inputs=x, layer_type='cv3', conv_size1=(3,3,3), conv_size2=(3,3,3), \n","                    filter1=128, filter2=128, stride1=(1,1,1), stride2=(1,1,1))\n","\n","  \n","  x = buildResBlock(inputs=x, layer_type='cv3', conv_size1=(3,3,3), conv_size2=(3,3,3), \n","                    filter1=256, filter2=256, stride1=(2,2,2), stride2=(1,1,1), stridec=(2,2,2), input_size_cross=True)\n","  \n","  x = buildResBlock(inputs=x, layer_type='cv3', conv_size1=(2,2,2), conv_size2=(2,2,2), \n","                    filter1=256, filter2=256, stride1=(1,1,1), stride2=(1,1,1))\n","  \n","\n","  \n","  x = layers.AveragePooling3D((2, 2, 2))(x)\n","  \n","  x = layers.Flatten()(x)\n","  \n","  network = models.Model(inputs=inputs, outputs=x)\n","  \n","  return network\n","\n","  \n","\n","res_cube = buildSingleResNetCube()\n","res_cube.summary()\n","plot_model(res_cube, to_file='drive/My Drive/MRI_Brain_Segmentation/resnet_diagram.png',show_shapes=True)\n","\n","#res_small = buildSingleResNetSmall()\n","#res_small.summary()\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_12 (InputLayer)           (None, 26, 26, 26, 1 0                                            \n","__________________________________________________________________________________________________\n","conv3d_67 (Conv3D)              (None, 13, 13, 13, 6 1792        input_12[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling3d_7 (MaxPooling3D)  (None, 6, 6, 6, 64)  0           conv3d_67[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_49 (BatchNo (None, 6, 6, 6, 64)  256         max_pooling3d_7[0][0]            \n","__________________________________________________________________________________________________\n","activation_49 (Activation)      (None, 6, 6, 6, 64)  0           batch_normalization_49[0][0]     \n","__________________________________________________________________________________________________\n","conv3d_68 (Conv3D)              (None, 6, 6, 6, 128) 221312      activation_49[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_50 (BatchNo (None, 6, 6, 6, 128) 512         conv3d_68[0][0]                  \n","__________________________________________________________________________________________________\n","activation_50 (Activation)      (None, 6, 6, 6, 128) 0           batch_normalization_50[0][0]     \n","__________________________________________________________________________________________________\n","conv3d_69 (Conv3D)              (None, 6, 6, 6, 128) 442496      activation_50[0][0]              \n","__________________________________________________________________________________________________\n","conv3d_70 (Conv3D)              (None, 6, 6, 6, 128) 8320        max_pooling3d_7[0][0]            \n","__________________________________________________________________________________________________\n","add_25 (Add)                    (None, 6, 6, 6, 128) 0           conv3d_69[0][0]                  \n","                                                                 conv3d_70[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_51 (BatchNo (None, 6, 6, 6, 128) 512         add_25[0][0]                     \n","__________________________________________________________________________________________________\n","activation_51 (Activation)      (None, 6, 6, 6, 128) 0           batch_normalization_51[0][0]     \n","__________________________________________________________________________________________________\n","conv3d_71 (Conv3D)              (None, 6, 6, 6, 128) 442496      activation_51[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_52 (BatchNo (None, 6, 6, 6, 128) 512         conv3d_71[0][0]                  \n","__________________________________________________________________________________________________\n","activation_52 (Activation)      (None, 6, 6, 6, 128) 0           batch_normalization_52[0][0]     \n","__________________________________________________________________________________________________\n","conv3d_72 (Conv3D)              (None, 6, 6, 6, 128) 442496      activation_52[0][0]              \n","__________________________________________________________________________________________________\n","add_26 (Add)                    (None, 6, 6, 6, 128) 0           conv3d_72[0][0]                  \n","                                                                 add_25[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_53 (BatchNo (None, 6, 6, 6, 128) 512         add_26[0][0]                     \n","__________________________________________________________________________________________________\n","activation_53 (Activation)      (None, 6, 6, 6, 128) 0           batch_normalization_53[0][0]     \n","__________________________________________________________________________________________________\n","conv3d_73 (Conv3D)              (None, 3, 3, 3, 256) 884992      activation_53[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_54 (BatchNo (None, 3, 3, 3, 256) 1024        conv3d_73[0][0]                  \n","__________________________________________________________________________________________________\n","activation_54 (Activation)      (None, 3, 3, 3, 256) 0           batch_normalization_54[0][0]     \n","__________________________________________________________________________________________________\n","conv3d_74 (Conv3D)              (None, 3, 3, 3, 256) 1769728     activation_54[0][0]              \n","__________________________________________________________________________________________________\n","conv3d_75 (Conv3D)              (None, 3, 3, 3, 256) 33024       add_26[0][0]                     \n","__________________________________________________________________________________________________\n","add_27 (Add)                    (None, 3, 3, 3, 256) 0           conv3d_74[0][0]                  \n","                                                                 conv3d_75[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_55 (BatchNo (None, 3, 3, 3, 256) 1024        add_27[0][0]                     \n","__________________________________________________________________________________________________\n","activation_55 (Activation)      (None, 3, 3, 3, 256) 0           batch_normalization_55[0][0]     \n","__________________________________________________________________________________________________\n","conv3d_76 (Conv3D)              (None, 3, 3, 3, 256) 524544      activation_55[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_56 (BatchNo (None, 3, 3, 3, 256) 1024        conv3d_76[0][0]                  \n","__________________________________________________________________________________________________\n","activation_56 (Activation)      (None, 3, 3, 3, 256) 0           batch_normalization_56[0][0]     \n","__________________________________________________________________________________________________\n","conv3d_77 (Conv3D)              (None, 3, 3, 3, 256) 524544      activation_56[0][0]              \n","__________________________________________________________________________________________________\n","add_28 (Add)                    (None, 3, 3, 3, 256) 0           conv3d_77[0][0]                  \n","                                                                 add_27[0][0]                     \n","__________________________________________________________________________________________________\n","average_pooling3d_7 (AveragePoo (None, 1, 1, 1, 256) 0           add_28[0][0]                     \n","__________________________________________________________________________________________________\n","flatten_7 (Flatten)             (None, 256)          0           average_pooling3d_7[0][0]        \n","==================================================================================================\n","Total params: 5,301,120\n","Trainable params: 5,298,432\n","Non-trainable params: 2,688\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"GFAIu1o0VMLT","colab_type":"code","outputId":"fa014e47-7fa4-4ea6-e6b0-225c99cfacbc","executionInfo":{"status":"ok","timestamp":1555943667814,"user_tz":-480,"elapsed":3564,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":["def buildFullNetwork():\n","  #x1 = layers.Input(shape=(16,16,1))\n","  #x2 = layers.Input(shape=(87,87,1))\n","  #y1 = layers.Input(shape=(16,16,1))\n","  #y2 = layers.Input(shape=(87,87,1))\n","  #z1 = layers.Input(shape=(16,16,1))\n","  #z2 = layers.Input(shape=(87,87,1))\n","  cube = layers.Input(shape=(26,26,26,1))\n","  \n","  #x1_i = buildSingleResNetSmall()(x1)\n","  #x2_i = buildSingleResNetLarge()(x2)\n","  \n","  #y1_i = buildSingleResNetSmall()(y1)\n","  #y2_i = buildSingleResNetLarge()(y2)\n","  \n","  #z1_i = buildSingleResNetSmall()(z1)\n","  #z2_i = buildSingleResNetLarge()(z2)\n","  \n","  cube_i = buildSingleResNetCube()(cube)\n","  \n","  #x = layers.Concatenate()([x1_i, x2_i, y1_i, y2_i, z1_i, z2_i, cube_i])\n","  #x = layers.Concatenate()([x2_i, y2_i, z2_i])\n","  #x = layers.Concatenate()([x1_i, y1_i, z1_i])\n","  x = layers.Dropout(rate=0.5)(cube_i)\n","  #x = layers.Dense(270, activation='relu')(x)\n","  x = layers.Dense(135, activation='softmax')(x)\n","\n","  #network = models.Model(inputs=[x1, y1, z1, x2, y2, z2, cube], outputs=x)\n","  #network = models.Model(inputs=[x2, y2, z2], outputs=x)\n","  #network = models.Model(inputs=[x1, y1, z1], outputs=x)\n","  network = models.Model(inputs=cube, outputs=x)\n","  return network\n","\n","resnet = buildFullNetwork()\n","resnet.summary()\n","plot_model(resnet, to_file='drive/My Drive/MRI_Brain_Segmentation/resnet_diagram_highlv.png',show_shapes=True)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_13 (InputLayer)        (None, 26, 26, 26, 1)     0         \n","_________________________________________________________________\n","model_13 (Model)             (None, 256)               5301120   \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 135)               34695     \n","=================================================================\n","Total params: 5,335,815\n","Trainable params: 5,333,127\n","Non-trainable params: 2,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"rrBMo2Q5nTAe","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","adam = optimizers.Adam(lr=2e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=False)\n","resnet.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"143_wQu8zS6R","colab_type":"code","outputId":"ea0d43ff-3461-41c7-e634-f33ec6028fab","executionInfo":{"status":"ok","timestamp":1555939813587,"user_tz":-480,"elapsed":8026176,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}},"colab":{"base_uri":"https://localhost:8080/","height":132889}},"cell_type":"code","source":["# define the overall number of epochs per training session of 15 patients\n","T_epochs = 1\n","train_batch_size = 32\n","samples_size = 2000\n","vald_size = 200\n","test_size = samples_size - vald_size\n","samples_epochs = 1\n","total_voxel_sample = 1000000\n","\n","weight_path = \"drive/My Drive/MRI_Brain_Segmentation/resnet_cube_26_3.h5\"\n","\n","#reload weights that have been saved by the previous training session\n","#\n","#resnet.load_weights(\"drive/My Drive/MRI_Brain_Segmentation/resnet_cube_26_2.h5\")\n","#\n","#print(\"Model Restored\")\n","train_hist_acc = []\n","train_hist_val = []\n","\n","print(\"Start Training\")\n","stime = time.time()\n","\n","\n","try:\n","  for epoch in range(T_epochs):\n","      correct = 0\n","      iter = 0\n","      #define lists to append the data for training to vectorize the input\n","      \n","      t1=[] #small_x\n","      t2=[] #small_y\n","      t3=[] #small_z\n","      \n","      t4=[] #large_x\n","      t5=[] #large_y\n","      t6=[] #large_z\n","      \n","      t7=[] #volume\n","      t8=[] #ground truth\n","      \n","      \n","      for i in range(0,len(dataset)):\n","        #total number of elements in the image\n","\n","        #print(\"\\n Train Patient Number \" + str(i))\n","        print(\"\\n Train Loop Number \" + str(i))\n","        #Sample 200,000 voxels in total from each patient.\n","        for j in range (0,total_voxel_sample):\n","          value = 0\n","          pat = random.randint(0,len(dataset)-1)\n","          total = np.size(dataset[pat][0])\n","          raw = dataset[pat][0]\n","          \n","          #only select non-background voxels for training\n","          while(value==0):\n","            voxel = random.randint(0,total-1)\n","\n","            # before each image is 256x256, \n","            # after padding 43, the new dimension is 342x342\n","            # find the voxel x,y,z values\n","            # z_layer is the slice number\n","            z_layer = (voxel//116964)\n","            row = (voxel%116964)//342\n","            col = (voxel%116964)%342\n","            value = dataset[pat][1][z_layer][row][col]\n","          \n","          \n","          #extract the 2D and 3D images needed for that pixel\n","          # small is 29x29; large is 87x87; volume is 13x13x13\n","          \n","          small_x = raw[z_layer, row-8:row+8,col-8:col+8]\n","          small_y = raw[z_layer-8:z_layer+8, row, col-8:col+8]\n","          small_z = raw[z_layer-8:z_layer+8, row-8:row+8, col]\n","          \n","          large_x = raw[z_layer, row-43:row+44,col-43:col+44]\n","          large_y = raw[z_layer-43:z_layer+44, row, col-43:col+44]\n","          large_z = raw[z_layer-43:z_layer+44, row-43:row+44, col]\n","          \n","          vol = raw[z_layer-13:z_layer+13,row-13:row+13,col-13:col+13]\n","          \n","          #reshape \n","          \n","          small_x = np.reshape(small_x, (16,16,1))\n","          small_y = np.reshape(small_y, (16,16,1))\n","          small_z = np.reshape(small_z, (16,16,1))\n","          \n","          large_x = np.reshape(large_x, (87,87,1))\n","          large_y = np.reshape(large_y, (87,87,1))\n","          large_z = np.reshape(large_z, (87,87,1))\n","          \n","          vol = np.reshape(vol, (26,26,26,1))\n","          \n","          #one-hot encode the output value (label) for training\n","          # The range of value/label is [1,134]\n","          output = keras.utils.to_categorical(value, num_classes=135)\n","          \n","          #append the data to the list for training\n","          \n","          t1.append(small_x)\n","          t2.append(small_y)\n","          t3.append(small_z)\n","          \n","          t4.append(large_x)\n","          t5.append(large_y)\n","          t6.append(large_z)\n","          \n","          t7.append(vol)\n","          \n","          t8.append(output)\n","          \n","          #Collect and train every certain samples.\n","          if((j+1)%samples_size == 0):\n","            print(j+1, end=' ',flush=True)\n","            # train the model with the 5000 vectorized data. \n","            # 4000 will be used for training and the remaining 1000 will be used\n","            # for validation \n","            \"\"\"\n","            resnet.fit([t1[0:4000],t2[0:4000],t3[0:4000],t4[0:4000],t5[0:4000],t6[0:4000],t7[0:4000]], [t8[0:4000]], \n","                       epochs=1,batch_size=256,verbose=1, \n","                       validation_data=([t1[4000:],t2[4000:],t3[4000:],t4[4000:],t5[4000:],t6[4000:],t7[4000:]], [t8[4000:]]))\n","            \"\"\"\n","            x_train = np.array(t7)\n","            y_train = np.array(t8)\n","            \n","            #print(x_train.shape)\n","            #print(y_train.shape)\n","            \n","            h = resnet.fit(x_train[0:test_size], y_train[0:test_size], epochs=samples_epochs, batch_size=train_batch_size, verbose=1,\n","                           validation_data=([t7[test_size:]], [t8[test_size:]]))\n","            \"\"\"\n","            resnet.fit([t1[0:test_size], t2[0:test_size], t3[0:test_size]], [t8[0:test_size]], \n","                       epochs=samples_epochs,batch_size=train_batch_size,verbose=1, \n","                       validation_data=([t1[test_size:], t2[test_size:], t3[test_size:]], [t8[test_size:]]))\n","            \"\"\"\n","            train_hist_acc.append(h.history['acc'])\n","            train_hist_val.append(h.history['val_acc'])\n","            \n","            #reset the lists to accomodate the new 5000 samples.\n","            \n","            t1=[]\n","            t2=[]\n","            t3=[]\n","            \n","            t4=[]\n","            t5=[]\n","            t6=[]\n","            \n","            t7=[]\n","            \n","            t8=[]\n","        \n","        #Backup training weights after 1 patient\n","        resnet.save_weights(weight_path)\n","   \n","  #END of the 2 FOR loops \n","\n","  print(\"\\n Training Completed: Time Taken %s s\" % (time.time()-stime))\n","\n","  print('Epoch', epoch+1, 'completed out of',T_epochs)\n","\n","  #END of the 3 FOR loops \n","  print(\"Training Complete and Saving Model with \" + str(j+1) +\" Iterations per patient\")\n","  #Save weights after training completes\n","  resnet.save_weights(weight_path)\n","  print(\"Saved model to disk\")\n","\n","  print(\"All training completed !\")\n","\n","except KeyboardInterrupt:\n","  print(\"Training Interrupted Saving Model with \" + str((j+1)) +\" Iterations/patient\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Start Training\n","\n"," Train Loop Number 0\n","2000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 7s 4ms/step - loss: 4.1303 - acc: 0.2006 - val_loss: 3.2339 - val_acc: 0.2700\n","4000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 2s 1ms/step - loss: 3.4869 - acc: 0.2556 - val_loss: 3.1562 - val_acc: 0.2650\n","6000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 2s 1ms/step - loss: 3.0896 - acc: 0.3239 - val_loss: 2.4580 - val_acc: 0.4000\n","8000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 2.7437 - acc: 0.3800 - val_loss: 2.5945 - val_acc: 0.3750\n","10000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 2s 1ms/step - loss: 2.5125 - acc: 0.4211 - val_loss: 2.3571 - val_acc: 0.4050\n","12000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 2.4850 - acc: 0.4044 - val_loss: 2.4801 - val_acc: 0.4200\n","14000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 2s 1ms/step - loss: 2.2832 - acc: 0.4544 - val_loss: 2.2140 - val_acc: 0.4600\n","16000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 2.1249 - acc: 0.4778 - val_loss: 1.9782 - val_acc: 0.4950\n","18000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 2s 1ms/step - loss: 2.1413 - acc: 0.4667 - val_loss: 1.9136 - val_acc: 0.5000\n","20000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.9986 - acc: 0.4961 - val_loss: 1.7338 - val_acc: 0.5600\n","22000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 2s 1ms/step - loss: 1.8635 - acc: 0.5228 - val_loss: 2.0772 - val_acc: 0.5350\n","24000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.9329 - acc: 0.5056 - val_loss: 1.4065 - val_acc: 0.6150\n","26000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.8646 - acc: 0.5161 - val_loss: 2.0097 - val_acc: 0.5000\n","28000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.7065 - acc: 0.5478 - val_loss: 3.2156 - val_acc: 0.3750\n","30000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.6576 - acc: 0.5567 - val_loss: 1.9531 - val_acc: 0.5050\n","32000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.6588 - acc: 0.5539 - val_loss: 1.4611 - val_acc: 0.5850\n","34000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.7018 - acc: 0.5500 - val_loss: 1.6211 - val_acc: 0.5650\n","36000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.4540 - acc: 0.6078 - val_loss: 1.7743 - val_acc: 0.5200\n","38000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.5455 - acc: 0.5761 - val_loss: 1.1676 - val_acc: 0.6350\n","40000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.4941 - acc: 0.5856 - val_loss: 1.5417 - val_acc: 0.5500\n","42000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.4596 - acc: 0.6083 - val_loss: 1.4430 - val_acc: 0.6050\n","44000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.4072 - acc: 0.6022 - val_loss: 1.7453 - val_acc: 0.5400\n","46000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.4573 - acc: 0.5961 - val_loss: 1.3291 - val_acc: 0.5800\n","48000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.2927 - acc: 0.6378 - val_loss: 1.4580 - val_acc: 0.5950\n","50000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.3131 - acc: 0.6267 - val_loss: 1.2968 - val_acc: 0.5850\n","52000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.3308 - acc: 0.6361 - val_loss: 1.2137 - val_acc: 0.6400\n","54000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.3883 - acc: 0.6117 - val_loss: 1.4403 - val_acc: 0.6100\n","56000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.2975 - acc: 0.6211 - val_loss: 1.2211 - val_acc: 0.6050\n","58000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.3635 - acc: 0.6194 - val_loss: 1.7123 - val_acc: 0.5300\n","60000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.2653 - acc: 0.6350 - val_loss: 1.5113 - val_acc: 0.5750\n","62000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 2s 1ms/step - loss: 1.2149 - acc: 0.6506 - val_loss: 1.5235 - val_acc: 0.5450\n","64000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.2743 - acc: 0.6361 - val_loss: 1.0779 - val_acc: 0.6600\n","66000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.2562 - acc: 0.6317 - val_loss: 1.0487 - val_acc: 0.6600\n","68000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.1980 - acc: 0.6433 - val_loss: 1.2666 - val_acc: 0.6350\n","70000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.1228 - acc: 0.6722 - val_loss: 1.6734 - val_acc: 0.4950\n","72000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.1678 - acc: 0.6506 - val_loss: 1.0944 - val_acc: 0.6600\n","74000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0647 - acc: 0.6878 - val_loss: 0.9020 - val_acc: 0.7550\n","76000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.1915 - acc: 0.6606 - val_loss: 1.1757 - val_acc: 0.6800\n","78000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.1457 - acc: 0.6667 - val_loss: 1.0500 - val_acc: 0.6850\n","80000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.1501 - acc: 0.6622 - val_loss: 0.8905 - val_acc: 0.7250\n","82000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9935 - acc: 0.6961 - val_loss: 0.9532 - val_acc: 0.7450\n","84000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0611 - acc: 0.6972 - val_loss: 1.1564 - val_acc: 0.6350\n","86000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0796 - acc: 0.6917 - val_loss: 1.0560 - val_acc: 0.6900\n","88000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0720 - acc: 0.6839 - val_loss: 1.4714 - val_acc: 0.6150\n","90000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9477 - acc: 0.7217 - val_loss: 0.8497 - val_acc: 0.7400\n","92000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0425 - acc: 0.6939 - val_loss: 1.1729 - val_acc: 0.6450\n","94000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0806 - acc: 0.6767 - val_loss: 1.0125 - val_acc: 0.6950\n","96000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0017 - acc: 0.6994 - val_loss: 1.0216 - val_acc: 0.6800\n","98000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0322 - acc: 0.6900 - val_loss: 0.8163 - val_acc: 0.7150\n","100000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9493 - acc: 0.7089 - val_loss: 0.8097 - val_acc: 0.7800\n","102000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9699 - acc: 0.7189 - val_loss: 1.6363 - val_acc: 0.5700\n","104000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0261 - acc: 0.6900 - val_loss: 1.5283 - val_acc: 0.5550\n","106000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0374 - acc: 0.6917 - val_loss: 0.9154 - val_acc: 0.6800\n","108000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0342 - acc: 0.6978 - val_loss: 1.1166 - val_acc: 0.6700\n","110000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9473 - acc: 0.7144 - val_loss: 1.0643 - val_acc: 0.6600\n","112000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9156 - acc: 0.7228 - val_loss: 1.0534 - val_acc: 0.6750\n","114000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9070 - acc: 0.7306 - val_loss: 1.0566 - val_acc: 0.6550\n","116000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9870 - acc: 0.7000 - val_loss: 0.8330 - val_acc: 0.7600\n","118000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0282 - acc: 0.6889 - val_loss: 0.7725 - val_acc: 0.7400\n","120000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9770 - acc: 0.7078 - val_loss: 0.9617 - val_acc: 0.7300\n","122000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9487 - acc: 0.7150 - val_loss: 0.9918 - val_acc: 0.7100\n","124000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9315 - acc: 0.7161 - val_loss: 0.7207 - val_acc: 0.7900\n","126000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9213 - acc: 0.7272 - val_loss: 0.6415 - val_acc: 0.7850\n","128000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9291 - acc: 0.7150 - val_loss: 1.3109 - val_acc: 0.6050\n","130000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9289 - acc: 0.7161 - val_loss: 0.7587 - val_acc: 0.7250\n","132000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 1.0027 - acc: 0.6906 - val_loss: 0.7611 - val_acc: 0.7600\n","134000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9621 - acc: 0.7144 - val_loss: 0.8444 - val_acc: 0.7600\n","136000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8806 - acc: 0.7344 - val_loss: 1.4753 - val_acc: 0.5900\n","138000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8754 - acc: 0.7272 - val_loss: 1.0065 - val_acc: 0.6950\n","140000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9103 - acc: 0.7206 - val_loss: 1.1487 - val_acc: 0.6500\n","142000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9025 - acc: 0.7278 - val_loss: 0.9033 - val_acc: 0.7250\n","144000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8683 - acc: 0.7406 - val_loss: 0.9059 - val_acc: 0.7250\n","146000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8984 - acc: 0.7183 - val_loss: 0.7836 - val_acc: 0.7600\n","148000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8709 - acc: 0.7322 - val_loss: 0.9516 - val_acc: 0.6750\n","150000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8426 - acc: 0.7311 - val_loss: 0.7863 - val_acc: 0.7550\n","152000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8387 - acc: 0.7294 - val_loss: 0.8273 - val_acc: 0.7400\n","154000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8060 - acc: 0.7506 - val_loss: 0.7444 - val_acc: 0.7800\n","156000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9124 - acc: 0.7333 - val_loss: 0.9057 - val_acc: 0.6950\n","158000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8680 - acc: 0.7361 - val_loss: 0.6441 - val_acc: 0.7550\n","160000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9282 - acc: 0.7144 - val_loss: 1.0472 - val_acc: 0.6450\n","162000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8177 - acc: 0.7533 - val_loss: 0.7720 - val_acc: 0.7800\n","164000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.9214 - acc: 0.7267 - val_loss: 0.9433 - val_acc: 0.7400\n","166000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8650 - acc: 0.7328 - val_loss: 0.8613 - val_acc: 0.7500\n","168000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8495 - acc: 0.7244 - val_loss: 0.7747 - val_acc: 0.7000\n","170000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8003 - acc: 0.7433 - val_loss: 1.1086 - val_acc: 0.6900\n","172000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8598 - acc: 0.7322 - val_loss: 0.6849 - val_acc: 0.7800\n","174000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7964 - acc: 0.7672 - val_loss: 0.8284 - val_acc: 0.7500\n","176000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7882 - acc: 0.7550 - val_loss: 0.9145 - val_acc: 0.6950\n","178000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7893 - acc: 0.7544 - val_loss: 0.8777 - val_acc: 0.7000\n","180000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8266 - acc: 0.7467 - val_loss: 0.9038 - val_acc: 0.7250\n","182000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7573 - acc: 0.7456 - val_loss: 0.8133 - val_acc: 0.7650\n","184000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8934 - acc: 0.7261 - val_loss: 0.8187 - val_acc: 0.7900\n","186000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8075 - acc: 0.7500 - val_loss: 0.7209 - val_acc: 0.7800\n","188000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7546 - acc: 0.7561 - val_loss: 0.8267 - val_acc: 0.7400\n","190000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8148 - acc: 0.7367 - val_loss: 0.9451 - val_acc: 0.6950\n","192000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7717 - acc: 0.7533 - val_loss: 0.8514 - val_acc: 0.6950\n","194000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7768 - acc: 0.7544 - val_loss: 0.6306 - val_acc: 0.7750\n","196000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8127 - acc: 0.7478 - val_loss: 0.9070 - val_acc: 0.7500\n","198000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7802 - acc: 0.7550 - val_loss: 2.5327 - val_acc: 0.4900\n","200000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7790 - acc: 0.7350 - val_loss: 0.6523 - val_acc: 0.7850\n","202000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7808 - acc: 0.7500 - val_loss: 0.6989 - val_acc: 0.7600\n","204000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7773 - acc: 0.7606 - val_loss: 1.1113 - val_acc: 0.6650\n","206000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7777 - acc: 0.7550 - val_loss: 0.7052 - val_acc: 0.7350\n","208000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7700 - acc: 0.7667 - val_loss: 0.7834 - val_acc: 0.7750\n","210000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7808 - acc: 0.7494 - val_loss: 0.7157 - val_acc: 0.7550\n","212000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8394 - acc: 0.7372 - val_loss: 0.7290 - val_acc: 0.7950\n","214000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8325 - acc: 0.7456 - val_loss: 0.7461 - val_acc: 0.7850\n","216000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.8038 - acc: 0.7439 - val_loss: 0.7095 - val_acc: 0.7750\n","218000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7164 - acc: 0.7722 - val_loss: 0.7649 - val_acc: 0.7400\n","220000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7394 - acc: 0.7644 - val_loss: 0.7330 - val_acc: 0.7950\n","222000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7596 - acc: 0.7633 - val_loss: 0.9332 - val_acc: 0.7100\n","224000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7557 - acc: 0.7567 - val_loss: 0.5798 - val_acc: 0.7950\n","226000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7767 - acc: 0.7489 - val_loss: 0.8455 - val_acc: 0.7600\n","228000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7538 - acc: 0.7656 - val_loss: 0.5571 - val_acc: 0.8100\n","230000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7435 - acc: 0.7761 - val_loss: 0.6091 - val_acc: 0.7750\n","232000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7346 - acc: 0.7650 - val_loss: 0.6147 - val_acc: 0.7800\n","234000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7758 - acc: 0.7567 - val_loss: 0.7992 - val_acc: 0.7400\n","236000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7989 - acc: 0.7517 - val_loss: 0.7843 - val_acc: 0.7550\n","238000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7223 - acc: 0.7622 - val_loss: 0.5982 - val_acc: 0.7750\n","240000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7358 - acc: 0.7567 - val_loss: 0.5008 - val_acc: 0.8100\n","242000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7258 - acc: 0.7683 - val_loss: 0.5779 - val_acc: 0.7950\n","244000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7083 - acc: 0.7744 - val_loss: 0.7039 - val_acc: 0.7150\n","246000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6809 - acc: 0.7772 - val_loss: 0.5268 - val_acc: 0.8200\n","248000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7854 - acc: 0.7589 - val_loss: 0.8343 - val_acc: 0.7400\n","250000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6655 - acc: 0.7844 - val_loss: 0.6972 - val_acc: 0.7500\n","252000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7487 - acc: 0.7550 - val_loss: 0.8818 - val_acc: 0.7150\n","254000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7084 - acc: 0.7750 - val_loss: 1.1459 - val_acc: 0.6400\n","256000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7445 - acc: 0.7622 - val_loss: 0.9079 - val_acc: 0.7150\n","258000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7530 - acc: 0.7567 - val_loss: 0.6162 - val_acc: 0.7950\n","260000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7717 - acc: 0.7450 - val_loss: 0.5973 - val_acc: 0.8200\n","262000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6725 - acc: 0.7811 - val_loss: 0.6941 - val_acc: 0.7750\n","264000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7210 - acc: 0.7678 - val_loss: 0.6131 - val_acc: 0.8050\n","266000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7926 - acc: 0.7583 - val_loss: 0.9700 - val_acc: 0.7100\n","268000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7176 - acc: 0.7644 - val_loss: 0.5971 - val_acc: 0.7900\n","270000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7027 - acc: 0.7667 - val_loss: 0.8255 - val_acc: 0.7400\n","272000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6633 - acc: 0.7822 - val_loss: 0.7009 - val_acc: 0.7900\n","274000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7186 - acc: 0.7706 - val_loss: 0.7495 - val_acc: 0.7400\n","276000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7845 - acc: 0.7517 - val_loss: 0.6935 - val_acc: 0.7600\n","278000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6723 - acc: 0.7844 - val_loss: 0.5211 - val_acc: 0.7900\n","280000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6938 - acc: 0.7783 - val_loss: 0.7251 - val_acc: 0.7700\n","282000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6905 - acc: 0.7706 - val_loss: 0.6519 - val_acc: 0.7700\n","284000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7493 - acc: 0.7556 - val_loss: 0.9500 - val_acc: 0.6850\n","286000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6668 - acc: 0.7778 - val_loss: 0.7753 - val_acc: 0.7450\n","288000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7024 - acc: 0.7750 - val_loss: 1.0399 - val_acc: 0.6600\n","290000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7378 - acc: 0.7683 - val_loss: 0.7139 - val_acc: 0.7800\n","292000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7043 - acc: 0.7633 - val_loss: 0.5460 - val_acc: 0.8150\n","294000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6570 - acc: 0.7833 - val_loss: 0.6071 - val_acc: 0.8050\n","296000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6403 - acc: 0.7922 - val_loss: 0.5957 - val_acc: 0.8250\n","298000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6556 - acc: 0.7844 - val_loss: 0.4190 - val_acc: 0.8300\n","300000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6900 - acc: 0.7711 - val_loss: 0.6824 - val_acc: 0.7600\n","302000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6811 - acc: 0.7844 - val_loss: 0.7695 - val_acc: 0.7550\n","304000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6086 - acc: 0.7950 - val_loss: 0.6434 - val_acc: 0.7950\n","306000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6674 - acc: 0.7833 - val_loss: 0.7497 - val_acc: 0.8000\n","308000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7402 - acc: 0.7567 - val_loss: 0.6583 - val_acc: 0.7850\n","310000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6778 - acc: 0.7833 - val_loss: 0.6640 - val_acc: 0.7550\n","312000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6595 - acc: 0.8000 - val_loss: 0.7259 - val_acc: 0.7550\n","314000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6946 - acc: 0.7828 - val_loss: 0.6458 - val_acc: 0.7650\n","316000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6814 - acc: 0.7800 - val_loss: 0.4758 - val_acc: 0.8300\n","318000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6932 - acc: 0.7739 - val_loss: 0.5566 - val_acc: 0.8000\n","320000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6816 - acc: 0.7933 - val_loss: 0.6590 - val_acc: 0.7800\n","322000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6482 - acc: 0.7844 - val_loss: 0.7220 - val_acc: 0.7300\n","324000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6765 - acc: 0.7689 - val_loss: 0.5464 - val_acc: 0.8350\n","326000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6765 - acc: 0.7822 - val_loss: 0.7840 - val_acc: 0.7500\n","328000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6522 - acc: 0.7867 - val_loss: 0.6767 - val_acc: 0.7950\n","330000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7017 - acc: 0.7744 - val_loss: 0.4591 - val_acc: 0.8350\n","332000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6196 - acc: 0.7856 - val_loss: 0.6198 - val_acc: 0.8050\n","334000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7053 - acc: 0.7650 - val_loss: 0.6207 - val_acc: 0.8050\n","336000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6468 - acc: 0.7928 - val_loss: 1.0403 - val_acc: 0.6750\n","338000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6293 - acc: 0.7961 - val_loss: 0.5751 - val_acc: 0.8450\n","340000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6502 - acc: 0.7872 - val_loss: 0.6914 - val_acc: 0.7450\n","342000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.7075 - acc: 0.7739 - val_loss: 0.7489 - val_acc: 0.7500\n","344000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6719 - acc: 0.7800 - val_loss: 0.6005 - val_acc: 0.8050\n","346000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6191 - acc: 0.7944 - val_loss: 0.5964 - val_acc: 0.7700\n","348000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6787 - acc: 0.7800 - val_loss: 0.5723 - val_acc: 0.7750\n","350000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6603 - acc: 0.7839 - val_loss: 0.7421 - val_acc: 0.7700\n","352000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6324 - acc: 0.7794 - val_loss: 0.5614 - val_acc: 0.7750\n","354000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5927 - acc: 0.7989 - val_loss: 0.6513 - val_acc: 0.8250\n","356000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6671 - acc: 0.7817 - val_loss: 0.4559 - val_acc: 0.8550\n","358000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6411 - acc: 0.7883 - val_loss: 0.7006 - val_acc: 0.7600\n","360000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6478 - acc: 0.7772 - val_loss: 0.4839 - val_acc: 0.8400\n","362000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6296 - acc: 0.7939 - val_loss: 0.5290 - val_acc: 0.8350\n","364000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6010 - acc: 0.7861 - val_loss: 0.7036 - val_acc: 0.7750\n","366000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6648 - acc: 0.7839 - val_loss: 0.6039 - val_acc: 0.7750\n","368000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6738 - acc: 0.7761 - val_loss: 0.6136 - val_acc: 0.7950\n","370000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6239 - acc: 0.7950 - val_loss: 0.7546 - val_acc: 0.7550\n","372000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6843 - acc: 0.7694 - val_loss: 0.5868 - val_acc: 0.7850\n","374000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6381 - acc: 0.7972 - val_loss: 0.5513 - val_acc: 0.8300\n","376000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6314 - acc: 0.8083 - val_loss: 0.5212 - val_acc: 0.8150\n","378000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6181 - acc: 0.7978 - val_loss: 0.5168 - val_acc: 0.8100\n","380000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6443 - acc: 0.7900 - val_loss: 0.7589 - val_acc: 0.7400\n","382000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5997 - acc: 0.7939 - val_loss: 0.5555 - val_acc: 0.8050\n","384000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6536 - acc: 0.7761 - val_loss: 0.7585 - val_acc: 0.7650\n","386000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6437 - acc: 0.7767 - val_loss: 0.5519 - val_acc: 0.7950\n","388000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6309 - acc: 0.7889 - val_loss: 0.6372 - val_acc: 0.7950\n","390000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6418 - acc: 0.7844 - val_loss: 0.5656 - val_acc: 0.8100\n","392000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5822 - acc: 0.8006 - val_loss: 0.5443 - val_acc: 0.8200\n","394000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5852 - acc: 0.8061 - val_loss: 0.8026 - val_acc: 0.7450\n","396000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5607 - acc: 0.8156 - val_loss: 0.6203 - val_acc: 0.8150\n","398000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6365 - acc: 0.7894 - val_loss: 0.4531 - val_acc: 0.8600\n","400000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5857 - acc: 0.8117 - val_loss: 0.6286 - val_acc: 0.7950\n","402000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6193 - acc: 0.8033 - val_loss: 0.6995 - val_acc: 0.7450\n","404000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6904 - acc: 0.7778 - val_loss: 0.6881 - val_acc: 0.7750\n","406000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5925 - acc: 0.7967 - val_loss: 0.5111 - val_acc: 0.8250\n","408000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5571 - acc: 0.8150 - val_loss: 0.5590 - val_acc: 0.8350\n","410000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6883 - acc: 0.7728 - val_loss: 0.5412 - val_acc: 0.8250\n","412000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6090 - acc: 0.7894 - val_loss: 0.6057 - val_acc: 0.7850\n","414000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6722 - acc: 0.7811 - val_loss: 0.5801 - val_acc: 0.8100\n","416000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5949 - acc: 0.8006 - val_loss: 0.5776 - val_acc: 0.7900\n","418000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5979 - acc: 0.8006 - val_loss: 0.4979 - val_acc: 0.8100\n","420000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6217 - acc: 0.8044 - val_loss: 0.5889 - val_acc: 0.8100\n","422000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6324 - acc: 0.7894 - val_loss: 0.5795 - val_acc: 0.7700\n","424000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5707 - acc: 0.8100 - val_loss: 0.5808 - val_acc: 0.7950\n","426000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5698 - acc: 0.8133 - val_loss: 0.5677 - val_acc: 0.8050\n","428000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6171 - acc: 0.7989 - val_loss: 0.5674 - val_acc: 0.8100\n","430000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6108 - acc: 0.7950 - val_loss: 0.4933 - val_acc: 0.8150\n","432000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5778 - acc: 0.7944 - val_loss: 0.6743 - val_acc: 0.8050\n","434000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6079 - acc: 0.7989 - val_loss: 0.5308 - val_acc: 0.7900\n","436000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5981 - acc: 0.7989 - val_loss: 0.8414 - val_acc: 0.7350\n","438000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5996 - acc: 0.8050 - val_loss: 0.7177 - val_acc: 0.8050\n","440000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6030 - acc: 0.7978 - val_loss: 0.4583 - val_acc: 0.8400\n","442000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5611 - acc: 0.8189 - val_loss: 0.7000 - val_acc: 0.8200\n","444000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5784 - acc: 0.8033 - val_loss: 0.7296 - val_acc: 0.7450\n","446000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5621 - acc: 0.8206 - val_loss: 0.4859 - val_acc: 0.8400\n","448000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6240 - acc: 0.7856 - val_loss: 0.4591 - val_acc: 0.8250\n","450000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5866 - acc: 0.8100 - val_loss: 0.6740 - val_acc: 0.7600\n","452000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6029 - acc: 0.8033 - val_loss: 0.5870 - val_acc: 0.7750\n","454000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5708 - acc: 0.8056 - val_loss: 0.4737 - val_acc: 0.8250\n","456000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5652 - acc: 0.8061 - val_loss: 0.5813 - val_acc: 0.8050\n","458000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6016 - acc: 0.7994 - val_loss: 0.5056 - val_acc: 0.7950\n","460000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5596 - acc: 0.8111 - val_loss: 0.5030 - val_acc: 0.8250\n","462000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5480 - acc: 0.8178 - val_loss: 0.4301 - val_acc: 0.8400\n","464000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5960 - acc: 0.7972 - val_loss: 0.6470 - val_acc: 0.8000\n","466000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6005 - acc: 0.8000 - val_loss: 0.5441 - val_acc: 0.8400\n","468000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5585 - acc: 0.8133 - val_loss: 0.6956 - val_acc: 0.7800\n","470000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5744 - acc: 0.8022 - val_loss: 0.4770 - val_acc: 0.8100\n","472000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5551 - acc: 0.8206 - val_loss: 0.5829 - val_acc: 0.8200\n","474000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6226 - acc: 0.7839 - val_loss: 0.5767 - val_acc: 0.8000\n","476000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6196 - acc: 0.8011 - val_loss: 0.5933 - val_acc: 0.8050\n","478000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5715 - acc: 0.8111 - val_loss: 0.4929 - val_acc: 0.8250\n","480000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5460 - acc: 0.8150 - val_loss: 0.5150 - val_acc: 0.8150\n","482000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5644 - acc: 0.8089 - val_loss: 0.3821 - val_acc: 0.8500\n","484000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5505 - acc: 0.8228 - val_loss: 0.5064 - val_acc: 0.8350\n","486000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5702 - acc: 0.7994 - val_loss: 0.6358 - val_acc: 0.7700\n","488000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5371 - acc: 0.8200 - val_loss: 0.5391 - val_acc: 0.8200\n","490000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.6091 - acc: 0.8078 - val_loss: 0.5589 - val_acc: 0.8100\n","492000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5146 - acc: 0.8283 - val_loss: 0.3989 - val_acc: 0.8350\n","494000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5769 - acc: 0.8083 - val_loss: 0.6381 - val_acc: 0.8050\n","496000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5571 - acc: 0.8167 - val_loss: 0.4620 - val_acc: 0.8250\n","498000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5667 - acc: 0.8194 - val_loss: 0.5146 - val_acc: 0.8200\n","500000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5329 - acc: 0.8194 - val_loss: 0.4500 - val_acc: 0.8600\n","502000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5724 - acc: 0.7983 - val_loss: 0.5416 - val_acc: 0.7950\n","504000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5849 - acc: 0.8144 - val_loss: 0.6356 - val_acc: 0.7950\n","506000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5109 - acc: 0.8267 - val_loss: 0.5523 - val_acc: 0.8050\n","508000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5904 - acc: 0.7994 - val_loss: 0.5249 - val_acc: 0.8000\n","510000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5830 - acc: 0.8072 - val_loss: 0.4993 - val_acc: 0.8400\n","512000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5674 - acc: 0.8139 - val_loss: 0.5408 - val_acc: 0.8000\n","514000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5812 - acc: 0.8033 - val_loss: 0.5564 - val_acc: 0.8250\n","516000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5622 - acc: 0.8111 - val_loss: 0.5825 - val_acc: 0.7600\n","518000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5736 - acc: 0.7978 - val_loss: 0.5694 - val_acc: 0.8200\n","520000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5422 - acc: 0.8156 - val_loss: 0.4395 - val_acc: 0.8350\n","522000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5478 - acc: 0.8156 - val_loss: 0.6278 - val_acc: 0.8000\n","524000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5555 - acc: 0.8061 - val_loss: 0.5174 - val_acc: 0.8300\n","526000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5679 - acc: 0.8089 - val_loss: 0.5409 - val_acc: 0.8350\n","528000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5540 - acc: 0.8072 - val_loss: 0.5781 - val_acc: 0.7950\n","530000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5127 - acc: 0.8150 - val_loss: 0.5341 - val_acc: 0.8300\n","532000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5493 - acc: 0.8139 - val_loss: 0.5509 - val_acc: 0.8050\n","534000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5458 - acc: 0.8167 - val_loss: 0.4924 - val_acc: 0.8400\n","536000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5382 - acc: 0.8194 - val_loss: 0.5277 - val_acc: 0.8300\n","538000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5940 - acc: 0.8000 - val_loss: 0.4179 - val_acc: 0.8250\n","540000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5599 - acc: 0.8111 - val_loss: 0.4622 - val_acc: 0.8300\n","542000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5619 - acc: 0.8089 - val_loss: 0.5396 - val_acc: 0.8050\n","544000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5915 - acc: 0.7967 - val_loss: 0.5155 - val_acc: 0.8200\n","546000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5339 - acc: 0.8278 - val_loss: 0.5544 - val_acc: 0.7700\n","548000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5287 - acc: 0.8267 - val_loss: 0.5260 - val_acc: 0.8100\n","550000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5507 - acc: 0.8144 - val_loss: 1.0001 - val_acc: 0.7250\n","552000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5941 - acc: 0.7972 - val_loss: 0.4988 - val_acc: 0.8150\n","554000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5341 - acc: 0.8233 - val_loss: 0.4368 - val_acc: 0.8400\n","556000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5752 - acc: 0.8117 - val_loss: 0.5434 - val_acc: 0.8150\n","558000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5636 - acc: 0.8072 - val_loss: 0.6019 - val_acc: 0.8050\n","560000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5283 - acc: 0.8167 - val_loss: 0.4214 - val_acc: 0.8400\n","562000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5740 - acc: 0.8094 - val_loss: 1.1196 - val_acc: 0.6750\n","564000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5340 - acc: 0.8194 - val_loss: 0.4311 - val_acc: 0.8450\n","566000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5654 - acc: 0.8144 - val_loss: 0.6622 - val_acc: 0.7750\n","568000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5774 - acc: 0.8156 - val_loss: 0.4505 - val_acc: 0.8650\n","570000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5102 - acc: 0.8261 - val_loss: 0.5342 - val_acc: 0.8000\n","572000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5355 - acc: 0.8172 - val_loss: 0.5553 - val_acc: 0.8000\n","574000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5969 - acc: 0.8072 - val_loss: 0.4443 - val_acc: 0.8750\n","576000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5416 - acc: 0.8178 - val_loss: 0.5088 - val_acc: 0.8300\n","578000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5627 - acc: 0.8222 - val_loss: 0.5065 - val_acc: 0.8300\n","580000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5186 - acc: 0.8144 - val_loss: 0.5029 - val_acc: 0.8150\n","582000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5386 - acc: 0.8172 - val_loss: 0.5267 - val_acc: 0.8000\n","584000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5370 - acc: 0.8156 - val_loss: 0.5084 - val_acc: 0.8300\n","586000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5335 - acc: 0.8161 - val_loss: 0.5956 - val_acc: 0.8100\n","588000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5383 - acc: 0.8094 - val_loss: 0.4415 - val_acc: 0.8600\n","590000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5511 - acc: 0.8206 - val_loss: 0.4339 - val_acc: 0.8400\n","592000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5356 - acc: 0.8150 - val_loss: 0.5727 - val_acc: 0.8000\n","594000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5442 - acc: 0.8133 - val_loss: 0.5599 - val_acc: 0.8150\n","596000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5553 - acc: 0.8089 - val_loss: 0.7397 - val_acc: 0.7750\n","598000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5209 - acc: 0.8233 - val_loss: 0.6755 - val_acc: 0.7800\n","600000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5526 - acc: 0.8122 - val_loss: 0.5458 - val_acc: 0.8200\n","602000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5394 - acc: 0.8228 - val_loss: 0.5686 - val_acc: 0.8100\n","604000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5017 - acc: 0.8367 - val_loss: 0.5295 - val_acc: 0.8350\n","606000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5240 - acc: 0.8228 - val_loss: 0.5855 - val_acc: 0.7950\n","608000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5441 - acc: 0.8122 - val_loss: 0.4799 - val_acc: 0.8300\n","610000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5058 - acc: 0.8233 - val_loss: 0.5986 - val_acc: 0.8200\n","612000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5441 - acc: 0.8083 - val_loss: 0.5096 - val_acc: 0.8300\n","614000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5217 - acc: 0.8106 - val_loss: 0.4235 - val_acc: 0.8250\n","616000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5480 - acc: 0.8094 - val_loss: 0.5775 - val_acc: 0.8050\n","618000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5219 - acc: 0.8183 - val_loss: 0.6834 - val_acc: 0.7750\n","620000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5207 - acc: 0.8300 - val_loss: 0.5697 - val_acc: 0.8200\n","622000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5544 - acc: 0.8167 - val_loss: 0.4445 - val_acc: 0.8500\n","624000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5136 - acc: 0.8250 - val_loss: 0.4862 - val_acc: 0.8350\n","626000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5248 - acc: 0.8139 - val_loss: 0.5235 - val_acc: 0.8050\n","628000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5195 - acc: 0.8133 - val_loss: 0.4492 - val_acc: 0.8050\n","630000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5744 - acc: 0.8078 - val_loss: 0.3970 - val_acc: 0.8350\n","632000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5441 - acc: 0.8144 - val_loss: 0.4949 - val_acc: 0.8300\n","634000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5428 - acc: 0.8183 - val_loss: 0.5262 - val_acc: 0.8300\n","636000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5262 - acc: 0.8128 - val_loss: 0.4609 - val_acc: 0.8400\n","638000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5069 - acc: 0.8283 - val_loss: 0.7165 - val_acc: 0.7600\n","640000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5077 - acc: 0.8250 - val_loss: 0.5254 - val_acc: 0.8100\n","642000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5661 - acc: 0.8106 - val_loss: 0.3852 - val_acc: 0.8250\n","644000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4791 - acc: 0.8378 - val_loss: 0.6203 - val_acc: 0.7800\n","646000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5381 - acc: 0.8183 - val_loss: 0.5631 - val_acc: 0.7950\n","648000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5133 - acc: 0.8228 - val_loss: 0.4410 - val_acc: 0.8450\n","650000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5414 - acc: 0.8200 - val_loss: 0.7288 - val_acc: 0.7900\n","652000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4901 - acc: 0.8300 - val_loss: 0.6519 - val_acc: 0.7950\n","654000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5039 - acc: 0.8206 - val_loss: 0.5371 - val_acc: 0.7850\n","656000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5337 - acc: 0.8222 - val_loss: 0.6950 - val_acc: 0.7650\n","658000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5633 - acc: 0.8039 - val_loss: 0.4081 - val_acc: 0.8650\n","660000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5304 - acc: 0.8206 - val_loss: 0.3845 - val_acc: 0.8550\n","662000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4999 - acc: 0.8328 - val_loss: 0.5275 - val_acc: 0.8150\n","664000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4425 - acc: 0.8483 - val_loss: 0.3628 - val_acc: 0.8800\n","666000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5157 - acc: 0.8244 - val_loss: 0.3586 - val_acc: 0.8650\n","668000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5009 - acc: 0.8339 - val_loss: 0.4083 - val_acc: 0.8850\n","670000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4929 - acc: 0.8333 - val_loss: 0.6259 - val_acc: 0.8000\n","672000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5317 - acc: 0.8217 - val_loss: 0.4062 - val_acc: 0.8800\n","674000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5306 - acc: 0.8072 - val_loss: 0.6087 - val_acc: 0.7950\n","676000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5070 - acc: 0.8328 - val_loss: 0.4351 - val_acc: 0.8500\n","678000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4826 - acc: 0.8456 - val_loss: 0.4719 - val_acc: 0.8350\n","680000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4784 - acc: 0.8283 - val_loss: 0.4805 - val_acc: 0.8200\n","682000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5091 - acc: 0.8311 - val_loss: 0.4424 - val_acc: 0.8350\n","684000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4840 - acc: 0.8361 - val_loss: 0.5019 - val_acc: 0.8100\n","686000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4994 - acc: 0.8211 - val_loss: 0.4105 - val_acc: 0.8500\n","688000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5428 - acc: 0.8106 - val_loss: 0.5575 - val_acc: 0.8050\n","690000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5214 - acc: 0.8300 - val_loss: 0.4520 - val_acc: 0.8350\n","692000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5215 - acc: 0.8167 - val_loss: 0.4473 - val_acc: 0.8150\n","694000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4573 - acc: 0.8317 - val_loss: 0.3668 - val_acc: 0.8550\n","696000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5107 - acc: 0.8306 - val_loss: 0.3704 - val_acc: 0.8550\n","698000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4976 - acc: 0.8311 - val_loss: 0.3278 - val_acc: 0.8850\n","700000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4514 - acc: 0.8344 - val_loss: 0.4325 - val_acc: 0.8450\n","702000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4823 - acc: 0.8244 - val_loss: 0.4389 - val_acc: 0.8500\n","704000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4898 - acc: 0.8333 - val_loss: 0.4825 - val_acc: 0.8350\n","706000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4877 - acc: 0.8239 - val_loss: 0.6525 - val_acc: 0.7600\n","708000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5290 - acc: 0.8167 - val_loss: 0.5175 - val_acc: 0.8350\n","710000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5042 - acc: 0.8167 - val_loss: 0.4068 - val_acc: 0.8700\n","712000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4870 - acc: 0.8339 - val_loss: 0.4280 - val_acc: 0.8400\n","714000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4537 - acc: 0.8439 - val_loss: 0.5232 - val_acc: 0.8150\n","716000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4854 - acc: 0.8361 - val_loss: 0.4598 - val_acc: 0.8400\n","718000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5518 - acc: 0.8183 - val_loss: 0.4243 - val_acc: 0.8600\n","720000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5232 - acc: 0.8167 - val_loss: 0.3709 - val_acc: 0.8900\n","722000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4872 - acc: 0.8289 - val_loss: 0.3544 - val_acc: 0.8650\n","724000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4928 - acc: 0.8300 - val_loss: 0.6180 - val_acc: 0.7850\n","726000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5111 - acc: 0.8294 - val_loss: 0.5653 - val_acc: 0.8000\n","728000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5090 - acc: 0.8394 - val_loss: 0.4505 - val_acc: 0.8400\n","730000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4905 - acc: 0.8267 - val_loss: 0.4110 - val_acc: 0.8350\n","732000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4787 - acc: 0.8261 - val_loss: 0.3068 - val_acc: 0.9000\n","734000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4845 - acc: 0.8367 - val_loss: 0.4042 - val_acc: 0.8550\n","736000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5383 - acc: 0.8083 - val_loss: 0.5583 - val_acc: 0.8100\n","738000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5216 - acc: 0.8161 - val_loss: 0.3703 - val_acc: 0.8700\n","740000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5119 - acc: 0.8272 - val_loss: 0.4242 - val_acc: 0.8250\n","742000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4940 - acc: 0.8311 - val_loss: 0.5825 - val_acc: 0.8100\n","744000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5021 - acc: 0.8289 - val_loss: 0.3959 - val_acc: 0.8600\n","746000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5224 - acc: 0.8167 - val_loss: 0.4476 - val_acc: 0.8450\n","748000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5050 - acc: 0.8328 - val_loss: 0.5755 - val_acc: 0.8000\n","750000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4976 - acc: 0.8356 - val_loss: 0.4408 - val_acc: 0.8300\n","752000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4627 - acc: 0.8483 - val_loss: 0.5002 - val_acc: 0.8300\n","754000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4851 - acc: 0.8317 - val_loss: 0.4603 - val_acc: 0.8550\n","756000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5112 - acc: 0.8283 - val_loss: 0.4310 - val_acc: 0.8550\n","758000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4664 - acc: 0.8444 - val_loss: 0.6342 - val_acc: 0.8150\n","760000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5041 - acc: 0.8233 - val_loss: 0.4800 - val_acc: 0.8400\n","762000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4539 - acc: 0.8472 - val_loss: 0.6408 - val_acc: 0.7600\n","764000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4888 - acc: 0.8233 - val_loss: 0.4016 - val_acc: 0.8500\n","766000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4894 - acc: 0.8250 - val_loss: 0.4281 - val_acc: 0.8600\n","768000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4912 - acc: 0.8261 - val_loss: 0.5379 - val_acc: 0.8100\n","770000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5613 - acc: 0.8100 - val_loss: 0.4443 - val_acc: 0.8700\n","772000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5076 - acc: 0.8233 - val_loss: 0.3656 - val_acc: 0.8500\n","774000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4626 - acc: 0.8428 - val_loss: 0.5459 - val_acc: 0.8200\n","776000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5019 - acc: 0.8244 - val_loss: 0.5176 - val_acc: 0.8150\n","778000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4929 - acc: 0.8328 - val_loss: 0.4551 - val_acc: 0.8300\n","780000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4388 - acc: 0.8456 - val_loss: 0.4821 - val_acc: 0.8250\n","782000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4961 - acc: 0.8322 - val_loss: 0.3809 - val_acc: 0.8650\n","784000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4804 - acc: 0.8272 - val_loss: 0.4936 - val_acc: 0.8650\n","786000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4581 - acc: 0.8367 - val_loss: 0.4282 - val_acc: 0.8400\n","788000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4909 - acc: 0.8378 - val_loss: 0.4313 - val_acc: 0.8750\n","790000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4937 - acc: 0.8383 - val_loss: 0.4011 - val_acc: 0.8700\n","792000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4630 - acc: 0.8306 - val_loss: 0.4262 - val_acc: 0.8500\n","794000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5056 - acc: 0.8233 - val_loss: 0.4593 - val_acc: 0.8250\n","796000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4687 - acc: 0.8383 - val_loss: 0.4379 - val_acc: 0.8450\n","798000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4649 - acc: 0.8322 - val_loss: 0.5175 - val_acc: 0.8150\n","800000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4650 - acc: 0.8461 - val_loss: 0.4159 - val_acc: 0.8450\n","802000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4376 - acc: 0.8533 - val_loss: 0.4105 - val_acc: 0.8650\n","804000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4627 - acc: 0.8306 - val_loss: 0.4393 - val_acc: 0.8250\n","806000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4489 - acc: 0.8450 - val_loss: 0.4219 - val_acc: 0.8450\n","808000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4627 - acc: 0.8383 - val_loss: 0.4065 - val_acc: 0.8400\n","810000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4474 - acc: 0.8428 - val_loss: 0.4412 - val_acc: 0.8300\n","812000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4909 - acc: 0.8261 - val_loss: 0.3759 - val_acc: 0.8800\n","814000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4870 - acc: 0.8294 - val_loss: 0.6143 - val_acc: 0.8050\n","816000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5014 - acc: 0.8283 - val_loss: 0.3840 - val_acc: 0.8600\n","818000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4602 - acc: 0.8317 - val_loss: 0.6378 - val_acc: 0.7800\n","820000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4908 - acc: 0.8372 - val_loss: 0.4110 - val_acc: 0.8350\n","822000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4856 - acc: 0.8283 - val_loss: 0.4277 - val_acc: 0.8750\n","824000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4741 - acc: 0.8400 - val_loss: 0.4709 - val_acc: 0.8500\n","826000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4383 - acc: 0.8383 - val_loss: 0.3724 - val_acc: 0.8600\n","828000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4375 - acc: 0.8528 - val_loss: 0.3818 - val_acc: 0.8550\n","830000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4717 - acc: 0.8494 - val_loss: 0.3430 - val_acc: 0.8600\n","832000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4573 - acc: 0.8422 - val_loss: 0.4922 - val_acc: 0.8300\n","834000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4515 - acc: 0.8378 - val_loss: 0.3354 - val_acc: 0.8600\n","836000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5044 - acc: 0.8289 - val_loss: 0.4248 - val_acc: 0.8600\n","838000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4721 - acc: 0.8283 - val_loss: 0.3595 - val_acc: 0.8350\n","840000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4780 - acc: 0.8372 - val_loss: 0.4979 - val_acc: 0.8350\n","842000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4776 - acc: 0.8244 - val_loss: 0.5315 - val_acc: 0.8000\n","844000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5008 - acc: 0.8311 - val_loss: 0.4186 - val_acc: 0.8200\n","846000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4926 - acc: 0.8244 - val_loss: 0.4682 - val_acc: 0.8250\n","848000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4760 - acc: 0.8306 - val_loss: 0.4370 - val_acc: 0.8400\n","850000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4925 - acc: 0.8228 - val_loss: 0.4993 - val_acc: 0.8300\n","852000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4777 - acc: 0.8306 - val_loss: 0.3595 - val_acc: 0.8850\n","854000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4416 - acc: 0.8517 - val_loss: 0.3430 - val_acc: 0.8750\n","856000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4686 - acc: 0.8333 - val_loss: 0.3793 - val_acc: 0.8550\n","858000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4891 - acc: 0.8272 - val_loss: 0.4461 - val_acc: 0.8550\n","860000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4780 - acc: 0.8339 - val_loss: 0.4589 - val_acc: 0.8200\n","862000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4883 - acc: 0.8156 - val_loss: 0.5977 - val_acc: 0.7950\n","864000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4598 - acc: 0.8389 - val_loss: 0.4863 - val_acc: 0.8250\n","866000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4801 - acc: 0.8261 - val_loss: 0.3871 - val_acc: 0.8450\n","868000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4168 - acc: 0.8528 - val_loss: 0.4038 - val_acc: 0.8500\n","870000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.5321 - acc: 0.8289 - val_loss: 0.3248 - val_acc: 0.9000\n","872000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4584 - acc: 0.8367 - val_loss: 0.4254 - val_acc: 0.8850\n","874000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4408 - acc: 0.8428 - val_loss: 0.3393 - val_acc: 0.8950\n","876000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4699 - acc: 0.8306 - val_loss: 0.6286 - val_acc: 0.7950\n","878000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4970 - acc: 0.8361 - val_loss: 0.3908 - val_acc: 0.8500\n","880000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4852 - acc: 0.8328 - val_loss: 0.3773 - val_acc: 0.8400\n","882000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4726 - acc: 0.8406 - val_loss: 0.3938 - val_acc: 0.8750\n","884000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4718 - acc: 0.8400 - val_loss: 0.5224 - val_acc: 0.8000\n","886000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4757 - acc: 0.8294 - val_loss: 0.4618 - val_acc: 0.8300\n","888000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4633 - acc: 0.8356 - val_loss: 0.4140 - val_acc: 0.8800\n","890000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4574 - acc: 0.8389 - val_loss: 0.5044 - val_acc: 0.8250\n","892000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4860 - acc: 0.8283 - val_loss: 0.3735 - val_acc: 0.8500\n","894000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4718 - acc: 0.8417 - val_loss: 0.4618 - val_acc: 0.8650\n","896000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4407 - acc: 0.8506 - val_loss: 0.3902 - val_acc: 0.8950\n","898000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4876 - acc: 0.8261 - val_loss: 0.4577 - val_acc: 0.8500\n","900000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4524 - acc: 0.8450 - val_loss: 0.4961 - val_acc: 0.8600\n","902000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4564 - acc: 0.8439 - val_loss: 0.4737 - val_acc: 0.8200\n","904000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4686 - acc: 0.8361 - val_loss: 0.4382 - val_acc: 0.8400\n","906000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4441 - acc: 0.8450 - val_loss: 0.4526 - val_acc: 0.8700\n","908000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4627 - acc: 0.8383 - val_loss: 0.3962 - val_acc: 0.8400\n","910000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4341 - acc: 0.8439 - val_loss: 0.3707 - val_acc: 0.8500\n","912000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4771 - acc: 0.8311 - val_loss: 0.4146 - val_acc: 0.8400\n","914000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4920 - acc: 0.8389 - val_loss: 0.2735 - val_acc: 0.9150\n","916000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4176 - acc: 0.8539 - val_loss: 0.4475 - val_acc: 0.8250\n","918000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4341 - acc: 0.8500 - val_loss: 0.3804 - val_acc: 0.8800\n","920000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4467 - acc: 0.8456 - val_loss: 0.4749 - val_acc: 0.8300\n","922000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4536 - acc: 0.8367 - val_loss: 0.4403 - val_acc: 0.8400\n","924000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4239 - acc: 0.8556 - val_loss: 0.6851 - val_acc: 0.7700\n","926000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4759 - acc: 0.8350 - val_loss: 0.3601 - val_acc: 0.8750\n","928000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4363 - acc: 0.8561 - val_loss: 0.3931 - val_acc: 0.8700\n","930000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4689 - acc: 0.8289 - val_loss: 0.7358 - val_acc: 0.7350\n","932000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4762 - acc: 0.8344 - val_loss: 0.4575 - val_acc: 0.8350\n","934000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4132 - acc: 0.8517 - val_loss: 0.5607 - val_acc: 0.7700\n","936000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4605 - acc: 0.8417 - val_loss: 0.4036 - val_acc: 0.8800\n","938000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4816 - acc: 0.8311 - val_loss: 0.4859 - val_acc: 0.8150\n","940000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4409 - acc: 0.8444 - val_loss: 0.4547 - val_acc: 0.8300\n","942000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4350 - acc: 0.8461 - val_loss: 0.3750 - val_acc: 0.8700\n","944000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4610 - acc: 0.8378 - val_loss: 0.4619 - val_acc: 0.8400\n","946000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4684 - acc: 0.8378 - val_loss: 0.2917 - val_acc: 0.8950\n","948000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4225 - acc: 0.8367 - val_loss: 0.4215 - val_acc: 0.8700\n","950000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4411 - acc: 0.8389 - val_loss: 0.4596 - val_acc: 0.8400\n","952000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 2ms/step - loss: 0.4215 - acc: 0.8517 - val_loss: 0.3158 - val_acc: 0.8700\n","954000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4621 - acc: 0.8428 - val_loss: 0.2826 - val_acc: 0.8950\n","956000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4362 - acc: 0.8494 - val_loss: 0.3125 - val_acc: 0.8700\n","958000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4567 - acc: 0.8422 - val_loss: 0.4653 - val_acc: 0.8300\n","960000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4644 - acc: 0.8422 - val_loss: 0.3851 - val_acc: 0.8650\n","962000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4435 - acc: 0.8472 - val_loss: 0.5048 - val_acc: 0.8300\n","964000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4547 - acc: 0.8400 - val_loss: 0.4734 - val_acc: 0.8200\n","966000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4478 - acc: 0.8422 - val_loss: 0.3686 - val_acc: 0.8650\n","968000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4311 - acc: 0.8589 - val_loss: 0.3980 - val_acc: 0.8700\n","970000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4738 - acc: 0.8383 - val_loss: 0.3883 - val_acc: 0.8600\n","972000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4423 - acc: 0.8472 - val_loss: 0.4211 - val_acc: 0.8600\n","974000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4347 - acc: 0.8511 - val_loss: 0.4461 - val_acc: 0.8600\n","976000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4971 - acc: 0.8300 - val_loss: 0.3662 - val_acc: 0.8500\n","978000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4348 - acc: 0.8422 - val_loss: 0.4337 - val_acc: 0.8300\n","980000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4822 - acc: 0.8317 - val_loss: 0.4966 - val_acc: 0.8100\n","982000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4469 - acc: 0.8456 - val_loss: 0.5239 - val_acc: 0.8150\n","984000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4399 - acc: 0.8500 - val_loss: 0.3429 - val_acc: 0.8800\n","986000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4427 - acc: 0.8389 - val_loss: 0.4049 - val_acc: 0.8500\n","988000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4187 - acc: 0.8656 - val_loss: 0.4820 - val_acc: 0.8500\n","990000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4575 - acc: 0.8428 - val_loss: 0.3655 - val_acc: 0.8800\n","992000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4004 - acc: 0.8639 - val_loss: 0.4004 - val_acc: 0.8350\n","994000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4583 - acc: 0.8322 - val_loss: 0.3925 - val_acc: 0.8650\n","996000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4334 - acc: 0.8417 - val_loss: 0.4176 - val_acc: 0.8550\n","998000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4243 - acc: 0.8506 - val_loss: 0.6493 - val_acc: 0.7700\n","1000000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4428 - acc: 0.8433 - val_loss: 0.5023 - val_acc: 0.8100\n","\n"," Train Loop Number 1\n","2000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4256 - acc: 0.8472 - val_loss: 0.3942 - val_acc: 0.8500\n","4000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4707 - acc: 0.8439 - val_loss: 0.4266 - val_acc: 0.8700\n","6000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4451 - acc: 0.8394 - val_loss: 0.3822 - val_acc: 0.8700\n","8000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4446 - acc: 0.8489 - val_loss: 0.4197 - val_acc: 0.8350\n","10000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4589 - acc: 0.8372 - val_loss: 0.4700 - val_acc: 0.8350\n","12000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4560 - acc: 0.8383 - val_loss: 0.3441 - val_acc: 0.8700\n","14000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4554 - acc: 0.8417 - val_loss: 0.3805 - val_acc: 0.8600\n","16000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4994 - acc: 0.8194 - val_loss: 0.3908 - val_acc: 0.8700\n","18000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4640 - acc: 0.8344 - val_loss: 0.3314 - val_acc: 0.8550\n","20000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4197 - acc: 0.8500 - val_loss: 0.4736 - val_acc: 0.8200\n","22000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4668 - acc: 0.8389 - val_loss: 0.4167 - val_acc: 0.8400\n","24000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4494 - acc: 0.8428 - val_loss: 0.4788 - val_acc: 0.8250\n","26000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4530 - acc: 0.8472 - val_loss: 0.3416 - val_acc: 0.8700\n","28000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4404 - acc: 0.8528 - val_loss: 0.3967 - val_acc: 0.8600\n","30000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4273 - acc: 0.8439 - val_loss: 0.4447 - val_acc: 0.8750\n","32000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4174 - acc: 0.8500 - val_loss: 0.4226 - val_acc: 0.8700\n","34000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4422 - acc: 0.8439 - val_loss: 0.4233 - val_acc: 0.8400\n","36000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4038 - acc: 0.8644 - val_loss: 0.5110 - val_acc: 0.8250\n","38000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4400 - acc: 0.8528 - val_loss: 0.3307 - val_acc: 0.8650\n","40000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4830 - acc: 0.8344 - val_loss: 0.4292 - val_acc: 0.8550\n","42000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4534 - acc: 0.8367 - val_loss: 0.4675 - val_acc: 0.8400\n","44000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4104 - acc: 0.8550 - val_loss: 0.4791 - val_acc: 0.8250\n","46000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4439 - acc: 0.8422 - val_loss: 0.5896 - val_acc: 0.8400\n","48000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4300 - acc: 0.8450 - val_loss: 0.3675 - val_acc: 0.8650\n","50000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4229 - acc: 0.8578 - val_loss: 0.4238 - val_acc: 0.8600\n","52000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4563 - acc: 0.8450 - val_loss: 0.4152 - val_acc: 0.8250\n","54000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3826 - acc: 0.8633 - val_loss: 0.3181 - val_acc: 0.8950\n","56000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4357 - acc: 0.8406 - val_loss: 0.3534 - val_acc: 0.8600\n","58000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4315 - acc: 0.8439 - val_loss: 0.3278 - val_acc: 0.8850\n","60000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4741 - acc: 0.8394 - val_loss: 0.3927 - val_acc: 0.8700\n","62000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4287 - acc: 0.8472 - val_loss: 0.3051 - val_acc: 0.9100\n","64000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3998 - acc: 0.8528 - val_loss: 0.4491 - val_acc: 0.8400\n","66000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4187 - acc: 0.8422 - val_loss: 0.5254 - val_acc: 0.8200\n","68000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3956 - acc: 0.8589 - val_loss: 0.4129 - val_acc: 0.8550\n","70000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4359 - acc: 0.8411 - val_loss: 0.3968 - val_acc: 0.8450\n","72000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4609 - acc: 0.8311 - val_loss: 0.3921 - val_acc: 0.8600\n","74000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4283 - acc: 0.8539 - val_loss: 0.4502 - val_acc: 0.8650\n","76000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4095 - acc: 0.8594 - val_loss: 0.3824 - val_acc: 0.8450\n","78000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4159 - acc: 0.8561 - val_loss: 0.3636 - val_acc: 0.8850\n","80000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4281 - acc: 0.8533 - val_loss: 0.4105 - val_acc: 0.8450\n","82000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4357 - acc: 0.8417 - val_loss: 0.4343 - val_acc: 0.8400\n","84000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4559 - acc: 0.8444 - val_loss: 0.5017 - val_acc: 0.8200\n","86000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4112 - acc: 0.8472 - val_loss: 0.4254 - val_acc: 0.8600\n","88000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4178 - acc: 0.8561 - val_loss: 0.4587 - val_acc: 0.8250\n","90000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4532 - acc: 0.8400 - val_loss: 0.4562 - val_acc: 0.8350\n","92000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4167 - acc: 0.8528 - val_loss: 0.3960 - val_acc: 0.8950\n","94000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4653 - acc: 0.8378 - val_loss: 0.4384 - val_acc: 0.8450\n","96000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4806 - acc: 0.8294 - val_loss: 0.3970 - val_acc: 0.8800\n","98000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4573 - acc: 0.8367 - val_loss: 0.4007 - val_acc: 0.8450\n","100000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4365 - acc: 0.8428 - val_loss: 0.2938 - val_acc: 0.8950\n","102000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4351 - acc: 0.8489 - val_loss: 0.4606 - val_acc: 0.8350\n","104000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4124 - acc: 0.8439 - val_loss: 0.4016 - val_acc: 0.8400\n","106000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4307 - acc: 0.8417 - val_loss: 0.3117 - val_acc: 0.8750\n","108000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3920 - acc: 0.8667 - val_loss: 0.4080 - val_acc: 0.8550\n","110000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4538 - acc: 0.8400 - val_loss: 0.3287 - val_acc: 0.8700\n","112000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4277 - acc: 0.8522 - val_loss: 0.3269 - val_acc: 0.8750\n","114000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4419 - acc: 0.8439 - val_loss: 0.3590 - val_acc: 0.8650\n","116000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4332 - acc: 0.8394 - val_loss: 0.4419 - val_acc: 0.8450\n","118000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4025 - acc: 0.8611 - val_loss: 0.3791 - val_acc: 0.8650\n","120000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4229 - acc: 0.8433 - val_loss: 0.3964 - val_acc: 0.8850\n","122000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4307 - acc: 0.8428 - val_loss: 0.4295 - val_acc: 0.8500\n","124000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3966 - acc: 0.8639 - val_loss: 0.3936 - val_acc: 0.8500\n","126000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4737 - acc: 0.8378 - val_loss: 0.3421 - val_acc: 0.8550\n","128000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4162 - acc: 0.8489 - val_loss: 0.3738 - val_acc: 0.8700\n","130000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4278 - acc: 0.8489 - val_loss: 0.5425 - val_acc: 0.8300\n","132000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4214 - acc: 0.8539 - val_loss: 0.4556 - val_acc: 0.8400\n","134000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4072 - acc: 0.8522 - val_loss: 0.5126 - val_acc: 0.8250\n","136000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3886 - acc: 0.8661 - val_loss: 0.4718 - val_acc: 0.8300\n","138000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3942 - acc: 0.8606 - val_loss: 0.4178 - val_acc: 0.8600\n","140000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4162 - acc: 0.8506 - val_loss: 0.3845 - val_acc: 0.8750\n","142000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4229 - acc: 0.8511 - val_loss: 0.4357 - val_acc: 0.8350\n","144000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4457 - acc: 0.8433 - val_loss: 0.4904 - val_acc: 0.8350\n","146000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4185 - acc: 0.8483 - val_loss: 0.4085 - val_acc: 0.8300\n","148000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4132 - acc: 0.8611 - val_loss: 0.3911 - val_acc: 0.8600\n","150000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4053 - acc: 0.8533 - val_loss: 0.3594 - val_acc: 0.8600\n","152000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4066 - acc: 0.8544 - val_loss: 0.2589 - val_acc: 0.9000\n","154000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4448 - acc: 0.8489 - val_loss: 0.4366 - val_acc: 0.8550\n","156000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4059 - acc: 0.8511 - val_loss: 0.3247 - val_acc: 0.8750\n","158000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4076 - acc: 0.8494 - val_loss: 0.3837 - val_acc: 0.8600\n","160000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4459 - acc: 0.8461 - val_loss: 0.3714 - val_acc: 0.8550\n","162000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4202 - acc: 0.8539 - val_loss: 0.4982 - val_acc: 0.8250\n","164000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3945 - acc: 0.8506 - val_loss: 0.3009 - val_acc: 0.9050\n","166000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4166 - acc: 0.8522 - val_loss: 0.4268 - val_acc: 0.8500\n","168000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3971 - acc: 0.8617 - val_loss: 0.3158 - val_acc: 0.9000\n","170000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4240 - acc: 0.8478 - val_loss: 0.4688 - val_acc: 0.8300\n","172000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4078 - acc: 0.8578 - val_loss: 0.3583 - val_acc: 0.8950\n","174000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4155 - acc: 0.8528 - val_loss: 0.4178 - val_acc: 0.8550\n","176000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4101 - acc: 0.8494 - val_loss: 0.4342 - val_acc: 0.8450\n","178000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4524 - acc: 0.8411 - val_loss: 0.4516 - val_acc: 0.8350\n","180000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4447 - acc: 0.8389 - val_loss: 0.4183 - val_acc: 0.8900\n","182000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4224 - acc: 0.8489 - val_loss: 0.3204 - val_acc: 0.8850\n","184000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4390 - acc: 0.8422 - val_loss: 0.3838 - val_acc: 0.8850\n","186000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4531 - acc: 0.8444 - val_loss: 0.3277 - val_acc: 0.9100\n","188000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4272 - acc: 0.8506 - val_loss: 0.3536 - val_acc: 0.8750\n","190000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4017 - acc: 0.8567 - val_loss: 0.3686 - val_acc: 0.8700\n","192000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3950 - acc: 0.8511 - val_loss: 0.3898 - val_acc: 0.8700\n","194000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4056 - acc: 0.8522 - val_loss: 0.3963 - val_acc: 0.8500\n","196000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4490 - acc: 0.8444 - val_loss: 0.5458 - val_acc: 0.7900\n","198000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4070 - acc: 0.8561 - val_loss: 0.3570 - val_acc: 0.8500\n","200000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4204 - acc: 0.8572 - val_loss: 0.3401 - val_acc: 0.8950\n","202000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3866 - acc: 0.8600 - val_loss: 0.2988 - val_acc: 0.9000\n","204000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4542 - acc: 0.8261 - val_loss: 0.3728 - val_acc: 0.8500\n","206000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3989 - acc: 0.8611 - val_loss: 0.3406 - val_acc: 0.8650\n","208000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4424 - acc: 0.8361 - val_loss: 0.4770 - val_acc: 0.8100\n","210000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4004 - acc: 0.8617 - val_loss: 0.4068 - val_acc: 0.8400\n","212000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4061 - acc: 0.8544 - val_loss: 0.4164 - val_acc: 0.8600\n","214000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4003 - acc: 0.8628 - val_loss: 0.3412 - val_acc: 0.8900\n","216000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3954 - acc: 0.8622 - val_loss: 0.3783 - val_acc: 0.8850\n","218000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4395 - acc: 0.8456 - val_loss: 0.2415 - val_acc: 0.9100\n","220000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3829 - acc: 0.8628 - val_loss: 0.5100 - val_acc: 0.8450\n","222000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3912 - acc: 0.8639 - val_loss: 0.2987 - val_acc: 0.8700\n","224000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4067 - acc: 0.8517 - val_loss: 0.3724 - val_acc: 0.8750\n","226000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4148 - acc: 0.8506 - val_loss: 0.3750 - val_acc: 0.8600\n","228000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3889 - acc: 0.8644 - val_loss: 0.3513 - val_acc: 0.8800\n","230000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3896 - acc: 0.8622 - val_loss: 0.4231 - val_acc: 0.8550\n","232000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3863 - acc: 0.8567 - val_loss: 0.3861 - val_acc: 0.8650\n","234000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4619 - acc: 0.8389 - val_loss: 0.4071 - val_acc: 0.8850\n","236000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3967 - acc: 0.8650 - val_loss: 0.3986 - val_acc: 0.8550\n","238000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3949 - acc: 0.8561 - val_loss: 0.4042 - val_acc: 0.8200\n","240000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3651 - acc: 0.8700 - val_loss: 0.3417 - val_acc: 0.8700\n","242000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3711 - acc: 0.8722 - val_loss: 0.3053 - val_acc: 0.8850\n","244000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3733 - acc: 0.8617 - val_loss: 0.3075 - val_acc: 0.8700\n","246000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4190 - acc: 0.8533 - val_loss: 0.3871 - val_acc: 0.8600\n","248000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3820 - acc: 0.8617 - val_loss: 0.3429 - val_acc: 0.8700\n","250000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4157 - acc: 0.8522 - val_loss: 0.4739 - val_acc: 0.8500\n","252000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4077 - acc: 0.8656 - val_loss: 0.4626 - val_acc: 0.8150\n","254000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4242 - acc: 0.8483 - val_loss: 0.5714 - val_acc: 0.8050\n","256000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3974 - acc: 0.8583 - val_loss: 0.2976 - val_acc: 0.8800\n","258000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4239 - acc: 0.8467 - val_loss: 0.4047 - val_acc: 0.8450\n","260000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3734 - acc: 0.8689 - val_loss: 0.3542 - val_acc: 0.8750\n","262000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4354 - acc: 0.8422 - val_loss: 0.4972 - val_acc: 0.8500\n","264000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3995 - acc: 0.8611 - val_loss: 0.3369 - val_acc: 0.8800\n","266000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4078 - acc: 0.8511 - val_loss: 0.3215 - val_acc: 0.8700\n","268000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4176 - acc: 0.8533 - val_loss: 0.4487 - val_acc: 0.8450\n","270000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3803 - acc: 0.8611 - val_loss: 0.3943 - val_acc: 0.8650\n","272000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4233 - acc: 0.8611 - val_loss: 0.3926 - val_acc: 0.8500\n","274000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4093 - acc: 0.8506 - val_loss: 0.4115 - val_acc: 0.8300\n","276000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3587 - acc: 0.8756 - val_loss: 0.2285 - val_acc: 0.9250\n","278000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3759 - acc: 0.8600 - val_loss: 0.3874 - val_acc: 0.8600\n","280000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4025 - acc: 0.8511 - val_loss: 0.4269 - val_acc: 0.8150\n","282000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4193 - acc: 0.8522 - val_loss: 0.3385 - val_acc: 0.8800\n","284000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4009 - acc: 0.8528 - val_loss: 0.4067 - val_acc: 0.8550\n","286000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4546 - acc: 0.8356 - val_loss: 0.3656 - val_acc: 0.8600\n","288000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4051 - acc: 0.8594 - val_loss: 0.4321 - val_acc: 0.8350\n","290000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3882 - acc: 0.8639 - val_loss: 0.3394 - val_acc: 0.8900\n","292000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4065 - acc: 0.8611 - val_loss: 0.4032 - val_acc: 0.8550\n","294000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4204 - acc: 0.8500 - val_loss: 0.3694 - val_acc: 0.8650\n","296000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3868 - acc: 0.8578 - val_loss: 0.3608 - val_acc: 0.8650\n","298000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3963 - acc: 0.8583 - val_loss: 0.3589 - val_acc: 0.8800\n","300000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4177 - acc: 0.8561 - val_loss: 0.2630 - val_acc: 0.9100\n","302000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3788 - acc: 0.8606 - val_loss: 0.4246 - val_acc: 0.8450\n","304000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4055 - acc: 0.8561 - val_loss: 0.3619 - val_acc: 0.8800\n","306000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4193 - acc: 0.8533 - val_loss: 0.4339 - val_acc: 0.8550\n","308000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4115 - acc: 0.8589 - val_loss: 0.2995 - val_acc: 0.8900\n","310000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4207 - acc: 0.8467 - val_loss: 0.3128 - val_acc: 0.8900\n","312000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4385 - acc: 0.8461 - val_loss: 0.3561 - val_acc: 0.8700\n","314000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4166 - acc: 0.8517 - val_loss: 0.2589 - val_acc: 0.9000\n","316000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3897 - acc: 0.8661 - val_loss: 0.5284 - val_acc: 0.8000\n","318000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3613 - acc: 0.8783 - val_loss: 0.3077 - val_acc: 0.8850\n","320000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4060 - acc: 0.8544 - val_loss: 0.3634 - val_acc: 0.8850\n","322000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3868 - acc: 0.8661 - val_loss: 0.4387 - val_acc: 0.8550\n","324000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3986 - acc: 0.8606 - val_loss: 0.2738 - val_acc: 0.9000\n","326000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3881 - acc: 0.8656 - val_loss: 0.3259 - val_acc: 0.8850\n","328000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4064 - acc: 0.8628 - val_loss: 0.3317 - val_acc: 0.9000\n","330000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4033 - acc: 0.8528 - val_loss: 0.3438 - val_acc: 0.8850\n","332000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3926 - acc: 0.8483 - val_loss: 0.3208 - val_acc: 0.8800\n","334000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3762 - acc: 0.8739 - val_loss: 0.3493 - val_acc: 0.8750\n","336000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3855 - acc: 0.8639 - val_loss: 0.3744 - val_acc: 0.8500\n","338000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4067 - acc: 0.8583 - val_loss: 0.3302 - val_acc: 0.8700\n","340000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4163 - acc: 0.8522 - val_loss: 0.3950 - val_acc: 0.8550\n","342000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4384 - acc: 0.8461 - val_loss: 0.3724 - val_acc: 0.8450\n","344000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4058 - acc: 0.8556 - val_loss: 0.3756 - val_acc: 0.8850\n","346000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4080 - acc: 0.8683 - val_loss: 0.4526 - val_acc: 0.8200\n","348000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3725 - acc: 0.8628 - val_loss: 0.3150 - val_acc: 0.8750\n","350000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3801 - acc: 0.8628 - val_loss: 0.3995 - val_acc: 0.8700\n","352000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3941 - acc: 0.8644 - val_loss: 0.4162 - val_acc: 0.8250\n","354000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4176 - acc: 0.8539 - val_loss: 0.4747 - val_acc: 0.8150\n","356000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3819 - acc: 0.8711 - val_loss: 0.3695 - val_acc: 0.8700\n","358000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4021 - acc: 0.8522 - val_loss: 0.4065 - val_acc: 0.8350\n","360000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3828 - acc: 0.8567 - val_loss: 0.3344 - val_acc: 0.8700\n","362000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4051 - acc: 0.8661 - val_loss: 0.2752 - val_acc: 0.8900\n","364000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4043 - acc: 0.8689 - val_loss: 0.4121 - val_acc: 0.8700\n","366000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4112 - acc: 0.8517 - val_loss: 0.3070 - val_acc: 0.8750\n","368000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4143 - acc: 0.8544 - val_loss: 0.4093 - val_acc: 0.8850\n","370000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3993 - acc: 0.8628 - val_loss: 0.3138 - val_acc: 0.9000\n","372000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4076 - acc: 0.8444 - val_loss: 0.3322 - val_acc: 0.8950\n","374000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3991 - acc: 0.8533 - val_loss: 0.3928 - val_acc: 0.8400\n","376000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4074 - acc: 0.8578 - val_loss: 0.3196 - val_acc: 0.8850\n","378000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4215 - acc: 0.8483 - val_loss: 0.4134 - val_acc: 0.8650\n","380000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3708 - acc: 0.8606 - val_loss: 0.4043 - val_acc: 0.8200\n","382000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3988 - acc: 0.8617 - val_loss: 0.3906 - val_acc: 0.8550\n","384000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4046 - acc: 0.8656 - val_loss: 0.2720 - val_acc: 0.9200\n","386000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4022 - acc: 0.8589 - val_loss: 0.3754 - val_acc: 0.8600\n","388000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3939 - acc: 0.8611 - val_loss: 0.4558 - val_acc: 0.8550\n","390000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3751 - acc: 0.8639 - val_loss: 0.3210 - val_acc: 0.8800\n","392000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3818 - acc: 0.8617 - val_loss: 0.4070 - val_acc: 0.8600\n","394000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3564 - acc: 0.8767 - val_loss: 0.5450 - val_acc: 0.7850\n","396000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4226 - acc: 0.8528 - val_loss: 0.2477 - val_acc: 0.9200\n","398000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3606 - acc: 0.8783 - val_loss: 0.3920 - val_acc: 0.8500\n","400000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3980 - acc: 0.8511 - val_loss: 0.5669 - val_acc: 0.8150\n","402000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3540 - acc: 0.8694 - val_loss: 0.3388 - val_acc: 0.8900\n","404000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3912 - acc: 0.8567 - val_loss: 0.3411 - val_acc: 0.9000\n","406000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3670 - acc: 0.8644 - val_loss: 0.3206 - val_acc: 0.8650\n","408000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4037 - acc: 0.8550 - val_loss: 0.4951 - val_acc: 0.8200\n","410000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3817 - acc: 0.8622 - val_loss: 0.3634 - val_acc: 0.8550\n","412000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3695 - acc: 0.8683 - val_loss: 0.2049 - val_acc: 0.9250\n","414000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3361 - acc: 0.8811 - val_loss: 0.3300 - val_acc: 0.8700\n","416000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3905 - acc: 0.8644 - val_loss: 0.3999 - val_acc: 0.8450\n","418000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3819 - acc: 0.8667 - val_loss: 0.4343 - val_acc: 0.8450\n","420000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3624 - acc: 0.8706 - val_loss: 0.3992 - val_acc: 0.8350\n","422000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4406 - acc: 0.8450 - val_loss: 0.3802 - val_acc: 0.8550\n","424000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3667 - acc: 0.8706 - val_loss: 0.3523 - val_acc: 0.9000\n","426000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4109 - acc: 0.8567 - val_loss: 0.2936 - val_acc: 0.9050\n","428000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4119 - acc: 0.8661 - val_loss: 0.3266 - val_acc: 0.8750\n","430000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3876 - acc: 0.8633 - val_loss: 0.3074 - val_acc: 0.8750\n","432000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3710 - acc: 0.8689 - val_loss: 0.2652 - val_acc: 0.9100\n","434000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3988 - acc: 0.8611 - val_loss: 0.3891 - val_acc: 0.8550\n","436000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4219 - acc: 0.8500 - val_loss: 0.3485 - val_acc: 0.8750\n","438000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3895 - acc: 0.8622 - val_loss: 0.3094 - val_acc: 0.8650\n","440000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3633 - acc: 0.8672 - val_loss: 0.4135 - val_acc: 0.8550\n","442000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3985 - acc: 0.8594 - val_loss: 0.2933 - val_acc: 0.9000\n","444000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3989 - acc: 0.8611 - val_loss: 0.4240 - val_acc: 0.8650\n","446000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3707 - acc: 0.8706 - val_loss: 0.3673 - val_acc: 0.8850\n","448000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3998 - acc: 0.8572 - val_loss: 0.3310 - val_acc: 0.8550\n","450000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3756 - acc: 0.8656 - val_loss: 0.4231 - val_acc: 0.8600\n","452000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3432 - acc: 0.8772 - val_loss: 0.3830 - val_acc: 0.8500\n","454000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4441 - acc: 0.8478 - val_loss: 0.3417 - val_acc: 0.8800\n","456000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3975 - acc: 0.8600 - val_loss: 0.4469 - val_acc: 0.8250\n","458000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3764 - acc: 0.8539 - val_loss: 0.3176 - val_acc: 0.8750\n","460000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4244 - acc: 0.8533 - val_loss: 0.3452 - val_acc: 0.8750\n","462000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3969 - acc: 0.8578 - val_loss: 0.2956 - val_acc: 0.8900\n","464000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3991 - acc: 0.8483 - val_loss: 0.4383 - val_acc: 0.8350\n","466000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3715 - acc: 0.8694 - val_loss: 0.3488 - val_acc: 0.8800\n","468000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3858 - acc: 0.8556 - val_loss: 0.2153 - val_acc: 0.9250\n","470000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3914 - acc: 0.8650 - val_loss: 0.3488 - val_acc: 0.8900\n","472000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3853 - acc: 0.8667 - val_loss: 0.4550 - val_acc: 0.8550\n","474000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4100 - acc: 0.8617 - val_loss: 0.3570 - val_acc: 0.8600\n","476000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4055 - acc: 0.8589 - val_loss: 0.3847 - val_acc: 0.8400\n","478000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3749 - acc: 0.8667 - val_loss: 0.3844 - val_acc: 0.8550\n","480000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3472 - acc: 0.8767 - val_loss: 0.3664 - val_acc: 0.9000\n","482000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3653 - acc: 0.8689 - val_loss: 0.4692 - val_acc: 0.8150\n","484000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3853 - acc: 0.8667 - val_loss: 0.3327 - val_acc: 0.8750\n","486000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4372 - acc: 0.8461 - val_loss: 0.3396 - val_acc: 0.8700\n","488000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4052 - acc: 0.8489 - val_loss: 0.4251 - val_acc: 0.8250\n","490000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3704 - acc: 0.8672 - val_loss: 0.3952 - val_acc: 0.8550\n","492000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4161 - acc: 0.8561 - val_loss: 0.4640 - val_acc: 0.8350\n","494000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3810 - acc: 0.8667 - val_loss: 0.3211 - val_acc: 0.8850\n","496000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3680 - acc: 0.8806 - val_loss: 0.2650 - val_acc: 0.9050\n","498000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3679 - acc: 0.8683 - val_loss: 0.3084 - val_acc: 0.8750\n","500000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3846 - acc: 0.8583 - val_loss: 0.3858 - val_acc: 0.8600\n","502000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4078 - acc: 0.8611 - val_loss: 0.3623 - val_acc: 0.8250\n","504000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3633 - acc: 0.8706 - val_loss: 0.2370 - val_acc: 0.9200\n","506000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3959 - acc: 0.8550 - val_loss: 0.2914 - val_acc: 0.8900\n","508000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3946 - acc: 0.8633 - val_loss: 0.3704 - val_acc: 0.8600\n","510000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4114 - acc: 0.8544 - val_loss: 0.2650 - val_acc: 0.8950\n","512000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3708 - acc: 0.8711 - val_loss: 0.2735 - val_acc: 0.8850\n","514000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3846 - acc: 0.8639 - val_loss: 0.3709 - val_acc: 0.8700\n","516000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3941 - acc: 0.8639 - val_loss: 0.3353 - val_acc: 0.8800\n","518000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3851 - acc: 0.8500 - val_loss: 0.3805 - val_acc: 0.8700\n","520000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3991 - acc: 0.8511 - val_loss: 0.4163 - val_acc: 0.8600\n","522000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3888 - acc: 0.8628 - val_loss: 0.3773 - val_acc: 0.8750\n","524000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3955 - acc: 0.8567 - val_loss: 0.3660 - val_acc: 0.8550\n","526000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3750 - acc: 0.8717 - val_loss: 0.3691 - val_acc: 0.8750\n","528000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3631 - acc: 0.8606 - val_loss: 0.3398 - val_acc: 0.8850\n","530000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4289 - acc: 0.8500 - val_loss: 0.3975 - val_acc: 0.8700\n","532000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3854 - acc: 0.8706 - val_loss: 0.4138 - val_acc: 0.8700\n","534000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3736 - acc: 0.8644 - val_loss: 0.3350 - val_acc: 0.8650\n","536000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3707 - acc: 0.8700 - val_loss: 0.3413 - val_acc: 0.8500\n","538000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3720 - acc: 0.8628 - val_loss: 0.3466 - val_acc: 0.8700\n","540000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3987 - acc: 0.8489 - val_loss: 0.3597 - val_acc: 0.8650\n","542000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3829 - acc: 0.8628 - val_loss: 0.3673 - val_acc: 0.8650\n","544000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3544 - acc: 0.8806 - val_loss: 0.5004 - val_acc: 0.8050\n","546000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3730 - acc: 0.8733 - val_loss: 0.5001 - val_acc: 0.8100\n","548000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3913 - acc: 0.8639 - val_loss: 0.4254 - val_acc: 0.8550\n","550000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 2ms/step - loss: 0.4122 - acc: 0.8517 - val_loss: 0.4141 - val_acc: 0.8550\n","552000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3494 - acc: 0.8739 - val_loss: 0.3348 - val_acc: 0.8750\n","554000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3709 - acc: 0.8672 - val_loss: 0.4035 - val_acc: 0.8350\n","556000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3767 - acc: 0.8694 - val_loss: 0.3688 - val_acc: 0.8650\n","558000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3837 - acc: 0.8717 - val_loss: 0.4985 - val_acc: 0.7950\n","560000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3594 - acc: 0.8683 - val_loss: 0.3082 - val_acc: 0.8900\n","562000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3764 - acc: 0.8644 - val_loss: 0.3302 - val_acc: 0.8850\n","564000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3662 - acc: 0.8700 - val_loss: 0.4701 - val_acc: 0.8550\n","566000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3859 - acc: 0.8600 - val_loss: 0.3175 - val_acc: 0.8900\n","568000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3929 - acc: 0.8461 - val_loss: 0.3134 - val_acc: 0.8800\n","570000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3664 - acc: 0.8678 - val_loss: 0.4493 - val_acc: 0.8350\n","572000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4082 - acc: 0.8517 - val_loss: 0.2350 - val_acc: 0.9100\n","574000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3782 - acc: 0.8706 - val_loss: 0.3090 - val_acc: 0.8750\n","576000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3832 - acc: 0.8633 - val_loss: 0.2211 - val_acc: 0.9200\n","578000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3697 - acc: 0.8661 - val_loss: 0.3934 - val_acc: 0.8900\n","580000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4016 - acc: 0.8533 - val_loss: 0.2715 - val_acc: 0.9050\n","582000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3581 - acc: 0.8744 - val_loss: 0.3598 - val_acc: 0.8750\n","584000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3972 - acc: 0.8494 - val_loss: 0.3737 - val_acc: 0.8900\n","586000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3455 - acc: 0.8750 - val_loss: 0.2445 - val_acc: 0.9300\n","588000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3884 - acc: 0.8606 - val_loss: 0.3273 - val_acc: 0.8800\n","590000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3834 - acc: 0.8644 - val_loss: 0.3508 - val_acc: 0.8750\n","592000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4051 - acc: 0.8544 - val_loss: 0.3202 - val_acc: 0.8800\n","594000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4064 - acc: 0.8478 - val_loss: 0.3647 - val_acc: 0.8550\n","596000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3740 - acc: 0.8611 - val_loss: 0.3095 - val_acc: 0.8800\n","598000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3902 - acc: 0.8622 - val_loss: 0.4060 - val_acc: 0.8400\n","600000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4047 - acc: 0.8422 - val_loss: 0.3273 - val_acc: 0.8850\n","602000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3952 - acc: 0.8656 - val_loss: 0.3375 - val_acc: 0.8650\n","604000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3773 - acc: 0.8583 - val_loss: 0.3595 - val_acc: 0.8550\n","606000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3954 - acc: 0.8600 - val_loss: 0.3536 - val_acc: 0.8950\n","608000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3867 - acc: 0.8683 - val_loss: 0.3980 - val_acc: 0.8500\n","610000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3907 - acc: 0.8556 - val_loss: 0.3472 - val_acc: 0.8750\n","612000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3657 - acc: 0.8683 - val_loss: 0.3961 - val_acc: 0.8400\n","614000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3768 - acc: 0.8678 - val_loss: 0.2314 - val_acc: 0.9050\n","616000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3531 - acc: 0.8672 - val_loss: 0.3633 - val_acc: 0.8600\n","618000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3613 - acc: 0.8689 - val_loss: 0.2986 - val_acc: 0.8850\n","620000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 2ms/step - loss: 0.3887 - acc: 0.8611 - val_loss: 0.3927 - val_acc: 0.8700\n","622000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3988 - acc: 0.8650 - val_loss: 0.2828 - val_acc: 0.8950\n","624000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3914 - acc: 0.8589 - val_loss: 0.4451 - val_acc: 0.8500\n","626000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3806 - acc: 0.8644 - val_loss: 0.3317 - val_acc: 0.8750\n","628000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3896 - acc: 0.8628 - val_loss: 0.3352 - val_acc: 0.8900\n","630000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3680 - acc: 0.8672 - val_loss: 0.3997 - val_acc: 0.8450\n","632000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3496 - acc: 0.8678 - val_loss: 0.3117 - val_acc: 0.8800\n","634000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3610 - acc: 0.8667 - val_loss: 0.3274 - val_acc: 0.8800\n","636000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3584 - acc: 0.8767 - val_loss: 0.2465 - val_acc: 0.9100\n","638000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3580 - acc: 0.8672 - val_loss: 0.3713 - val_acc: 0.8500\n","640000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3962 - acc: 0.8611 - val_loss: 0.3187 - val_acc: 0.8850\n","642000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3549 - acc: 0.8772 - val_loss: 0.4131 - val_acc: 0.8600\n","644000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3715 - acc: 0.8706 - val_loss: 0.2765 - val_acc: 0.8900\n","646000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3814 - acc: 0.8650 - val_loss: 0.2508 - val_acc: 0.9000\n","648000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4013 - acc: 0.8550 - val_loss: 0.3250 - val_acc: 0.8600\n","650000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3924 - acc: 0.8578 - val_loss: 0.3682 - val_acc: 0.8850\n","652000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3734 - acc: 0.8706 - val_loss: 0.2676 - val_acc: 0.9100\n","654000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3709 - acc: 0.8644 - val_loss: 0.4310 - val_acc: 0.8650\n","656000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3735 - acc: 0.8689 - val_loss: 0.4498 - val_acc: 0.8450\n","658000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3737 - acc: 0.8672 - val_loss: 0.2837 - val_acc: 0.8700\n","660000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4043 - acc: 0.8567 - val_loss: 0.3950 - val_acc: 0.8150\n","662000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3505 - acc: 0.8800 - val_loss: 0.3592 - val_acc: 0.8600\n","664000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3464 - acc: 0.8828 - val_loss: 0.1668 - val_acc: 0.9350\n","666000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3520 - acc: 0.8689 - val_loss: 0.4158 - val_acc: 0.8350\n","668000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3546 - acc: 0.8778 - val_loss: 0.3773 - val_acc: 0.8450\n","670000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3787 - acc: 0.8606 - val_loss: 0.4232 - val_acc: 0.8400\n","672000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3704 - acc: 0.8711 - val_loss: 0.3628 - val_acc: 0.8500\n","674000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4251 - acc: 0.8567 - val_loss: 0.3168 - val_acc: 0.8750\n","676000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4107 - acc: 0.8528 - val_loss: 0.3611 - val_acc: 0.8700\n","678000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3937 - acc: 0.8589 - val_loss: 0.4441 - val_acc: 0.8450\n","680000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3853 - acc: 0.8467 - val_loss: 0.3280 - val_acc: 0.8800\n","682000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3848 - acc: 0.8650 - val_loss: 0.4331 - val_acc: 0.8800\n","684000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3784 - acc: 0.8528 - val_loss: 0.3729 - val_acc: 0.8650\n","686000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3559 - acc: 0.8706 - val_loss: 0.3817 - val_acc: 0.8400\n","688000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3516 - acc: 0.8778 - val_loss: 0.3267 - val_acc: 0.8800\n","690000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3759 - acc: 0.8717 - val_loss: 0.3502 - val_acc: 0.8550\n","692000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3852 - acc: 0.8639 - val_loss: 0.4117 - val_acc: 0.8550\n","694000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3900 - acc: 0.8617 - val_loss: 0.2346 - val_acc: 0.8950\n","696000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3539 - acc: 0.8728 - val_loss: 0.3231 - val_acc: 0.8900\n","698000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3325 - acc: 0.8756 - val_loss: 0.3237 - val_acc: 0.8650\n","700000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3926 - acc: 0.8589 - val_loss: 0.3228 - val_acc: 0.8650\n","702000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3441 - acc: 0.8894 - val_loss: 0.3808 - val_acc: 0.8750\n","704000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3968 - acc: 0.8606 - val_loss: 0.3208 - val_acc: 0.9100\n","706000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3509 - acc: 0.8789 - val_loss: 0.4281 - val_acc: 0.8550\n","708000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3296 - acc: 0.8744 - val_loss: 0.3905 - val_acc: 0.8500\n","710000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3403 - acc: 0.8872 - val_loss: 0.3551 - val_acc: 0.8750\n","712000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3585 - acc: 0.8611 - val_loss: 0.2372 - val_acc: 0.9050\n","714000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3742 - acc: 0.8689 - val_loss: 0.2007 - val_acc: 0.9350\n","716000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3552 - acc: 0.8683 - val_loss: 0.2720 - val_acc: 0.9050\n","718000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3833 - acc: 0.8639 - val_loss: 0.3163 - val_acc: 0.8600\n","720000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3434 - acc: 0.8800 - val_loss: 0.2502 - val_acc: 0.9000\n","722000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3705 - acc: 0.8672 - val_loss: 0.3717 - val_acc: 0.8550\n","724000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3723 - acc: 0.8644 - val_loss: 0.3413 - val_acc: 0.8650\n","726000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3673 - acc: 0.8794 - val_loss: 0.2543 - val_acc: 0.9100\n","728000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3462 - acc: 0.8717 - val_loss: 0.3249 - val_acc: 0.8850\n","730000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3684 - acc: 0.8756 - val_loss: 0.2599 - val_acc: 0.9200\n","732000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3654 - acc: 0.8689 - val_loss: 0.2293 - val_acc: 0.9200\n","734000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3314 - acc: 0.8800 - val_loss: 0.3335 - val_acc: 0.9000\n","736000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3325 - acc: 0.8817 - val_loss: 0.2328 - val_acc: 0.9150\n","738000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3908 - acc: 0.8628 - val_loss: 0.3532 - val_acc: 0.8750\n","740000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3648 - acc: 0.8689 - val_loss: 0.3542 - val_acc: 0.8400\n","742000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3417 - acc: 0.8767 - val_loss: 0.3129 - val_acc: 0.8700\n","744000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3669 - acc: 0.8717 - val_loss: 0.2831 - val_acc: 0.9000\n","746000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3693 - acc: 0.8661 - val_loss: 0.3677 - val_acc: 0.8500\n","748000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3662 - acc: 0.8739 - val_loss: 0.3050 - val_acc: 0.8900\n","750000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3771 - acc: 0.8706 - val_loss: 0.2952 - val_acc: 0.8900\n","752000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3981 - acc: 0.8633 - val_loss: 0.2468 - val_acc: 0.9100\n","754000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3483 - acc: 0.8833 - val_loss: 0.2894 - val_acc: 0.8750\n","756000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4178 - acc: 0.8511 - val_loss: 0.3236 - val_acc: 0.8650\n","758000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3653 - acc: 0.8683 - val_loss: 0.3315 - val_acc: 0.8850\n","760000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3705 - acc: 0.8678 - val_loss: 0.2627 - val_acc: 0.9050\n","762000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3932 - acc: 0.8650 - val_loss: 0.2706 - val_acc: 0.9000\n","764000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3521 - acc: 0.8739 - val_loss: 0.2621 - val_acc: 0.9100\n","766000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3546 - acc: 0.8678 - val_loss: 0.4408 - val_acc: 0.8400\n","768000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3803 - acc: 0.8628 - val_loss: 0.2924 - val_acc: 0.8950\n","770000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3394 - acc: 0.8778 - val_loss: 0.2745 - val_acc: 0.8800\n","772000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3602 - acc: 0.8750 - val_loss: 0.3306 - val_acc: 0.8650\n","774000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3339 - acc: 0.8811 - val_loss: 0.3347 - val_acc: 0.8750\n","776000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3536 - acc: 0.8733 - val_loss: 0.3182 - val_acc: 0.8850\n","778000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3735 - acc: 0.8633 - val_loss: 0.4029 - val_acc: 0.8450\n","780000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3379 - acc: 0.8728 - val_loss: 0.2741 - val_acc: 0.9100\n","782000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3725 - acc: 0.8650 - val_loss: 0.2334 - val_acc: 0.9050\n","784000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3587 - acc: 0.8778 - val_loss: 0.3655 - val_acc: 0.8650\n","786000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3673 - acc: 0.8689 - val_loss: 0.4333 - val_acc: 0.8600\n","788000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3650 - acc: 0.8600 - val_loss: 0.2454 - val_acc: 0.9050\n","790000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3937 - acc: 0.8583 - val_loss: 0.3619 - val_acc: 0.8650\n","792000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3509 - acc: 0.8711 - val_loss: 0.3269 - val_acc: 0.8650\n","794000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3397 - acc: 0.8833 - val_loss: 0.3192 - val_acc: 0.8700\n","796000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3931 - acc: 0.8600 - val_loss: 0.3548 - val_acc: 0.8450\n","798000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3949 - acc: 0.8644 - val_loss: 0.2841 - val_acc: 0.9100\n","800000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3944 - acc: 0.8650 - val_loss: 0.2325 - val_acc: 0.9100\n","802000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3606 - acc: 0.8694 - val_loss: 0.3685 - val_acc: 0.8550\n","804000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3557 - acc: 0.8644 - val_loss: 0.3040 - val_acc: 0.8850\n","806000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3712 - acc: 0.8606 - val_loss: 0.3718 - val_acc: 0.8750\n","808000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3859 - acc: 0.8622 - val_loss: 0.3875 - val_acc: 0.8550\n","810000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4015 - acc: 0.8567 - val_loss: 0.2570 - val_acc: 0.9100\n","812000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4028 - acc: 0.8550 - val_loss: 0.3439 - val_acc: 0.8850\n","814000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3583 - acc: 0.8689 - val_loss: 0.3815 - val_acc: 0.8600\n","816000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3647 - acc: 0.8694 - val_loss: 0.3594 - val_acc: 0.8650\n","818000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3346 - acc: 0.8861 - val_loss: 0.3438 - val_acc: 0.8850\n","820000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3570 - acc: 0.8767 - val_loss: 0.2796 - val_acc: 0.9200\n","822000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3348 - acc: 0.8850 - val_loss: 0.2983 - val_acc: 0.8950\n","824000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3327 - acc: 0.8772 - val_loss: 0.3827 - val_acc: 0.8700\n","826000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3502 - acc: 0.8672 - val_loss: 0.4620 - val_acc: 0.8500\n","828000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3388 - acc: 0.8800 - val_loss: 0.2306 - val_acc: 0.9050\n","830000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3508 - acc: 0.8739 - val_loss: 0.3103 - val_acc: 0.8900\n","832000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3594 - acc: 0.8778 - val_loss: 0.3091 - val_acc: 0.8850\n","834000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3730 - acc: 0.8728 - val_loss: 0.4054 - val_acc: 0.8800\n","836000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3767 - acc: 0.8728 - val_loss: 0.3771 - val_acc: 0.8600\n","838000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3851 - acc: 0.8644 - val_loss: 0.3822 - val_acc: 0.8600\n","840000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3444 - acc: 0.8800 - val_loss: 0.3374 - val_acc: 0.8900\n","842000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3505 - acc: 0.8744 - val_loss: 0.2861 - val_acc: 0.8750\n","844000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3683 - acc: 0.8656 - val_loss: 0.3319 - val_acc: 0.8950\n","846000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3547 - acc: 0.8683 - val_loss: 0.3066 - val_acc: 0.8750\n","848000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3789 - acc: 0.8728 - val_loss: 0.4706 - val_acc: 0.8150\n","850000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3541 - acc: 0.8861 - val_loss: 0.3814 - val_acc: 0.8100\n","852000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3567 - acc: 0.8689 - val_loss: 0.3390 - val_acc: 0.8900\n","854000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3684 - acc: 0.8722 - val_loss: 0.2787 - val_acc: 0.8850\n","856000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3613 - acc: 0.8633 - val_loss: 0.3867 - val_acc: 0.8650\n","858000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3372 - acc: 0.8722 - val_loss: 0.3708 - val_acc: 0.8500\n","860000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3589 - acc: 0.8717 - val_loss: 0.3688 - val_acc: 0.8600\n","862000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3643 - acc: 0.8711 - val_loss: 0.2490 - val_acc: 0.9250\n","864000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4045 - acc: 0.8506 - val_loss: 0.3382 - val_acc: 0.8550\n","866000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3612 - acc: 0.8683 - val_loss: 0.4270 - val_acc: 0.8550\n","868000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3458 - acc: 0.8756 - val_loss: 0.2380 - val_acc: 0.9100\n","870000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3599 - acc: 0.8611 - val_loss: 0.3384 - val_acc: 0.8500\n","872000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3437 - acc: 0.8844 - val_loss: 0.2970 - val_acc: 0.8600\n","874000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3250 - acc: 0.8800 - val_loss: 0.2903 - val_acc: 0.9100\n","876000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3516 - acc: 0.8700 - val_loss: 0.2846 - val_acc: 0.8900\n","878000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3858 - acc: 0.8656 - val_loss: 0.3107 - val_acc: 0.8950\n","880000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3889 - acc: 0.8622 - val_loss: 0.2613 - val_acc: 0.9200\n","882000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3489 - acc: 0.8761 - val_loss: 0.4261 - val_acc: 0.8350\n","884000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3510 - acc: 0.8750 - val_loss: 0.3323 - val_acc: 0.8700\n","886000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3753 - acc: 0.8756 - val_loss: 0.2796 - val_acc: 0.8900\n","888000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3490 - acc: 0.8750 - val_loss: 0.2766 - val_acc: 0.8950\n","890000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3463 - acc: 0.8756 - val_loss: 0.3244 - val_acc: 0.8600\n","892000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3309 - acc: 0.8817 - val_loss: 0.3824 - val_acc: 0.8400\n","894000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3616 - acc: 0.8706 - val_loss: 0.3533 - val_acc: 0.8450\n","896000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3258 - acc: 0.8772 - val_loss: 0.3511 - val_acc: 0.8550\n","898000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3378 - acc: 0.8744 - val_loss: 0.3956 - val_acc: 0.8200\n","900000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3638 - acc: 0.8667 - val_loss: 0.3353 - val_acc: 0.8900\n","902000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3418 - acc: 0.8711 - val_loss: 0.3557 - val_acc: 0.8550\n","904000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3549 - acc: 0.8722 - val_loss: 0.2824 - val_acc: 0.8650\n","906000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3825 - acc: 0.8678 - val_loss: 0.3046 - val_acc: 0.8900\n","908000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3547 - acc: 0.8767 - val_loss: 0.3881 - val_acc: 0.8800\n","910000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3518 - acc: 0.8722 - val_loss: 0.2578 - val_acc: 0.9050\n","912000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3619 - acc: 0.8706 - val_loss: 0.3066 - val_acc: 0.8550\n","914000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3438 - acc: 0.8683 - val_loss: 0.2812 - val_acc: 0.8850\n","916000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3357 - acc: 0.8861 - val_loss: 0.3813 - val_acc: 0.8350\n","918000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3674 - acc: 0.8728 - val_loss: 0.4246 - val_acc: 0.8500\n","920000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.4172 - acc: 0.8511 - val_loss: 0.3151 - val_acc: 0.8800\n","922000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3711 - acc: 0.8756 - val_loss: 0.3542 - val_acc: 0.8750\n","924000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3848 - acc: 0.8550 - val_loss: 0.2571 - val_acc: 0.8900\n","926000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3684 - acc: 0.8617 - val_loss: 0.2861 - val_acc: 0.8900\n","928000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3359 - acc: 0.8867 - val_loss: 0.2386 - val_acc: 0.9250\n","930000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3413 - acc: 0.8761 - val_loss: 0.3278 - val_acc: 0.8700\n","932000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3708 - acc: 0.8589 - val_loss: 0.3396 - val_acc: 0.8450\n","934000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3626 - acc: 0.8794 - val_loss: 0.2875 - val_acc: 0.8900\n","936000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3419 - acc: 0.8700 - val_loss: 0.3002 - val_acc: 0.8950\n","938000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3366 - acc: 0.8817 - val_loss: 0.3795 - val_acc: 0.8800\n","940000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3519 - acc: 0.8717 - val_loss: 0.3509 - val_acc: 0.8900\n","942000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3508 - acc: 0.8589 - val_loss: 0.2712 - val_acc: 0.8900\n","944000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3627 - acc: 0.8700 - val_loss: 0.3349 - val_acc: 0.8600\n","946000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3548 - acc: 0.8706 - val_loss: 0.3035 - val_acc: 0.9050\n","948000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3323 - acc: 0.8806 - val_loss: 0.3896 - val_acc: 0.8700\n","950000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3343 - acc: 0.8722 - val_loss: 0.3067 - val_acc: 0.8950\n","952000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3796 - acc: 0.8656 - val_loss: 0.3523 - val_acc: 0.8550\n","954000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3481 - acc: 0.8733 - val_loss: 0.4120 - val_acc: 0.8650\n","956000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3216 - acc: 0.8833 - val_loss: 0.3357 - val_acc: 0.8650\n","958000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3591 - acc: 0.8700 - val_loss: 0.3150 - val_acc: 0.8900\n","960000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3490 - acc: 0.8644 - val_loss: 0.2595 - val_acc: 0.9050\n","962000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3620 - acc: 0.8678 - val_loss: 0.2735 - val_acc: 0.8950\n","964000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3298 - acc: 0.8817 - val_loss: 0.3327 - val_acc: 0.8850\n","966000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3521 - acc: 0.8744 - val_loss: 0.3694 - val_acc: 0.8550\n","968000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3550 - acc: 0.8700 - val_loss: 0.2842 - val_acc: 0.9050\n","970000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3263 - acc: 0.8733 - val_loss: 0.4408 - val_acc: 0.8350\n","972000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3269 - acc: 0.8844 - val_loss: 0.3039 - val_acc: 0.9000\n","974000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3740 - acc: 0.8694 - val_loss: 0.2985 - val_acc: 0.9050\n","976000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3197 - acc: 0.8872 - val_loss: 0.3756 - val_acc: 0.8450\n","978000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3573 - acc: 0.8733 - val_loss: 0.3692 - val_acc: 0.8650\n","980000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3968 - acc: 0.8561 - val_loss: 0.2851 - val_acc: 0.8950\n","982000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3082 - acc: 0.8939 - val_loss: 0.4231 - val_acc: 0.8750\n","984000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3559 - acc: 0.8772 - val_loss: 0.3052 - val_acc: 0.8950\n","986000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3080 - acc: 0.8822 - val_loss: 0.3096 - val_acc: 0.8850\n","988000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3191 - acc: 0.8872 - val_loss: 0.2949 - val_acc: 0.9250\n","990000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3278 - acc: 0.8783 - val_loss: 0.3426 - val_acc: 0.8800\n","992000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3216 - acc: 0.8839 - val_loss: 0.3232 - val_acc: 0.8800\n","994000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3662 - acc: 0.8656 - val_loss: 0.4704 - val_acc: 0.8400\n","996000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3787 - acc: 0.8589 - val_loss: 0.3037 - val_acc: 0.8800\n","998000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3261 - acc: 0.8789 - val_loss: 0.3061 - val_acc: 0.8900\n","1000000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3798 - acc: 0.8633 - val_loss: 0.4385 - val_acc: 0.8300\n","\n"," Train Loop Number 2\n","2000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3415 - acc: 0.8744 - val_loss: 0.3995 - val_acc: 0.8450\n","4000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3384 - acc: 0.8722 - val_loss: 0.3296 - val_acc: 0.8750\n","6000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3545 - acc: 0.8572 - val_loss: 0.2647 - val_acc: 0.8750\n","8000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3143 - acc: 0.8856 - val_loss: 0.2839 - val_acc: 0.8800\n","10000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3203 - acc: 0.8939 - val_loss: 0.2554 - val_acc: 0.9000\n","12000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3422 - acc: 0.8756 - val_loss: 0.4261 - val_acc: 0.8450\n","14000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3349 - acc: 0.8739 - val_loss: 0.3467 - val_acc: 0.8800\n","16000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3406 - acc: 0.8817 - val_loss: 0.3431 - val_acc: 0.8800\n","18000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3465 - acc: 0.8717 - val_loss: 0.3105 - val_acc: 0.8800\n","20000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3655 - acc: 0.8650 - val_loss: 0.3174 - val_acc: 0.8950\n","22000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3473 - acc: 0.8711 - val_loss: 0.2987 - val_acc: 0.8600\n","24000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3201 - acc: 0.8878 - val_loss: 0.3582 - val_acc: 0.8700\n","26000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3552 - acc: 0.8839 - val_loss: 0.3488 - val_acc: 0.8600\n","28000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3175 - acc: 0.8839 - val_loss: 0.2545 - val_acc: 0.9050\n","30000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3626 - acc: 0.8728 - val_loss: 0.2670 - val_acc: 0.8950\n","32000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3560 - acc: 0.8761 - val_loss: 0.2665 - val_acc: 0.8900\n","34000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3381 - acc: 0.8822 - val_loss: 0.4140 - val_acc: 0.8400\n","36000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3446 - acc: 0.8767 - val_loss: 0.3034 - val_acc: 0.8950\n","38000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3643 - acc: 0.8722 - val_loss: 0.3384 - val_acc: 0.8400\n","40000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3760 - acc: 0.8700 - val_loss: 0.3261 - val_acc: 0.8800\n","42000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3526 - acc: 0.8706 - val_loss: 0.2952 - val_acc: 0.9100\n","44000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3645 - acc: 0.8644 - val_loss: 0.2339 - val_acc: 0.9150\n","46000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3402 - acc: 0.8750 - val_loss: 0.3441 - val_acc: 0.8900\n","48000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3700 - acc: 0.8672 - val_loss: 0.2908 - val_acc: 0.8900\n","50000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3209 - acc: 0.8861 - val_loss: 0.3643 - val_acc: 0.8800\n","52000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3813 - acc: 0.8683 - val_loss: 0.3244 - val_acc: 0.8800\n","54000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3047 - acc: 0.8894 - val_loss: 0.3126 - val_acc: 0.8850\n","56000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2972 - acc: 0.8950 - val_loss: 0.4090 - val_acc: 0.8300\n","58000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3530 - acc: 0.8717 - val_loss: 0.2960 - val_acc: 0.9000\n","60000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3540 - acc: 0.8650 - val_loss: 0.4003 - val_acc: 0.8250\n","62000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3880 - acc: 0.8656 - val_loss: 0.2377 - val_acc: 0.8950\n","64000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3591 - acc: 0.8694 - val_loss: 0.3596 - val_acc: 0.8650\n","66000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3479 - acc: 0.8794 - val_loss: 0.3027 - val_acc: 0.8900\n","68000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3293 - acc: 0.8817 - val_loss: 0.3490 - val_acc: 0.8650\n","70000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3627 - acc: 0.8639 - val_loss: 0.3515 - val_acc: 0.8650\n","72000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3081 - acc: 0.8867 - val_loss: 0.2149 - val_acc: 0.9200\n","74000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3350 - acc: 0.8861 - val_loss: 0.2174 - val_acc: 0.9100\n","76000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3409 - acc: 0.8711 - val_loss: 0.4376 - val_acc: 0.8450\n","78000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3702 - acc: 0.8650 - val_loss: 0.3629 - val_acc: 0.8800\n","80000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3202 - acc: 0.8844 - val_loss: 0.2838 - val_acc: 0.8850\n","82000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3536 - acc: 0.8794 - val_loss: 0.2482 - val_acc: 0.9000\n","84000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2921 - acc: 0.8961 - val_loss: 0.2265 - val_acc: 0.9300\n","86000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3240 - acc: 0.8811 - val_loss: 0.2312 - val_acc: 0.9100\n","88000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3408 - acc: 0.8728 - val_loss: 0.2723 - val_acc: 0.9050\n","90000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3035 - acc: 0.8939 - val_loss: 0.2258 - val_acc: 0.9350\n","92000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3393 - acc: 0.8828 - val_loss: 0.4046 - val_acc: 0.8350\n","94000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3886 - acc: 0.8633 - val_loss: 0.3530 - val_acc: 0.8600\n","96000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3395 - acc: 0.8756 - val_loss: 0.3248 - val_acc: 0.8800\n","98000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3656 - acc: 0.8678 - val_loss: 0.3404 - val_acc: 0.8600\n","100000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3336 - acc: 0.8828 - val_loss: 0.3764 - val_acc: 0.8750\n","102000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3766 - acc: 0.8661 - val_loss: 0.2541 - val_acc: 0.9200\n","104000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3341 - acc: 0.8794 - val_loss: 0.2415 - val_acc: 0.8900\n","106000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3761 - acc: 0.8683 - val_loss: 0.3111 - val_acc: 0.8750\n","108000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3247 - acc: 0.8894 - val_loss: 0.4352 - val_acc: 0.8450\n","110000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3517 - acc: 0.8767 - val_loss: 0.3158 - val_acc: 0.8750\n","112000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3212 - acc: 0.8872 - val_loss: 0.3095 - val_acc: 0.8750\n","114000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3327 - acc: 0.8772 - val_loss: 0.2649 - val_acc: 0.9000\n","116000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3419 - acc: 0.8778 - val_loss: 0.3511 - val_acc: 0.8500\n","118000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3018 - acc: 0.8861 - val_loss: 0.3061 - val_acc: 0.8950\n","120000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3587 - acc: 0.8761 - val_loss: 0.3842 - val_acc: 0.8650\n","122000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3586 - acc: 0.8678 - val_loss: 0.3370 - val_acc: 0.8700\n","124000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3260 - acc: 0.8767 - val_loss: 0.3013 - val_acc: 0.8850\n","126000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3188 - acc: 0.8800 - val_loss: 0.3347 - val_acc: 0.8600\n","128000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3395 - acc: 0.8628 - val_loss: 0.2861 - val_acc: 0.8950\n","130000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3651 - acc: 0.8772 - val_loss: 0.3956 - val_acc: 0.8800\n","132000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3569 - acc: 0.8644 - val_loss: 0.3036 - val_acc: 0.9100\n","134000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3192 - acc: 0.8822 - val_loss: 0.2228 - val_acc: 0.9100\n","136000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3194 - acc: 0.8789 - val_loss: 0.2827 - val_acc: 0.8700\n","138000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3470 - acc: 0.8800 - val_loss: 0.3435 - val_acc: 0.8650\n","140000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3854 - acc: 0.8656 - val_loss: 0.2958 - val_acc: 0.8850\n","142000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3367 - acc: 0.8756 - val_loss: 0.3310 - val_acc: 0.8550\n","144000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3508 - acc: 0.8683 - val_loss: 0.2412 - val_acc: 0.9350\n","146000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3354 - acc: 0.8800 - val_loss: 0.3070 - val_acc: 0.8900\n","148000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3517 - acc: 0.8789 - val_loss: 0.2719 - val_acc: 0.9150\n","150000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3410 - acc: 0.8772 - val_loss: 0.4257 - val_acc: 0.8750\n","152000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3441 - acc: 0.8744 - val_loss: 0.2780 - val_acc: 0.9000\n","154000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3442 - acc: 0.8783 - val_loss: 0.3259 - val_acc: 0.8850\n","156000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3344 - acc: 0.8844 - val_loss: 0.3078 - val_acc: 0.9000\n","158000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3259 - acc: 0.8800 - val_loss: 0.2941 - val_acc: 0.8750\n","160000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3222 - acc: 0.8839 - val_loss: 0.3868 - val_acc: 0.8650\n","162000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3337 - acc: 0.8761 - val_loss: 0.2973 - val_acc: 0.9150\n","164000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3274 - acc: 0.8811 - val_loss: 0.3294 - val_acc: 0.8700\n","166000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3355 - acc: 0.8828 - val_loss: 0.2699 - val_acc: 0.9000\n","168000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3461 - acc: 0.8706 - val_loss: 0.3240 - val_acc: 0.8550\n","170000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3991 - acc: 0.8517 - val_loss: 0.3284 - val_acc: 0.8850\n","172000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3123 - acc: 0.8828 - val_loss: 0.3773 - val_acc: 0.8700\n","174000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3557 - acc: 0.8794 - val_loss: 0.2773 - val_acc: 0.9100\n","176000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3444 - acc: 0.8794 - val_loss: 0.3297 - val_acc: 0.8850\n","178000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3509 - acc: 0.8761 - val_loss: 0.3012 - val_acc: 0.8750\n","180000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3203 - acc: 0.8700 - val_loss: 0.2678 - val_acc: 0.8950\n","182000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3323 - acc: 0.8783 - val_loss: 0.2861 - val_acc: 0.9000\n","184000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3077 - acc: 0.8867 - val_loss: 0.2712 - val_acc: 0.9100\n","186000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3622 - acc: 0.8706 - val_loss: 0.2594 - val_acc: 0.8850\n","188000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3399 - acc: 0.8772 - val_loss: 0.3061 - val_acc: 0.9100\n","190000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3422 - acc: 0.8867 - val_loss: 0.2483 - val_acc: 0.9250\n","192000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3558 - acc: 0.8733 - val_loss: 0.2864 - val_acc: 0.8900\n","194000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3347 - acc: 0.8850 - val_loss: 0.2145 - val_acc: 0.9200\n","196000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3517 - acc: 0.8794 - val_loss: 0.3737 - val_acc: 0.8700\n","198000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3635 - acc: 0.8644 - val_loss: 0.2418 - val_acc: 0.9150\n","200000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3137 - acc: 0.8828 - val_loss: 0.2616 - val_acc: 0.9150\n","202000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3165 - acc: 0.8883 - val_loss: 0.2563 - val_acc: 0.9100\n","204000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3508 - acc: 0.8811 - val_loss: 0.2440 - val_acc: 0.9200\n","206000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3237 - acc: 0.8833 - val_loss: 0.2585 - val_acc: 0.9200\n","208000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3285 - acc: 0.8844 - val_loss: 0.3634 - val_acc: 0.8650\n","210000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3469 - acc: 0.8800 - val_loss: 0.2595 - val_acc: 0.9000\n","212000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3535 - acc: 0.8783 - val_loss: 0.3415 - val_acc: 0.8650\n","214000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3448 - acc: 0.8694 - val_loss: 0.2264 - val_acc: 0.9300\n","216000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3150 - acc: 0.8867 - val_loss: 0.3737 - val_acc: 0.8600\n","218000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3729 - acc: 0.8628 - val_loss: 0.3854 - val_acc: 0.8450\n","220000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3414 - acc: 0.8744 - val_loss: 0.3555 - val_acc: 0.9000\n","222000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3193 - acc: 0.8850 - val_loss: 0.3656 - val_acc: 0.8750\n","224000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3007 - acc: 0.8894 - val_loss: 0.3011 - val_acc: 0.8800\n","226000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3211 - acc: 0.8750 - val_loss: 0.2759 - val_acc: 0.9150\n","228000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3477 - acc: 0.8756 - val_loss: 0.3319 - val_acc: 0.8300\n","230000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3062 - acc: 0.8983 - val_loss: 0.2970 - val_acc: 0.8950\n","232000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3138 - acc: 0.8844 - val_loss: 0.2777 - val_acc: 0.8850\n","234000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3177 - acc: 0.8878 - val_loss: 0.3046 - val_acc: 0.8850\n","236000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3271 - acc: 0.8883 - val_loss: 0.3279 - val_acc: 0.8800\n","238000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3351 - acc: 0.8767 - val_loss: 0.3595 - val_acc: 0.8750\n","240000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3325 - acc: 0.8756 - val_loss: 0.3638 - val_acc: 0.8600\n","242000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3084 - acc: 0.8833 - val_loss: 0.3889 - val_acc: 0.8550\n","244000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3166 - acc: 0.8878 - val_loss: 0.2375 - val_acc: 0.9150\n","246000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3649 - acc: 0.8750 - val_loss: 0.3125 - val_acc: 0.8850\n","248000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3225 - acc: 0.8900 - val_loss: 0.3807 - val_acc: 0.8500\n","250000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3132 - acc: 0.8861 - val_loss: 0.3328 - val_acc: 0.9000\n","252000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3525 - acc: 0.8728 - val_loss: 0.2497 - val_acc: 0.9250\n","254000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3327 - acc: 0.8789 - val_loss: 0.3249 - val_acc: 0.8650\n","256000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3305 - acc: 0.8833 - val_loss: 0.2081 - val_acc: 0.9150\n","258000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3396 - acc: 0.8772 - val_loss: 0.2463 - val_acc: 0.8850\n","260000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3357 - acc: 0.8839 - val_loss: 0.3780 - val_acc: 0.8650\n","262000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3785 - acc: 0.8672 - val_loss: 0.3331 - val_acc: 0.8450\n","264000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3531 - acc: 0.8739 - val_loss: 0.3557 - val_acc: 0.8650\n","266000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2996 - acc: 0.8906 - val_loss: 0.2612 - val_acc: 0.9000\n","268000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3145 - acc: 0.8917 - val_loss: 0.3090 - val_acc: 0.8900\n","270000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3462 - acc: 0.8772 - val_loss: 0.3193 - val_acc: 0.8900\n","272000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3456 - acc: 0.8767 - val_loss: 0.3397 - val_acc: 0.9000\n","274000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3374 - acc: 0.8839 - val_loss: 0.2053 - val_acc: 0.9350\n","276000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3358 - acc: 0.8817 - val_loss: 0.3353 - val_acc: 0.8850\n","278000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3337 - acc: 0.8794 - val_loss: 0.2998 - val_acc: 0.9050\n","280000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3224 - acc: 0.8883 - val_loss: 0.3330 - val_acc: 0.8950\n","282000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3318 - acc: 0.8678 - val_loss: 0.3326 - val_acc: 0.8700\n","284000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3528 - acc: 0.8744 - val_loss: 0.2918 - val_acc: 0.8900\n","286000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3449 - acc: 0.8767 - val_loss: 0.3085 - val_acc: 0.9050\n","288000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3097 - acc: 0.8861 - val_loss: 0.3255 - val_acc: 0.8750\n","290000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3512 - acc: 0.8717 - val_loss: 0.2971 - val_acc: 0.8950\n","292000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3412 - acc: 0.8750 - val_loss: 0.3367 - val_acc: 0.8700\n","294000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3506 - acc: 0.8789 - val_loss: 0.2088 - val_acc: 0.9150\n","296000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3390 - acc: 0.8750 - val_loss: 0.3764 - val_acc: 0.8450\n","298000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2994 - acc: 0.8878 - val_loss: 0.2945 - val_acc: 0.8900\n","300000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3526 - acc: 0.8833 - val_loss: 0.2152 - val_acc: 0.9200\n","302000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3272 - acc: 0.8739 - val_loss: 0.2955 - val_acc: 0.8700\n","304000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3134 - acc: 0.8900 - val_loss: 0.3634 - val_acc: 0.8750\n","306000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3175 - acc: 0.8839 - val_loss: 0.2243 - val_acc: 0.9100\n","308000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3218 - acc: 0.8833 - val_loss: 0.3507 - val_acc: 0.8450\n","310000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3451 - acc: 0.8806 - val_loss: 0.3023 - val_acc: 0.8850\n","312000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3025 - acc: 0.8861 - val_loss: 0.2935 - val_acc: 0.9000\n","314000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3304 - acc: 0.8872 - val_loss: 0.4028 - val_acc: 0.8600\n","316000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3240 - acc: 0.8844 - val_loss: 0.2873 - val_acc: 0.8850\n","318000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3348 - acc: 0.8761 - val_loss: 0.3369 - val_acc: 0.8700\n","320000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3581 - acc: 0.8761 - val_loss: 0.4128 - val_acc: 0.8600\n","322000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3324 - acc: 0.8778 - val_loss: 0.3669 - val_acc: 0.8550\n","324000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3347 - acc: 0.8772 - val_loss: 0.3213 - val_acc: 0.8650\n","326000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3288 - acc: 0.8861 - val_loss: 0.2746 - val_acc: 0.9050\n","328000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3004 - acc: 0.8922 - val_loss: 0.3157 - val_acc: 0.8950\n","330000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3263 - acc: 0.8822 - val_loss: 0.2493 - val_acc: 0.9200\n","332000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3206 - acc: 0.8889 - val_loss: 0.3665 - val_acc: 0.8600\n","334000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3114 - acc: 0.8878 - val_loss: 0.3026 - val_acc: 0.8750\n","336000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3439 - acc: 0.8867 - val_loss: 0.2175 - val_acc: 0.9100\n","338000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3303 - acc: 0.8806 - val_loss: 0.2940 - val_acc: 0.8700\n","340000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3391 - acc: 0.8728 - val_loss: 0.3446 - val_acc: 0.8750\n","342000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3111 - acc: 0.8872 - val_loss: 0.3263 - val_acc: 0.8900\n","344000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3261 - acc: 0.8733 - val_loss: 0.2825 - val_acc: 0.8850\n","346000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3483 - acc: 0.8683 - val_loss: 0.3542 - val_acc: 0.8750\n","348000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3236 - acc: 0.8789 - val_loss: 0.3510 - val_acc: 0.8650\n","350000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3510 - acc: 0.8722 - val_loss: 0.3888 - val_acc: 0.8600\n","352000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.2844 - val_acc: 0.8950\n","354000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3309 - acc: 0.8850 - val_loss: 0.2847 - val_acc: 0.8950\n","356000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3303 - acc: 0.8772 - val_loss: 0.3528 - val_acc: 0.8800\n","358000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3344 - acc: 0.8811 - val_loss: 0.3471 - val_acc: 0.8800\n","360000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3283 - acc: 0.8806 - val_loss: 0.3049 - val_acc: 0.9050\n","362000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3469 - acc: 0.8667 - val_loss: 0.2606 - val_acc: 0.9100\n","364000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3358 - acc: 0.8811 - val_loss: 0.2528 - val_acc: 0.8900\n","366000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2937 - acc: 0.8883 - val_loss: 0.2872 - val_acc: 0.8900\n","368000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3130 - acc: 0.8844 - val_loss: 0.3721 - val_acc: 0.8700\n","370000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3004 - acc: 0.9022 - val_loss: 0.2720 - val_acc: 0.9000\n","372000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3291 - acc: 0.8794 - val_loss: 0.3337 - val_acc: 0.8700\n","374000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3531 - acc: 0.8717 - val_loss: 0.3011 - val_acc: 0.8900\n","376000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3237 - acc: 0.8822 - val_loss: 0.3695 - val_acc: 0.8650\n","378000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3142 - acc: 0.8811 - val_loss: 0.2006 - val_acc: 0.9400\n","380000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3320 - acc: 0.8906 - val_loss: 0.3324 - val_acc: 0.8650\n","382000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3278 - acc: 0.8772 - val_loss: 0.2221 - val_acc: 0.9150\n","384000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2920 - acc: 0.8917 - val_loss: 0.2519 - val_acc: 0.9050\n","386000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3274 - acc: 0.8867 - val_loss: 0.2890 - val_acc: 0.9000\n","388000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3390 - acc: 0.8822 - val_loss: 0.2552 - val_acc: 0.9050\n","390000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2726 - acc: 0.8983 - val_loss: 0.2397 - val_acc: 0.8950\n","392000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3187 - acc: 0.8833 - val_loss: 0.2307 - val_acc: 0.9100\n","394000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3130 - acc: 0.8822 - val_loss: 0.3561 - val_acc: 0.8550\n","396000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3388 - acc: 0.8772 - val_loss: 0.3853 - val_acc: 0.8650\n","398000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3259 - acc: 0.8817 - val_loss: 0.3159 - val_acc: 0.8600\n","400000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2883 - acc: 0.9006 - val_loss: 0.2386 - val_acc: 0.9150\n","402000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2806 - acc: 0.9028 - val_loss: 0.3439 - val_acc: 0.8700\n","404000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3554 - acc: 0.8756 - val_loss: 0.3555 - val_acc: 0.8750\n","406000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3413 - acc: 0.8794 - val_loss: 0.3311 - val_acc: 0.9000\n","408000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3082 - acc: 0.8894 - val_loss: 0.2740 - val_acc: 0.9050\n","410000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3460 - acc: 0.8761 - val_loss: 0.2650 - val_acc: 0.9200\n","412000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3347 - acc: 0.8817 - val_loss: 0.3221 - val_acc: 0.8800\n","414000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3193 - acc: 0.8867 - val_loss: 0.2817 - val_acc: 0.8950\n","416000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3166 - acc: 0.8856 - val_loss: 0.3568 - val_acc: 0.8600\n","418000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3281 - acc: 0.8800 - val_loss: 0.3581 - val_acc: 0.8700\n","420000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3472 - acc: 0.8778 - val_loss: 0.2491 - val_acc: 0.9000\n","422000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3097 - acc: 0.8872 - val_loss: 0.2481 - val_acc: 0.9100\n","424000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3318 - acc: 0.8839 - val_loss: 0.3018 - val_acc: 0.8750\n","426000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3199 - acc: 0.8806 - val_loss: 0.3290 - val_acc: 0.9050\n","428000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3332 - acc: 0.8806 - val_loss: 0.2375 - val_acc: 0.9150\n","430000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3190 - acc: 0.8833 - val_loss: 0.2265 - val_acc: 0.8800\n","432000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3340 - acc: 0.8778 - val_loss: 0.3666 - val_acc: 0.8600\n","434000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3101 - acc: 0.8922 - val_loss: 0.3564 - val_acc: 0.8650\n","436000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2963 - acc: 0.8861 - val_loss: 0.3148 - val_acc: 0.9000\n","438000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3437 - acc: 0.8867 - val_loss: 0.2347 - val_acc: 0.9050\n","440000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3220 - acc: 0.8828 - val_loss: 0.3963 - val_acc: 0.8550\n","442000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3290 - acc: 0.8861 - val_loss: 0.2072 - val_acc: 0.9250\n","444000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3106 - acc: 0.8872 - val_loss: 0.3792 - val_acc: 0.8650\n","446000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3455 - acc: 0.8811 - val_loss: 0.3170 - val_acc: 0.8800\n","448000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3056 - acc: 0.8878 - val_loss: 0.4071 - val_acc: 0.8450\n","450000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3607 - acc: 0.8683 - val_loss: 0.3980 - val_acc: 0.8550\n","452000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3097 - acc: 0.8956 - val_loss: 0.2414 - val_acc: 0.9100\n","454000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3302 - acc: 0.8822 - val_loss: 0.2924 - val_acc: 0.8700\n","456000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3319 - acc: 0.8856 - val_loss: 0.2772 - val_acc: 0.9100\n","458000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3158 - acc: 0.8944 - val_loss: 0.2502 - val_acc: 0.9200\n","460000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3169 - acc: 0.8894 - val_loss: 0.2333 - val_acc: 0.9050\n","462000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2967 - acc: 0.8906 - val_loss: 0.2528 - val_acc: 0.8950\n","464000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3352 - acc: 0.8767 - val_loss: 0.2381 - val_acc: 0.9250\n","466000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3221 - acc: 0.8844 - val_loss: 0.3154 - val_acc: 0.8700\n","468000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2792 - acc: 0.8989 - val_loss: 0.2475 - val_acc: 0.8900\n","470000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2964 - acc: 0.8944 - val_loss: 0.2924 - val_acc: 0.8950\n","472000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3312 - acc: 0.8906 - val_loss: 0.3412 - val_acc: 0.8700\n","474000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3513 - acc: 0.8750 - val_loss: 0.3630 - val_acc: 0.8450\n","476000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3321 - acc: 0.8694 - val_loss: 0.3108 - val_acc: 0.8750\n","478000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2899 - acc: 0.8978 - val_loss: 0.2205 - val_acc: 0.9300\n","480000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3266 - acc: 0.8922 - val_loss: 0.4234 - val_acc: 0.8450\n","482000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3090 - acc: 0.8867 - val_loss: 0.3421 - val_acc: 0.8550\n","484000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3494 - acc: 0.8833 - val_loss: 0.2499 - val_acc: 0.9100\n","486000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3185 - acc: 0.8778 - val_loss: 0.4029 - val_acc: 0.8500\n","488000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3522 - acc: 0.8706 - val_loss: 0.2853 - val_acc: 0.8750\n","490000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2856 - acc: 0.9022 - val_loss: 0.2789 - val_acc: 0.8900\n","492000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3094 - acc: 0.8844 - val_loss: 0.3126 - val_acc: 0.8900\n","494000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3314 - acc: 0.8817 - val_loss: 0.2815 - val_acc: 0.8850\n","496000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3096 - acc: 0.8756 - val_loss: 0.2596 - val_acc: 0.9000\n","498000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3146 - acc: 0.8933 - val_loss: 0.3810 - val_acc: 0.8650\n","500000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3171 - acc: 0.8794 - val_loss: 0.2043 - val_acc: 0.9250\n","502000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2789 - acc: 0.8989 - val_loss: 0.3565 - val_acc: 0.8550\n","504000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3354 - acc: 0.8800 - val_loss: 0.3236 - val_acc: 0.9050\n","506000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3257 - acc: 0.8761 - val_loss: 0.3253 - val_acc: 0.8900\n","508000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3215 - acc: 0.8861 - val_loss: 0.2712 - val_acc: 0.8950\n","510000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3282 - acc: 0.8772 - val_loss: 0.3701 - val_acc: 0.8650\n","512000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3334 - acc: 0.8850 - val_loss: 0.3370 - val_acc: 0.8600\n","514000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3029 - acc: 0.8889 - val_loss: 0.3130 - val_acc: 0.8650\n","516000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3264 - acc: 0.8828 - val_loss: 0.3006 - val_acc: 0.8700\n","518000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3273 - acc: 0.8833 - val_loss: 0.2695 - val_acc: 0.8950\n","520000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3267 - acc: 0.8889 - val_loss: 0.3759 - val_acc: 0.8750\n","522000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2970 - acc: 0.8872 - val_loss: 0.3156 - val_acc: 0.8700\n","524000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3520 - acc: 0.8800 - val_loss: 0.2421 - val_acc: 0.9250\n","526000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3187 - acc: 0.8861 - val_loss: 0.2558 - val_acc: 0.9150\n","528000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3423 - acc: 0.8767 - val_loss: 0.2833 - val_acc: 0.9000\n","530000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3140 - acc: 0.8828 - val_loss: 0.2550 - val_acc: 0.9200\n","532000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3312 - acc: 0.8850 - val_loss: 0.2933 - val_acc: 0.9000\n","534000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2991 - acc: 0.8950 - val_loss: 0.2481 - val_acc: 0.9150\n","536000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3182 - acc: 0.8861 - val_loss: 0.2961 - val_acc: 0.8950\n","538000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3169 - acc: 0.8822 - val_loss: 0.2153 - val_acc: 0.9200\n","540000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3001 - acc: 0.8967 - val_loss: 0.3007 - val_acc: 0.8950\n","542000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3233 - acc: 0.8839 - val_loss: 0.3197 - val_acc: 0.8750\n","544000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3221 - acc: 0.8844 - val_loss: 0.2444 - val_acc: 0.9050\n","546000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3430 - acc: 0.8789 - val_loss: 0.2340 - val_acc: 0.9300\n","548000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3279 - acc: 0.8850 - val_loss: 0.3274 - val_acc: 0.8850\n","550000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3563 - acc: 0.8722 - val_loss: 0.2650 - val_acc: 0.9050\n","552000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3055 - acc: 0.8872 - val_loss: 0.2853 - val_acc: 0.8750\n","554000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3288 - acc: 0.8772 - val_loss: 0.3017 - val_acc: 0.8700\n","556000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3414 - acc: 0.8794 - val_loss: 0.2185 - val_acc: 0.9150\n","558000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3121 - acc: 0.8950 - val_loss: 0.2319 - val_acc: 0.9200\n","560000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3630 - acc: 0.8711 - val_loss: 0.2521 - val_acc: 0.8950\n","562000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2976 - acc: 0.8972 - val_loss: 0.4992 - val_acc: 0.8350\n","564000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3030 - acc: 0.8844 - val_loss: 0.2649 - val_acc: 0.9200\n","566000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2958 - acc: 0.8856 - val_loss: 0.2485 - val_acc: 0.9100\n","568000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3254 - acc: 0.8817 - val_loss: 0.2523 - val_acc: 0.9050\n","570000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3368 - acc: 0.8733 - val_loss: 0.2759 - val_acc: 0.8850\n","572000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3099 - acc: 0.8906 - val_loss: 0.2923 - val_acc: 0.8900\n","574000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3087 - acc: 0.8817 - val_loss: 0.3580 - val_acc: 0.8650\n","576000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3241 - acc: 0.8817 - val_loss: 0.2523 - val_acc: 0.9000\n","578000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3126 - acc: 0.8906 - val_loss: 0.2593 - val_acc: 0.8950\n","580000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3155 - acc: 0.8811 - val_loss: 0.3093 - val_acc: 0.8900\n","582000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2971 - acc: 0.8911 - val_loss: 0.3444 - val_acc: 0.8500\n","584000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3642 - acc: 0.8661 - val_loss: 0.3165 - val_acc: 0.8750\n","586000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3113 - acc: 0.8889 - val_loss: 0.2996 - val_acc: 0.9150\n","588000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2974 - acc: 0.8906 - val_loss: 0.3131 - val_acc: 0.8750\n","590000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3311 - acc: 0.8833 - val_loss: 0.2925 - val_acc: 0.9100\n","592000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3009 - acc: 0.8911 - val_loss: 0.3321 - val_acc: 0.8900\n","594000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3301 - acc: 0.8833 - val_loss: 0.3311 - val_acc: 0.8700\n","596000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3262 - acc: 0.8778 - val_loss: 0.2608 - val_acc: 0.9100\n","598000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3137 - acc: 0.8856 - val_loss: 0.2573 - val_acc: 0.9000\n","600000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3176 - acc: 0.8806 - val_loss: 0.3619 - val_acc: 0.8450\n","602000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3404 - acc: 0.8733 - val_loss: 0.2838 - val_acc: 0.8600\n","604000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3251 - acc: 0.8900 - val_loss: 0.2228 - val_acc: 0.8950\n","606000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3172 - acc: 0.8878 - val_loss: 0.3098 - val_acc: 0.8950\n","608000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3547 - acc: 0.8694 - val_loss: 0.2196 - val_acc: 0.9250\n","610000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3005 - acc: 0.8872 - val_loss: 0.3299 - val_acc: 0.8750\n","612000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2939 - acc: 0.8867 - val_loss: 0.2412 - val_acc: 0.8800\n","614000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3187 - acc: 0.8867 - val_loss: 0.2277 - val_acc: 0.9200\n","616000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3145 - acc: 0.8778 - val_loss: 0.3324 - val_acc: 0.8600\n","618000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2983 - acc: 0.8889 - val_loss: 0.3163 - val_acc: 0.8800\n","620000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3123 - acc: 0.8922 - val_loss: 0.2748 - val_acc: 0.8800\n","622000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3204 - acc: 0.8856 - val_loss: 0.3172 - val_acc: 0.8800\n","624000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2890 - acc: 0.8917 - val_loss: 0.3226 - val_acc: 0.8700\n","626000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3634 - acc: 0.8717 - val_loss: 0.2441 - val_acc: 0.9150\n","628000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2971 - acc: 0.8911 - val_loss: 0.3020 - val_acc: 0.8950\n","630000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3546 - acc: 0.8722 - val_loss: 0.2525 - val_acc: 0.8900\n","632000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3421 - acc: 0.8783 - val_loss: 0.2661 - val_acc: 0.8950\n","634000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3530 - acc: 0.8667 - val_loss: 0.2023 - val_acc: 0.9200\n","636000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3624 - acc: 0.8778 - val_loss: 0.3434 - val_acc: 0.8600\n","638000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3018 - acc: 0.8939 - val_loss: 0.3168 - val_acc: 0.8900\n","640000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3060 - acc: 0.8956 - val_loss: 0.2631 - val_acc: 0.9050\n","642000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2915 - acc: 0.8944 - val_loss: 0.2585 - val_acc: 0.8700\n","644000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2982 - acc: 0.8911 - val_loss: 0.2430 - val_acc: 0.9000\n","646000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2915 - acc: 0.8950 - val_loss: 0.2557 - val_acc: 0.9050\n","648000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2859 - acc: 0.8933 - val_loss: 0.2598 - val_acc: 0.8850\n","650000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2968 - acc: 0.8944 - val_loss: 0.2875 - val_acc: 0.9000\n","652000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2661 - acc: 0.9011 - val_loss: 0.2984 - val_acc: 0.8950\n","654000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2943 - acc: 0.8994 - val_loss: 0.3193 - val_acc: 0.8800\n","656000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2972 - acc: 0.8989 - val_loss: 0.2200 - val_acc: 0.9250\n","658000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2921 - acc: 0.8928 - val_loss: 0.2277 - val_acc: 0.9250\n","660000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3025 - acc: 0.8878 - val_loss: 0.2850 - val_acc: 0.8950\n","662000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3179 - acc: 0.8900 - val_loss: 0.2966 - val_acc: 0.8950\n","664000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3451 - acc: 0.8750 - val_loss: 0.3105 - val_acc: 0.8750\n","666000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3569 - acc: 0.8761 - val_loss: 0.2457 - val_acc: 0.9200\n","668000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3170 - acc: 0.8894 - val_loss: 0.3048 - val_acc: 0.9000\n","670000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2751 - acc: 0.8956 - val_loss: 0.2874 - val_acc: 0.8750\n","672000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3199 - acc: 0.8872 - val_loss: 0.1893 - val_acc: 0.9400\n","674000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3471 - acc: 0.8850 - val_loss: 0.2982 - val_acc: 0.8950\n","676000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3395 - acc: 0.8689 - val_loss: 0.3494 - val_acc: 0.8650\n","678000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3290 - acc: 0.8850 - val_loss: 0.2640 - val_acc: 0.9050\n","680000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3007 - acc: 0.8967 - val_loss: 0.2724 - val_acc: 0.8900\n","682000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2829 - acc: 0.8994 - val_loss: 0.2692 - val_acc: 0.9100\n","684000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2603 - acc: 0.8967 - val_loss: 0.2694 - val_acc: 0.9100\n","686000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2986 - acc: 0.8933 - val_loss: 0.3086 - val_acc: 0.8750\n","688000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3379 - acc: 0.8783 - val_loss: 0.2373 - val_acc: 0.9100\n","690000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3299 - acc: 0.8772 - val_loss: 0.2477 - val_acc: 0.9150\n","692000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3127 - acc: 0.8856 - val_loss: 0.2963 - val_acc: 0.8800\n","694000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3320 - acc: 0.8706 - val_loss: 0.3648 - val_acc: 0.8350\n","696000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3032 - acc: 0.8928 - val_loss: 0.1860 - val_acc: 0.9250\n","698000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3170 - acc: 0.8850 - val_loss: 0.2663 - val_acc: 0.9100\n","700000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2904 - acc: 0.8994 - val_loss: 0.2819 - val_acc: 0.9000\n","702000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2985 - acc: 0.8878 - val_loss: 0.3780 - val_acc: 0.8650\n","704000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3198 - acc: 0.8872 - val_loss: 0.3214 - val_acc: 0.8800\n","706000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2918 - acc: 0.8922 - val_loss: 0.3461 - val_acc: 0.8750\n","708000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3580 - acc: 0.8722 - val_loss: 0.3014 - val_acc: 0.8850\n","710000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3102 - acc: 0.8839 - val_loss: 0.3372 - val_acc: 0.8650\n","712000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3381 - acc: 0.8778 - val_loss: 0.2495 - val_acc: 0.9000\n","714000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3067 - acc: 0.8861 - val_loss: 0.2033 - val_acc: 0.9250\n","716000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3184 - acc: 0.8894 - val_loss: 0.3026 - val_acc: 0.8800\n","718000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2929 - acc: 0.8989 - val_loss: 0.3554 - val_acc: 0.8850\n","720000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3165 - acc: 0.8839 - val_loss: 0.3457 - val_acc: 0.8700\n","722000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2857 - acc: 0.8978 - val_loss: 0.2879 - val_acc: 0.8850\n","724000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3237 - acc: 0.8833 - val_loss: 0.2605 - val_acc: 0.9000\n","726000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3192 - acc: 0.8817 - val_loss: 0.2672 - val_acc: 0.9000\n","728000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3238 - acc: 0.8911 - val_loss: 0.2324 - val_acc: 0.9300\n","730000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3243 - acc: 0.8817 - val_loss: 0.2774 - val_acc: 0.8750\n","732000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3559 - acc: 0.8672 - val_loss: 0.2700 - val_acc: 0.8850\n","734000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2882 - acc: 0.8961 - val_loss: 0.2852 - val_acc: 0.8850\n","736000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3544 - acc: 0.8700 - val_loss: 0.3558 - val_acc: 0.8550\n","738000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3053 - acc: 0.8889 - val_loss: 0.2906 - val_acc: 0.8950\n","740000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3154 - acc: 0.8900 - val_loss: 0.2641 - val_acc: 0.9000\n","742000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3072 - acc: 0.8906 - val_loss: 0.2397 - val_acc: 0.8850\n","744000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2994 - acc: 0.8889 - val_loss: 0.3074 - val_acc: 0.8950\n","746000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2982 - acc: 0.8894 - val_loss: 0.3078 - val_acc: 0.8750\n","748000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3177 - acc: 0.8828 - val_loss: 0.2940 - val_acc: 0.8850\n","750000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3169 - acc: 0.8856 - val_loss: 0.2021 - val_acc: 0.9350\n","752000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3401 - acc: 0.8811 - val_loss: 0.2838 - val_acc: 0.9100\n","754000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2975 - acc: 0.8861 - val_loss: 0.2921 - val_acc: 0.8800\n","756000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2937 - acc: 0.8883 - val_loss: 0.2569 - val_acc: 0.9100\n","758000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2985 - acc: 0.8844 - val_loss: 0.3088 - val_acc: 0.8950\n","760000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2993 - acc: 0.8894 - val_loss: 0.2971 - val_acc: 0.8850\n","762000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3340 - acc: 0.8772 - val_loss: 0.3682 - val_acc: 0.9000\n","764000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3005 - acc: 0.9000 - val_loss: 0.3180 - val_acc: 0.8600\n","766000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3131 - acc: 0.8933 - val_loss: 0.2981 - val_acc: 0.8850\n","768000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2820 - acc: 0.8911 - val_loss: 0.2740 - val_acc: 0.8850\n","770000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3234 - acc: 0.8856 - val_loss: 0.1678 - val_acc: 0.9500\n","772000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3048 - acc: 0.8967 - val_loss: 0.3272 - val_acc: 0.8750\n","774000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3278 - acc: 0.8783 - val_loss: 0.2355 - val_acc: 0.9100\n","776000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3167 - acc: 0.8811 - val_loss: 0.2414 - val_acc: 0.9300\n","778000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3217 - acc: 0.8883 - val_loss: 0.3194 - val_acc: 0.8900\n","780000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3031 - acc: 0.8933 - val_loss: 0.3707 - val_acc: 0.8800\n","782000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3214 - acc: 0.8739 - val_loss: 0.3609 - val_acc: 0.8500\n","784000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3109 - acc: 0.8872 - val_loss: 0.3634 - val_acc: 0.8650\n","786000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3015 - acc: 0.8928 - val_loss: 0.2150 - val_acc: 0.9300\n","788000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2991 - acc: 0.8833 - val_loss: 0.3215 - val_acc: 0.8700\n","790000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3101 - acc: 0.8883 - val_loss: 0.1372 - val_acc: 0.9450\n","792000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3124 - acc: 0.8900 - val_loss: 0.2857 - val_acc: 0.8900\n","794000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3007 - acc: 0.8900 - val_loss: 0.3431 - val_acc: 0.8650\n","796000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3022 - acc: 0.8917 - val_loss: 0.2476 - val_acc: 0.9200\n","798000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3352 - acc: 0.8700 - val_loss: 0.3719 - val_acc: 0.8850\n","800000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2792 - acc: 0.8989 - val_loss: 0.3478 - val_acc: 0.8850\n","802000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3059 - acc: 0.8961 - val_loss: 0.2960 - val_acc: 0.8950\n","804000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3264 - acc: 0.8789 - val_loss: 0.2457 - val_acc: 0.8900\n","806000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3358 - acc: 0.8717 - val_loss: 0.2189 - val_acc: 0.9200\n","808000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3242 - acc: 0.8850 - val_loss: 0.2620 - val_acc: 0.9050\n","810000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3069 - acc: 0.8806 - val_loss: 0.3522 - val_acc: 0.8700\n","812000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3294 - acc: 0.8822 - val_loss: 0.2584 - val_acc: 0.9150\n","814000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3016 - acc: 0.8889 - val_loss: 0.3544 - val_acc: 0.8700\n","816000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2869 - acc: 0.8900 - val_loss: 0.2551 - val_acc: 0.9000\n","818000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3000 - acc: 0.8867 - val_loss: 0.2412 - val_acc: 0.9200\n","820000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3069 - acc: 0.8894 - val_loss: 0.2843 - val_acc: 0.9000\n","822000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3082 - acc: 0.8900 - val_loss: 0.2617 - val_acc: 0.9000\n","824000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2918 - acc: 0.8889 - val_loss: 0.2719 - val_acc: 0.8800\n","826000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3144 - acc: 0.8900 - val_loss: 0.3476 - val_acc: 0.8500\n","828000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2864 - acc: 0.9011 - val_loss: 0.2333 - val_acc: 0.9150\n","830000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3024 - acc: 0.8861 - val_loss: 0.2107 - val_acc: 0.9300\n","832000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2941 - acc: 0.8894 - val_loss: 0.2691 - val_acc: 0.8750\n","834000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2875 - acc: 0.9000 - val_loss: 0.2814 - val_acc: 0.9000\n","836000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3168 - acc: 0.8906 - val_loss: 0.2632 - val_acc: 0.9100\n","838000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3203 - acc: 0.8883 - val_loss: 0.2353 - val_acc: 0.9100\n","840000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3293 - acc: 0.8728 - val_loss: 0.2756 - val_acc: 0.9150\n","842000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2990 - acc: 0.8928 - val_loss: 0.2186 - val_acc: 0.9250\n","844000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2933 - acc: 0.8911 - val_loss: 0.2629 - val_acc: 0.9050\n","846000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3005 - acc: 0.8922 - val_loss: 0.2511 - val_acc: 0.9000\n","848000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3197 - acc: 0.8867 - val_loss: 0.2363 - val_acc: 0.9100\n","850000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2943 - acc: 0.8894 - val_loss: 0.2768 - val_acc: 0.9100\n","852000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3190 - acc: 0.8811 - val_loss: 0.2831 - val_acc: 0.8900\n","854000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3034 - acc: 0.8872 - val_loss: 0.2835 - val_acc: 0.9000\n","856000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3241 - acc: 0.8894 - val_loss: 0.2569 - val_acc: 0.9000\n","858000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2997 - acc: 0.8928 - val_loss: 0.2779 - val_acc: 0.9000\n","860000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2918 - acc: 0.8994 - val_loss: 0.2814 - val_acc: 0.8800\n","862000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3188 - acc: 0.8822 - val_loss: 0.2692 - val_acc: 0.9050\n","864000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2966 - acc: 0.8811 - val_loss: 0.2012 - val_acc: 0.9100\n","866000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3248 - acc: 0.8861 - val_loss: 0.2635 - val_acc: 0.8900\n","868000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3041 - acc: 0.8883 - val_loss: 0.2351 - val_acc: 0.9050\n","870000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3370 - acc: 0.8844 - val_loss: 0.2775 - val_acc: 0.8800\n","872000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3064 - acc: 0.9011 - val_loss: 0.3454 - val_acc: 0.8950\n","874000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3031 - acc: 0.8867 - val_loss: 0.2427 - val_acc: 0.9200\n","876000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2951 - acc: 0.8972 - val_loss: 0.3831 - val_acc: 0.8500\n","878000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3205 - acc: 0.8794 - val_loss: 0.2218 - val_acc: 0.9100\n","880000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2957 - acc: 0.8933 - val_loss: 0.2966 - val_acc: 0.9150\n","882000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3054 - acc: 0.8872 - val_loss: 0.2080 - val_acc: 0.9200\n","884000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2703 - acc: 0.9000 - val_loss: 0.3070 - val_acc: 0.8750\n","886000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2933 - acc: 0.8933 - val_loss: 0.1610 - val_acc: 0.9550\n","888000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3129 - acc: 0.8978 - val_loss: 0.3130 - val_acc: 0.8950\n","890000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3249 - acc: 0.8817 - val_loss: 0.3081 - val_acc: 0.8750\n","892000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3106 - acc: 0.8933 - val_loss: 0.2845 - val_acc: 0.8850\n","894000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3144 - acc: 0.8889 - val_loss: 0.2323 - val_acc: 0.9100\n","896000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3290 - acc: 0.8806 - val_loss: 0.3683 - val_acc: 0.8750\n","898000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3160 - acc: 0.8783 - val_loss: 0.2120 - val_acc: 0.9100\n","900000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3136 - acc: 0.8867 - val_loss: 0.2442 - val_acc: 0.9050\n","902000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2726 - acc: 0.9011 - val_loss: 0.2768 - val_acc: 0.8800\n","904000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2820 - acc: 0.8989 - val_loss: 0.2278 - val_acc: 0.9200\n","906000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2971 - acc: 0.9006 - val_loss: 0.2083 - val_acc: 0.9150\n","908000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3053 - acc: 0.8856 - val_loss: 0.2867 - val_acc: 0.8950\n","910000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3229 - acc: 0.8800 - val_loss: 0.3636 - val_acc: 0.8900\n","912000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3211 - acc: 0.8789 - val_loss: 0.2289 - val_acc: 0.9250\n","914000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2672 - acc: 0.9039 - val_loss: 0.2557 - val_acc: 0.8850\n","916000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2958 - acc: 0.8917 - val_loss: 0.3169 - val_acc: 0.8900\n","918000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3334 - acc: 0.8778 - val_loss: 0.3349 - val_acc: 0.8600\n","920000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2823 - acc: 0.8983 - val_loss: 0.3196 - val_acc: 0.8900\n","922000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2770 - acc: 0.8989 - val_loss: 0.2750 - val_acc: 0.8850\n","924000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3005 - acc: 0.8944 - val_loss: 0.2530 - val_acc: 0.9050\n","926000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3008 - acc: 0.8878 - val_loss: 0.3688 - val_acc: 0.8700\n","928000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3197 - acc: 0.8806 - val_loss: 0.2794 - val_acc: 0.8850\n","930000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3281 - acc: 0.8956 - val_loss: 0.3752 - val_acc: 0.8600\n","932000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2744 - acc: 0.9056 - val_loss: 0.2741 - val_acc: 0.9050\n","934000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2984 - acc: 0.8911 - val_loss: 0.2686 - val_acc: 0.9100\n","936000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2931 - acc: 0.8889 - val_loss: 0.2537 - val_acc: 0.8950\n","938000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3119 - acc: 0.8861 - val_loss: 0.2454 - val_acc: 0.8900\n","940000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2803 - acc: 0.9033 - val_loss: 0.2398 - val_acc: 0.9100\n","942000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3208 - acc: 0.8872 - val_loss: 0.2363 - val_acc: 0.9200\n","944000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3034 - acc: 0.8872 - val_loss: 0.2049 - val_acc: 0.9250\n","946000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3159 - acc: 0.8906 - val_loss: 0.2692 - val_acc: 0.9100\n","948000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2851 - acc: 0.8989 - val_loss: 0.3333 - val_acc: 0.8750\n","950000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2981 - acc: 0.9033 - val_loss: 0.3489 - val_acc: 0.8550\n","952000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2973 - acc: 0.8972 - val_loss: 0.2823 - val_acc: 0.8800\n","954000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2867 - acc: 0.9022 - val_loss: 0.3547 - val_acc: 0.8600\n","956000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2837 - acc: 0.8922 - val_loss: 0.2082 - val_acc: 0.9300\n","958000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3068 - acc: 0.8850 - val_loss: 0.3303 - val_acc: 0.8850\n","960000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3092 - acc: 0.8894 - val_loss: 0.2051 - val_acc: 0.9200\n","962000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2892 - acc: 0.8972 - val_loss: 0.3153 - val_acc: 0.9000\n","964000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2756 - acc: 0.9006 - val_loss: 0.2675 - val_acc: 0.9000\n","966000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2842 - acc: 0.8933 - val_loss: 0.2292 - val_acc: 0.9300\n","968000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3097 - acc: 0.8944 - val_loss: 0.3082 - val_acc: 0.9000\n","970000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3060 - acc: 0.8967 - val_loss: 0.4031 - val_acc: 0.8650\n","972000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3226 - acc: 0.8822 - val_loss: 0.3054 - val_acc: 0.9000\n","974000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2789 - acc: 0.8922 - val_loss: 0.2869 - val_acc: 0.8950\n","976000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2722 - acc: 0.9011 - val_loss: 0.2586 - val_acc: 0.9050\n","978000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3098 - acc: 0.8889 - val_loss: 0.3308 - val_acc: 0.8950\n","980000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3032 - acc: 0.8944 - val_loss: 0.2217 - val_acc: 0.9350\n","982000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2929 - acc: 0.8989 - val_loss: 0.1743 - val_acc: 0.9450\n","984000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3242 - acc: 0.8789 - val_loss: 0.3026 - val_acc: 0.9000\n","986000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3023 - acc: 0.8911 - val_loss: 0.2566 - val_acc: 0.8950\n","988000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3104 - acc: 0.8806 - val_loss: 0.1712 - val_acc: 0.9250\n","990000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3078 - acc: 0.8928 - val_loss: 0.3041 - val_acc: 0.9200\n","992000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3069 - acc: 0.8878 - val_loss: 0.2975 - val_acc: 0.8650\n","994000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3132 - acc: 0.8872 - val_loss: 0.3590 - val_acc: 0.8900\n","996000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2701 - acc: 0.8978 - val_loss: 0.2981 - val_acc: 0.8950\n","998000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3108 - acc: 0.8856 - val_loss: 0.2360 - val_acc: 0.9100\n","1000000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3228 - acc: 0.8850 - val_loss: 0.2643 - val_acc: 0.9100\n","\n"," Train Loop Number 3\n","2000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2814 - acc: 0.8961 - val_loss: 0.2259 - val_acc: 0.9100\n","4000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3265 - acc: 0.8694 - val_loss: 0.2242 - val_acc: 0.9350\n","6000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2976 - acc: 0.8917 - val_loss: 0.2993 - val_acc: 0.8900\n","8000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3081 - acc: 0.8872 - val_loss: 0.2836 - val_acc: 0.8900\n","10000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3014 - acc: 0.8950 - val_loss: 0.2351 - val_acc: 0.9050\n","12000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2827 - acc: 0.8939 - val_loss: 0.2104 - val_acc: 0.9100\n","14000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2663 - acc: 0.9050 - val_loss: 0.2238 - val_acc: 0.9050\n","16000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3437 - acc: 0.8794 - val_loss: 0.2853 - val_acc: 0.9050\n","18000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3098 - acc: 0.8956 - val_loss: 0.3014 - val_acc: 0.8850\n","20000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2929 - acc: 0.8972 - val_loss: 0.3080 - val_acc: 0.8900\n","22000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2761 - acc: 0.8933 - val_loss: 0.2570 - val_acc: 0.8850\n","24000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3042 - acc: 0.8861 - val_loss: 0.2567 - val_acc: 0.9100\n","26000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2991 - acc: 0.8889 - val_loss: 0.2419 - val_acc: 0.9250\n","28000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2696 - acc: 0.8989 - val_loss: 0.2725 - val_acc: 0.9100\n","30000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2809 - acc: 0.8956 - val_loss: 0.2704 - val_acc: 0.8850\n","32000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2961 - acc: 0.8894 - val_loss: 0.2322 - val_acc: 0.8950\n","34000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2751 - acc: 0.9039 - val_loss: 0.2012 - val_acc: 0.9300\n","36000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2742 - acc: 0.9017 - val_loss: 0.1851 - val_acc: 0.9200\n","38000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3244 - acc: 0.8822 - val_loss: 0.2478 - val_acc: 0.8900\n","40000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3044 - acc: 0.8889 - val_loss: 0.2991 - val_acc: 0.8800\n","42000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3090 - acc: 0.8844 - val_loss: 0.2806 - val_acc: 0.8750\n","44000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2712 - acc: 0.8956 - val_loss: 0.2144 - val_acc: 0.9000\n","46000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2656 - acc: 0.9006 - val_loss: 0.2908 - val_acc: 0.8750\n","48000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3356 - acc: 0.8822 - val_loss: 0.2902 - val_acc: 0.8900\n","50000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3072 - acc: 0.8917 - val_loss: 0.2809 - val_acc: 0.8800\n","52000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2867 - acc: 0.8861 - val_loss: 0.3223 - val_acc: 0.8850\n","54000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2831 - acc: 0.8983 - val_loss: 0.2909 - val_acc: 0.8850\n","56000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2864 - acc: 0.9006 - val_loss: 0.2464 - val_acc: 0.9150\n","58000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2781 - acc: 0.9050 - val_loss: 0.3559 - val_acc: 0.8950\n","60000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2859 - acc: 0.8867 - val_loss: 0.2304 - val_acc: 0.9300\n","62000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3160 - acc: 0.8867 - val_loss: 0.2637 - val_acc: 0.9200\n","64000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2950 - acc: 0.8956 - val_loss: 0.2771 - val_acc: 0.9050\n","66000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2830 - acc: 0.8933 - val_loss: 0.2652 - val_acc: 0.9050\n","68000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2661 - acc: 0.9006 - val_loss: 0.2225 - val_acc: 0.9100\n","70000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3161 - acc: 0.8806 - val_loss: 0.1916 - val_acc: 0.9300\n","72000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2991 - acc: 0.8933 - val_loss: 0.2172 - val_acc: 0.9250\n","74000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2869 - acc: 0.8922 - val_loss: 0.2847 - val_acc: 0.9150\n","76000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2843 - acc: 0.8944 - val_loss: 0.3219 - val_acc: 0.9050\n","78000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3065 - acc: 0.8872 - val_loss: 0.2757 - val_acc: 0.8750\n","80000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2815 - acc: 0.8983 - val_loss: 0.2534 - val_acc: 0.9100\n","82000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2886 - acc: 0.8878 - val_loss: 0.4057 - val_acc: 0.8700\n","84000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2792 - acc: 0.8933 - val_loss: 0.2003 - val_acc: 0.9250\n","86000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2994 - acc: 0.8917 - val_loss: 0.2521 - val_acc: 0.9300\n","88000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2885 - acc: 0.8900 - val_loss: 0.3024 - val_acc: 0.8850\n","90000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3097 - acc: 0.8856 - val_loss: 0.4083 - val_acc: 0.8500\n","92000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2692 - acc: 0.9039 - val_loss: 0.2988 - val_acc: 0.9050\n","94000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2852 - acc: 0.9028 - val_loss: 0.3312 - val_acc: 0.8750\n","96000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2825 - acc: 0.8978 - val_loss: 0.2373 - val_acc: 0.9050\n","98000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2974 - acc: 0.8872 - val_loss: 0.3018 - val_acc: 0.8750\n","100000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2783 - acc: 0.8978 - val_loss: 0.2164 - val_acc: 0.9000\n","102000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3118 - acc: 0.8856 - val_loss: 0.2981 - val_acc: 0.8850\n","104000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3091 - acc: 0.8828 - val_loss: 0.2120 - val_acc: 0.9350\n","106000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3109 - acc: 0.8856 - val_loss: 0.3634 - val_acc: 0.8750\n","108000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2903 - acc: 0.8917 - val_loss: 0.2798 - val_acc: 0.8750\n","110000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2752 - acc: 0.8933 - val_loss: 0.2221 - val_acc: 0.9250\n","112000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3020 - acc: 0.8828 - val_loss: 0.2554 - val_acc: 0.9050\n","114000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2749 - acc: 0.8950 - val_loss: 0.1912 - val_acc: 0.9450\n","116000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2593 - acc: 0.9083 - val_loss: 0.3106 - val_acc: 0.8600\n","118000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2712 - acc: 0.8972 - val_loss: 0.2871 - val_acc: 0.8750\n","120000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2797 - acc: 0.8967 - val_loss: 0.2603 - val_acc: 0.9350\n","122000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3054 - acc: 0.8928 - val_loss: 0.3678 - val_acc: 0.8650\n","124000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2907 - acc: 0.8972 - val_loss: 0.1932 - val_acc: 0.9300\n","126000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3090 - acc: 0.8861 - val_loss: 0.2157 - val_acc: 0.9100\n","128000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3042 - acc: 0.8872 - val_loss: 0.4519 - val_acc: 0.8250\n","130000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2718 - acc: 0.9056 - val_loss: 0.3402 - val_acc: 0.8600\n","132000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2670 - acc: 0.9067 - val_loss: 0.2705 - val_acc: 0.9000\n","134000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2881 - acc: 0.8994 - val_loss: 0.3169 - val_acc: 0.8850\n","136000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2940 - acc: 0.8978 - val_loss: 0.3044 - val_acc: 0.9000\n","138000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3024 - acc: 0.8856 - val_loss: 0.3129 - val_acc: 0.8800\n","140000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2888 - acc: 0.8978 - val_loss: 0.2669 - val_acc: 0.9000\n","142000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3343 - acc: 0.8833 - val_loss: 0.2449 - val_acc: 0.8950\n","144000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2699 - acc: 0.9033 - val_loss: 0.2609 - val_acc: 0.8950\n","146000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3016 - acc: 0.8928 - val_loss: 0.2431 - val_acc: 0.9050\n","148000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2896 - acc: 0.8978 - val_loss: 0.3475 - val_acc: 0.8650\n","150000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2999 - acc: 0.8911 - val_loss: 0.3363 - val_acc: 0.8750\n","152000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2911 - acc: 0.8911 - val_loss: 0.2573 - val_acc: 0.9050\n","154000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3209 - acc: 0.8906 - val_loss: 0.3109 - val_acc: 0.9100\n","156000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2830 - acc: 0.8917 - val_loss: 0.2681 - val_acc: 0.9100\n","158000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2861 - acc: 0.8939 - val_loss: 0.2790 - val_acc: 0.8800\n","160000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2681 - acc: 0.8978 - val_loss: 0.2222 - val_acc: 0.9050\n","162000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2757 - acc: 0.8994 - val_loss: 0.2192 - val_acc: 0.9350\n","164000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3077 - acc: 0.8872 - val_loss: 0.2901 - val_acc: 0.9000\n","166000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3029 - acc: 0.8906 - val_loss: 0.2442 - val_acc: 0.9050\n","168000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2886 - acc: 0.8883 - val_loss: 0.3192 - val_acc: 0.8750\n","170000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3051 - acc: 0.8856 - val_loss: 0.3267 - val_acc: 0.8600\n","172000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3096 - acc: 0.8972 - val_loss: 0.2165 - val_acc: 0.9150\n","174000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2999 - acc: 0.8944 - val_loss: 0.1941 - val_acc: 0.9200\n","176000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3282 - acc: 0.8811 - val_loss: 0.3410 - val_acc: 0.8700\n","178000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2586 - acc: 0.9033 - val_loss: 0.2152 - val_acc: 0.9250\n","180000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2975 - acc: 0.9006 - val_loss: 0.3368 - val_acc: 0.8700\n","182000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3077 - acc: 0.8867 - val_loss: 0.3368 - val_acc: 0.8700\n","184000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3128 - acc: 0.8783 - val_loss: 0.2890 - val_acc: 0.8750\n","186000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3016 - acc: 0.8883 - val_loss: 0.2870 - val_acc: 0.8750\n","188000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2757 - acc: 0.9078 - val_loss: 0.2282 - val_acc: 0.9150\n","190000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2809 - acc: 0.9033 - val_loss: 0.3797 - val_acc: 0.8750\n","192000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2644 - acc: 0.9011 - val_loss: 0.2812 - val_acc: 0.9100\n","194000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2907 - acc: 0.8967 - val_loss: 0.3020 - val_acc: 0.8900\n","196000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2968 - acc: 0.8972 - val_loss: 0.2722 - val_acc: 0.8800\n","198000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2673 - acc: 0.9033 - val_loss: 0.2984 - val_acc: 0.8850\n","200000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3269 - acc: 0.8822 - val_loss: 0.2673 - val_acc: 0.8900\n","202000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3462 - acc: 0.8733 - val_loss: 0.3314 - val_acc: 0.8750\n","204000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2918 - acc: 0.8972 - val_loss: 0.2371 - val_acc: 0.9200\n","206000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2833 - acc: 0.8928 - val_loss: 0.3245 - val_acc: 0.9050\n","208000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3213 - acc: 0.8839 - val_loss: 0.2857 - val_acc: 0.9050\n","210000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2852 - acc: 0.9017 - val_loss: 0.3473 - val_acc: 0.8600\n","212000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2953 - acc: 0.8994 - val_loss: 0.3800 - val_acc: 0.8700\n","214000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3009 - acc: 0.8928 - val_loss: 0.1721 - val_acc: 0.9350\n","216000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2984 - acc: 0.8861 - val_loss: 0.2702 - val_acc: 0.8950\n","218000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2989 - acc: 0.8928 - val_loss: 0.2794 - val_acc: 0.8750\n","220000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2942 - acc: 0.8883 - val_loss: 0.2225 - val_acc: 0.9150\n","222000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2784 - acc: 0.9033 - val_loss: 0.2343 - val_acc: 0.8850\n","224000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2569 - acc: 0.9022 - val_loss: 0.2573 - val_acc: 0.8800\n","226000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2667 - acc: 0.9050 - val_loss: 0.2747 - val_acc: 0.9050\n","228000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3144 - acc: 0.8956 - val_loss: 0.2486 - val_acc: 0.9350\n","230000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2662 - acc: 0.9067 - val_loss: 0.3019 - val_acc: 0.9100\n","232000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2924 - acc: 0.8844 - val_loss: 0.2379 - val_acc: 0.9200\n","234000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3333 - acc: 0.8839 - val_loss: 0.2694 - val_acc: 0.9100\n","236000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2854 - acc: 0.8972 - val_loss: 0.3344 - val_acc: 0.8700\n","238000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3392 - acc: 0.8850 - val_loss: 0.2019 - val_acc: 0.9200\n","240000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2540 - acc: 0.8994 - val_loss: 0.3328 - val_acc: 0.8550\n","242000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2841 - acc: 0.8961 - val_loss: 0.2613 - val_acc: 0.9050\n","244000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2869 - acc: 0.8817 - val_loss: 0.2334 - val_acc: 0.9150\n","246000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2941 - acc: 0.8939 - val_loss: 0.2632 - val_acc: 0.9150\n","248000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2902 - acc: 0.8967 - val_loss: 0.1941 - val_acc: 0.9300\n","250000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3181 - acc: 0.8778 - val_loss: 0.2431 - val_acc: 0.9200\n","252000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2551 - acc: 0.9039 - val_loss: 0.2671 - val_acc: 0.8800\n","254000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2834 - acc: 0.8989 - val_loss: 0.2570 - val_acc: 0.8900\n","256000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2870 - acc: 0.8956 - val_loss: 0.2719 - val_acc: 0.9000\n","258000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2848 - acc: 0.8922 - val_loss: 0.2130 - val_acc: 0.9200\n","260000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3247 - acc: 0.8856 - val_loss: 0.1994 - val_acc: 0.9450\n","262000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3087 - acc: 0.8811 - val_loss: 0.1998 - val_acc: 0.9350\n","264000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3045 - acc: 0.8856 - val_loss: 0.3341 - val_acc: 0.8950\n","266000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2758 - acc: 0.9006 - val_loss: 0.2943 - val_acc: 0.8900\n","268000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2870 - acc: 0.9011 - val_loss: 0.2777 - val_acc: 0.9100\n","270000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2992 - acc: 0.8961 - val_loss: 0.2381 - val_acc: 0.9050\n","272000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2854 - acc: 0.8944 - val_loss: 0.2363 - val_acc: 0.9200\n","274000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3087 - acc: 0.8928 - val_loss: 0.3677 - val_acc: 0.8850\n","276000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2783 - acc: 0.9006 - val_loss: 0.2369 - val_acc: 0.9100\n","278000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2760 - acc: 0.8994 - val_loss: 0.3557 - val_acc: 0.8700\n","280000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2826 - acc: 0.9039 - val_loss: 0.2523 - val_acc: 0.9100\n","282000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3294 - acc: 0.8867 - val_loss: 0.2166 - val_acc: 0.9150\n","284000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2850 - acc: 0.8928 - val_loss: 0.2192 - val_acc: 0.9250\n","286000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2819 - acc: 0.8989 - val_loss: 0.1951 - val_acc: 0.9200\n","288000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3213 - acc: 0.8878 - val_loss: 0.3444 - val_acc: 0.8750\n","290000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3158 - acc: 0.8839 - val_loss: 0.2308 - val_acc: 0.9000\n","292000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2606 - acc: 0.8983 - val_loss: 0.2541 - val_acc: 0.9000\n","294000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2709 - acc: 0.9072 - val_loss: 0.3102 - val_acc: 0.8900\n","296000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2831 - acc: 0.8956 - val_loss: 0.2938 - val_acc: 0.8850\n","298000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2971 - acc: 0.8894 - val_loss: 0.3267 - val_acc: 0.9050\n","300000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2897 - acc: 0.8950 - val_loss: 0.1862 - val_acc: 0.9400\n","302000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2866 - acc: 0.8889 - val_loss: 0.2706 - val_acc: 0.9100\n","304000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2947 - acc: 0.8872 - val_loss: 0.2708 - val_acc: 0.9100\n","306000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3007 - acc: 0.8939 - val_loss: 0.3668 - val_acc: 0.8850\n","308000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2840 - acc: 0.8989 - val_loss: 0.2132 - val_acc: 0.9150\n","310000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2748 - acc: 0.8983 - val_loss: 0.2678 - val_acc: 0.9150\n","312000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2567 - acc: 0.9028 - val_loss: 0.1819 - val_acc: 0.9300\n","314000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2798 - acc: 0.9006 - val_loss: 0.2042 - val_acc: 0.9200\n","316000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2786 - acc: 0.8961 - val_loss: 0.2702 - val_acc: 0.9050\n","318000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2645 - acc: 0.8950 - val_loss: 0.3413 - val_acc: 0.8800\n","320000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2716 - acc: 0.9000 - val_loss: 0.2754 - val_acc: 0.9250\n","322000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2734 - acc: 0.9000 - val_loss: 0.3147 - val_acc: 0.8850\n","324000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2737 - acc: 0.9022 - val_loss: 0.3279 - val_acc: 0.8600\n","326000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2828 - acc: 0.8978 - val_loss: 0.2270 - val_acc: 0.8900\n","328000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3138 - acc: 0.8856 - val_loss: 0.1823 - val_acc: 0.9400\n","330000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2830 - acc: 0.8922 - val_loss: 0.2546 - val_acc: 0.9150\n","332000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3026 - acc: 0.8922 - val_loss: 0.2220 - val_acc: 0.9050\n","334000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2831 - acc: 0.9006 - val_loss: 0.3027 - val_acc: 0.8800\n","336000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2725 - acc: 0.8956 - val_loss: 0.2740 - val_acc: 0.9050\n","338000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2568 - acc: 0.9106 - val_loss: 0.2424 - val_acc: 0.8900\n","340000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3055 - acc: 0.8911 - val_loss: 0.2979 - val_acc: 0.8800\n","342000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2919 - acc: 0.8922 - val_loss: 0.2215 - val_acc: 0.9350\n","344000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3047 - acc: 0.8922 - val_loss: 0.2438 - val_acc: 0.9050\n","346000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2892 - acc: 0.8917 - val_loss: 0.2076 - val_acc: 0.9200\n","348000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2849 - acc: 0.8917 - val_loss: 0.2569 - val_acc: 0.8900\n","350000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3043 - acc: 0.8967 - val_loss: 0.3348 - val_acc: 0.8900\n","352000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2874 - acc: 0.8928 - val_loss: 0.3262 - val_acc: 0.8750\n","354000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2934 - acc: 0.8933 - val_loss: 0.2868 - val_acc: 0.9050\n","356000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2922 - acc: 0.8950 - val_loss: 0.2483 - val_acc: 0.9050\n","358000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3202 - acc: 0.8850 - val_loss: 0.2695 - val_acc: 0.9150\n","360000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2949 - acc: 0.8872 - val_loss: 0.2852 - val_acc: 0.8950\n","362000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2670 - acc: 0.8978 - val_loss: 0.3232 - val_acc: 0.8700\n","364000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3155 - acc: 0.8928 - val_loss: 0.2553 - val_acc: 0.8800\n","366000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3032 - acc: 0.8944 - val_loss: 0.2568 - val_acc: 0.9100\n","368000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2656 - acc: 0.9017 - val_loss: 0.2202 - val_acc: 0.9150\n","370000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3002 - acc: 0.8956 - val_loss: 0.3557 - val_acc: 0.8950\n","372000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2862 - acc: 0.8983 - val_loss: 0.2233 - val_acc: 0.9050\n","374000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2756 - acc: 0.9011 - val_loss: 0.2441 - val_acc: 0.9300\n","376000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3096 - acc: 0.8894 - val_loss: 0.2217 - val_acc: 0.9200\n","378000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2703 - acc: 0.9006 - val_loss: 0.2989 - val_acc: 0.9100\n","380000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2740 - acc: 0.8989 - val_loss: 0.2950 - val_acc: 0.9000\n","382000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2617 - acc: 0.9017 - val_loss: 0.3134 - val_acc: 0.8550\n","384000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2683 - acc: 0.9017 - val_loss: 0.2193 - val_acc: 0.9100\n","386000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2851 - acc: 0.8950 - val_loss: 0.2875 - val_acc: 0.9200\n","388000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2867 - acc: 0.9000 - val_loss: 0.3580 - val_acc: 0.8950\n","390000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2626 - acc: 0.9033 - val_loss: 0.3286 - val_acc: 0.8850\n","392000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2891 - acc: 0.8950 - val_loss: 0.3054 - val_acc: 0.9000\n","394000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2733 - acc: 0.8933 - val_loss: 0.2334 - val_acc: 0.9200\n","396000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2671 - acc: 0.9039 - val_loss: 0.3005 - val_acc: 0.8900\n","398000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2505 - acc: 0.9072 - val_loss: 0.2203 - val_acc: 0.9400\n","400000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2852 - acc: 0.8922 - val_loss: 0.2772 - val_acc: 0.9000\n","402000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2790 - acc: 0.8967 - val_loss: 0.3104 - val_acc: 0.8750\n","404000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2952 - acc: 0.8933 - val_loss: 0.2477 - val_acc: 0.9150\n","406000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2760 - acc: 0.8983 - val_loss: 0.3093 - val_acc: 0.8800\n","408000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2835 - acc: 0.8989 - val_loss: 0.3027 - val_acc: 0.8800\n","410000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3027 - acc: 0.8883 - val_loss: 0.2424 - val_acc: 0.9000\n","412000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2730 - acc: 0.9017 - val_loss: 0.2946 - val_acc: 0.8900\n","414000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2564 - acc: 0.9033 - val_loss: 0.3239 - val_acc: 0.8700\n","416000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3059 - acc: 0.8828 - val_loss: 0.2153 - val_acc: 0.9300\n","418000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2899 - acc: 0.8972 - val_loss: 0.2516 - val_acc: 0.8900\n","420000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3003 - acc: 0.8928 - val_loss: 0.2692 - val_acc: 0.8900\n","422000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2702 - acc: 0.8950 - val_loss: 0.2406 - val_acc: 0.9200\n","424000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2737 - acc: 0.8983 - val_loss: 0.2972 - val_acc: 0.8850\n","426000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2892 - acc: 0.8906 - val_loss: 0.2253 - val_acc: 0.9250\n","428000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2829 - acc: 0.8933 - val_loss: 0.3208 - val_acc: 0.8800\n","430000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2981 - acc: 0.9022 - val_loss: 0.1831 - val_acc: 0.9450\n","432000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2659 - acc: 0.9011 - val_loss: 0.2479 - val_acc: 0.9100\n","434000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2865 - acc: 0.8978 - val_loss: 0.2864 - val_acc: 0.8850\n","436000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3023 - acc: 0.8911 - val_loss: 0.2673 - val_acc: 0.9050\n","438000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3119 - acc: 0.8833 - val_loss: 0.1822 - val_acc: 0.9200\n","440000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3080 - acc: 0.8983 - val_loss: 0.2942 - val_acc: 0.8900\n","442000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2782 - acc: 0.8906 - val_loss: 0.2793 - val_acc: 0.9200\n","444000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3125 - acc: 0.8917 - val_loss: 0.2338 - val_acc: 0.9200\n","446000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2770 - acc: 0.8989 - val_loss: 0.2245 - val_acc: 0.9200\n","448000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3240 - acc: 0.8883 - val_loss: 0.2523 - val_acc: 0.9050\n","450000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2967 - acc: 0.8839 - val_loss: 0.3666 - val_acc: 0.8800\n","452000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3062 - acc: 0.8922 - val_loss: 0.2325 - val_acc: 0.8950\n","454000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2539 - acc: 0.9100 - val_loss: 0.3298 - val_acc: 0.8900\n","456000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2794 - acc: 0.8944 - val_loss: 0.3063 - val_acc: 0.9050\n","458000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2998 - acc: 0.8928 - val_loss: 0.3449 - val_acc: 0.8550\n","460000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3026 - acc: 0.9006 - val_loss: 0.2707 - val_acc: 0.9000\n","462000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2642 - acc: 0.9017 - val_loss: 0.2223 - val_acc: 0.9250\n","464000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3094 - acc: 0.8839 - val_loss: 0.2361 - val_acc: 0.9150\n","466000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2841 - acc: 0.8922 - val_loss: 0.2493 - val_acc: 0.9000\n","468000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2762 - acc: 0.9022 - val_loss: 0.2526 - val_acc: 0.8950\n","470000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3273 - acc: 0.8800 - val_loss: 0.3205 - val_acc: 0.8750\n","472000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2784 - acc: 0.8978 - val_loss: 0.3318 - val_acc: 0.8600\n","474000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2885 - acc: 0.8944 - val_loss: 0.2175 - val_acc: 0.9200\n","476000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2671 - acc: 0.9006 - val_loss: 0.2620 - val_acc: 0.8800\n","478000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3003 - acc: 0.8961 - val_loss: 0.1933 - val_acc: 0.9150\n","480000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2373 - acc: 0.9117 - val_loss: 0.2929 - val_acc: 0.8950\n","482000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2869 - acc: 0.8978 - val_loss: 0.2489 - val_acc: 0.8900\n","484000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2906 - acc: 0.8900 - val_loss: 0.2145 - val_acc: 0.9050\n","486000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2780 - acc: 0.8922 - val_loss: 0.1755 - val_acc: 0.9200\n","488000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2853 - acc: 0.9078 - val_loss: 0.2873 - val_acc: 0.8800\n","490000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2955 - acc: 0.8889 - val_loss: 0.2293 - val_acc: 0.9150\n","492000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2860 - acc: 0.8978 - val_loss: 0.4724 - val_acc: 0.8300\n","494000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2569 - acc: 0.9033 - val_loss: 0.2187 - val_acc: 0.9050\n","496000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2417 - acc: 0.9133 - val_loss: 0.2227 - val_acc: 0.9400\n","498000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2781 - acc: 0.9044 - val_loss: 0.3582 - val_acc: 0.8800\n","500000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3157 - acc: 0.8922 - val_loss: 0.2606 - val_acc: 0.8900\n","502000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2720 - acc: 0.8956 - val_loss: 0.2094 - val_acc: 0.8950\n","504000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2733 - acc: 0.8989 - val_loss: 0.2224 - val_acc: 0.9200\n","506000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2914 - acc: 0.8950 - val_loss: 0.4034 - val_acc: 0.8800\n","508000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2855 - acc: 0.8900 - val_loss: 0.2558 - val_acc: 0.9000\n","510000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2852 - acc: 0.8933 - val_loss: 0.2355 - val_acc: 0.9250\n","512000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2714 - acc: 0.9072 - val_loss: 0.2392 - val_acc: 0.8950\n","514000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3133 - acc: 0.8767 - val_loss: 0.3449 - val_acc: 0.8450\n","516000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2943 - acc: 0.8939 - val_loss: 0.2599 - val_acc: 0.9050\n","518000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3115 - acc: 0.8800 - val_loss: 0.2787 - val_acc: 0.9000\n","520000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2973 - acc: 0.8978 - val_loss: 0.2764 - val_acc: 0.8900\n","522000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3149 - acc: 0.8889 - val_loss: 0.1854 - val_acc: 0.9300\n","524000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2867 - acc: 0.8983 - val_loss: 0.2640 - val_acc: 0.8950\n","526000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2767 - acc: 0.8944 - val_loss: 0.1887 - val_acc: 0.9050\n","528000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3086 - acc: 0.8928 - val_loss: 0.3182 - val_acc: 0.8850\n","530000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2965 - acc: 0.8944 - val_loss: 0.2422 - val_acc: 0.9150\n","532000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2991 - acc: 0.8972 - val_loss: 0.2526 - val_acc: 0.8850\n","534000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2865 - acc: 0.9033 - val_loss: 0.2597 - val_acc: 0.9150\n","536000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2665 - acc: 0.9050 - val_loss: 0.1748 - val_acc: 0.9100\n","538000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2830 - acc: 0.8972 - val_loss: 0.2656 - val_acc: 0.9050\n","540000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2744 - acc: 0.8922 - val_loss: 0.2427 - val_acc: 0.9100\n","542000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2706 - acc: 0.8978 - val_loss: 0.2317 - val_acc: 0.9100\n","544000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2877 - acc: 0.8950 - val_loss: 0.2498 - val_acc: 0.9050\n","546000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2604 - acc: 0.9044 - val_loss: 0.2616 - val_acc: 0.9100\n","548000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2908 - acc: 0.8978 - val_loss: 0.2325 - val_acc: 0.9100\n","550000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2945 - acc: 0.8967 - val_loss: 0.2861 - val_acc: 0.9000\n","552000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2706 - acc: 0.9017 - val_loss: 0.2655 - val_acc: 0.9050\n","554000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3178 - acc: 0.8900 - val_loss: 0.2167 - val_acc: 0.9400\n","556000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2745 - acc: 0.8956 - val_loss: 0.2965 - val_acc: 0.9050\n","558000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2673 - acc: 0.9111 - val_loss: 0.3124 - val_acc: 0.8800\n","560000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2942 - acc: 0.8894 - val_loss: 0.2394 - val_acc: 0.9250\n","562000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2842 - acc: 0.8983 - val_loss: 0.1805 - val_acc: 0.9250\n","564000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2773 - acc: 0.9000 - val_loss: 0.1619 - val_acc: 0.9200\n","566000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2935 - acc: 0.8911 - val_loss: 0.2959 - val_acc: 0.8750\n","568000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2593 - acc: 0.9078 - val_loss: 0.2524 - val_acc: 0.8850\n","570000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2799 - acc: 0.8878 - val_loss: 0.2153 - val_acc: 0.9100\n","572000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2436 - acc: 0.9056 - val_loss: 0.2392 - val_acc: 0.9000\n","574000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3020 - acc: 0.8828 - val_loss: 0.2973 - val_acc: 0.8900\n","576000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2850 - acc: 0.8972 - val_loss: 0.2768 - val_acc: 0.8950\n","578000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2857 - acc: 0.8944 - val_loss: 0.3130 - val_acc: 0.8750\n","580000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3186 - acc: 0.8772 - val_loss: 0.2570 - val_acc: 0.9000\n","582000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2775 - acc: 0.9017 - val_loss: 0.2193 - val_acc: 0.9200\n","584000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2929 - acc: 0.8861 - val_loss: 0.2919 - val_acc: 0.9000\n","586000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2607 - acc: 0.9089 - val_loss: 0.3690 - val_acc: 0.8500\n","588000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2744 - acc: 0.9011 - val_loss: 0.2004 - val_acc: 0.9300\n","590000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2960 - acc: 0.8950 - val_loss: 0.4248 - val_acc: 0.8250\n","592000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2953 - acc: 0.8956 - val_loss: 0.2645 - val_acc: 0.8950\n","594000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2824 - acc: 0.8956 - val_loss: 0.3900 - val_acc: 0.8750\n","596000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2789 - acc: 0.9011 - val_loss: 0.2780 - val_acc: 0.8850\n","598000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2898 - acc: 0.8972 - val_loss: 0.2708 - val_acc: 0.8800\n","600000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2806 - acc: 0.8917 - val_loss: 0.2558 - val_acc: 0.9050\n","602000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2764 - acc: 0.8917 - val_loss: 0.1834 - val_acc: 0.9350\n","604000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2930 - acc: 0.8939 - val_loss: 0.2617 - val_acc: 0.8950\n","606000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2734 - acc: 0.9039 - val_loss: 0.2406 - val_acc: 0.9100\n","608000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2811 - acc: 0.8972 - val_loss: 0.2700 - val_acc: 0.8850\n","610000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2805 - acc: 0.8983 - val_loss: 0.3086 - val_acc: 0.8850\n","612000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2646 - acc: 0.9000 - val_loss: 0.2529 - val_acc: 0.9000\n","614000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2816 - acc: 0.9061 - val_loss: 0.1695 - val_acc: 0.9350\n","616000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2822 - acc: 0.9044 - val_loss: 0.2235 - val_acc: 0.9200\n","618000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2378 - acc: 0.9094 - val_loss: 0.2165 - val_acc: 0.9150\n","620000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2915 - acc: 0.8939 - val_loss: 0.2585 - val_acc: 0.8900\n","622000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2765 - acc: 0.9022 - val_loss: 0.3186 - val_acc: 0.9050\n","624000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2678 - acc: 0.9033 - val_loss: 0.3059 - val_acc: 0.8800\n","626000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2823 - acc: 0.8972 - val_loss: 0.2389 - val_acc: 0.9000\n","628000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2719 - acc: 0.9028 - val_loss: 0.3093 - val_acc: 0.8850\n","630000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2843 - acc: 0.8950 - val_loss: 0.2605 - val_acc: 0.9000\n","632000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2839 - acc: 0.8956 - val_loss: 0.2049 - val_acc: 0.9350\n","634000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3082 - acc: 0.8839 - val_loss: 0.3170 - val_acc: 0.8950\n","636000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2814 - acc: 0.8967 - val_loss: 0.2497 - val_acc: 0.9050\n","638000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2588 - acc: 0.9050 - val_loss: 0.2097 - val_acc: 0.9200\n","640000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2848 - acc: 0.9056 - val_loss: 0.1969 - val_acc: 0.9400\n","642000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2644 - acc: 0.9061 - val_loss: 0.2501 - val_acc: 0.9150\n","644000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2767 - acc: 0.8922 - val_loss: 0.2960 - val_acc: 0.8950\n","646000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2721 - acc: 0.8961 - val_loss: 0.2962 - val_acc: 0.9100\n","648000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2932 - acc: 0.8961 - val_loss: 0.2211 - val_acc: 0.9150\n","650000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2836 - acc: 0.8939 - val_loss: 0.2694 - val_acc: 0.8950\n","652000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2701 - acc: 0.8983 - val_loss: 0.2356 - val_acc: 0.9050\n","654000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2835 - acc: 0.8967 - val_loss: 0.3001 - val_acc: 0.8900\n","656000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2621 - acc: 0.9078 - val_loss: 0.2553 - val_acc: 0.8900\n","658000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2827 - acc: 0.8967 - val_loss: 0.2735 - val_acc: 0.8900\n","660000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2689 - acc: 0.8967 - val_loss: 0.2041 - val_acc: 0.9250\n","662000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2894 - acc: 0.8906 - val_loss: 0.3245 - val_acc: 0.8750\n","664000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2600 - acc: 0.9050 - val_loss: 0.2921 - val_acc: 0.9000\n","666000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2691 - acc: 0.9017 - val_loss: 0.1451 - val_acc: 0.9400\n","668000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2882 - acc: 0.8889 - val_loss: 0.2877 - val_acc: 0.8950\n","670000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3001 - acc: 0.8967 - val_loss: 0.2740 - val_acc: 0.8800\n","672000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2795 - acc: 0.8917 - val_loss: 0.2701 - val_acc: 0.8900\n","674000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2808 - acc: 0.9122 - val_loss: 0.2576 - val_acc: 0.8950\n","676000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2808 - acc: 0.9011 - val_loss: 0.2653 - val_acc: 0.9000\n","678000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2900 - acc: 0.8933 - val_loss: 0.2039 - val_acc: 0.9200\n","680000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2882 - acc: 0.8972 - val_loss: 0.2748 - val_acc: 0.9000\n","682000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2812 - acc: 0.8911 - val_loss: 0.2802 - val_acc: 0.8950\n","684000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2850 - acc: 0.9006 - val_loss: 0.2929 - val_acc: 0.8900\n","686000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2744 - acc: 0.8989 - val_loss: 0.1615 - val_acc: 0.9450\n","688000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2587 - acc: 0.8994 - val_loss: 0.2760 - val_acc: 0.8950\n","690000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2717 - acc: 0.9067 - val_loss: 0.2875 - val_acc: 0.9000\n","692000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2741 - acc: 0.8983 - val_loss: 0.2658 - val_acc: 0.9050\n","694000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2786 - acc: 0.9006 - val_loss: 0.2839 - val_acc: 0.8950\n","696000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2864 - acc: 0.8961 - val_loss: 0.1949 - val_acc: 0.9350\n","698000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2860 - acc: 0.8961 - val_loss: 0.3110 - val_acc: 0.8950\n","700000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2706 - acc: 0.8978 - val_loss: 0.2015 - val_acc: 0.9250\n","702000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3069 - acc: 0.8972 - val_loss: 0.2614 - val_acc: 0.9050\n","704000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2783 - acc: 0.9000 - val_loss: 0.2165 - val_acc: 0.9350\n","706000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2566 - acc: 0.9000 - val_loss: 0.2617 - val_acc: 0.8900\n","708000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2662 - acc: 0.9011 - val_loss: 0.3329 - val_acc: 0.9050\n","710000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2679 - acc: 0.9067 - val_loss: 0.2395 - val_acc: 0.8950\n","712000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2723 - acc: 0.8994 - val_loss: 0.2085 - val_acc: 0.9450\n","714000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2855 - acc: 0.9000 - val_loss: 0.2830 - val_acc: 0.9050\n","716000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2774 - acc: 0.9044 - val_loss: 0.3451 - val_acc: 0.8650\n","718000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2756 - acc: 0.8994 - val_loss: 0.2063 - val_acc: 0.9250\n","720000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2774 - acc: 0.8983 - val_loss: 0.2362 - val_acc: 0.9050\n","722000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2817 - acc: 0.8983 - val_loss: 0.2909 - val_acc: 0.9000\n","724000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2751 - acc: 0.8961 - val_loss: 0.4130 - val_acc: 0.8550\n","726000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2674 - acc: 0.8994 - val_loss: 0.2360 - val_acc: 0.9200\n","728000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2682 - acc: 0.9006 - val_loss: 0.2836 - val_acc: 0.8700\n","730000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2589 - acc: 0.9039 - val_loss: 0.2631 - val_acc: 0.9050\n","732000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2764 - acc: 0.9000 - val_loss: 0.2213 - val_acc: 0.9050\n","734000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2863 - acc: 0.9017 - val_loss: 0.2795 - val_acc: 0.8850\n","736000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2896 - acc: 0.8850 - val_loss: 0.2494 - val_acc: 0.9150\n","738000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2477 - acc: 0.9033 - val_loss: 0.2238 - val_acc: 0.9000\n","740000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2620 - acc: 0.8972 - val_loss: 0.2422 - val_acc: 0.9250\n","742000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2688 - acc: 0.9006 - val_loss: 0.1698 - val_acc: 0.9300\n","744000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2832 - acc: 0.8933 - val_loss: 0.2847 - val_acc: 0.8950\n","746000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2807 - acc: 0.9044 - val_loss: 0.2514 - val_acc: 0.9150\n","748000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2805 - acc: 0.9017 - val_loss: 0.2193 - val_acc: 0.9350\n","750000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2511 - acc: 0.9111 - val_loss: 0.2060 - val_acc: 0.9100\n","752000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2977 - acc: 0.8894 - val_loss: 0.3029 - val_acc: 0.9000\n","754000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2500 - acc: 0.9083 - val_loss: 0.1749 - val_acc: 0.9350\n","756000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2602 - acc: 0.9078 - val_loss: 0.3217 - val_acc: 0.8750\n","758000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2781 - acc: 0.8956 - val_loss: 0.2599 - val_acc: 0.8900\n","760000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2744 - acc: 0.8889 - val_loss: 0.3256 - val_acc: 0.8850\n","762000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2812 - acc: 0.8983 - val_loss: 0.2749 - val_acc: 0.8700\n","764000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2848 - acc: 0.9017 - val_loss: 0.3209 - val_acc: 0.8850\n","766000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2728 - acc: 0.9044 - val_loss: 0.2112 - val_acc: 0.8950\n","768000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2934 - acc: 0.8828 - val_loss: 0.2672 - val_acc: 0.9100\n","770000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2807 - acc: 0.8839 - val_loss: 0.2287 - val_acc: 0.9100\n","772000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2591 - acc: 0.9011 - val_loss: 0.4096 - val_acc: 0.8550\n","774000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2878 - acc: 0.8956 - val_loss: 0.2349 - val_acc: 0.9150\n","776000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2742 - acc: 0.8983 - val_loss: 0.2752 - val_acc: 0.9000\n","778000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3029 - acc: 0.8917 - val_loss: 0.3575 - val_acc: 0.8800\n","780000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2872 - acc: 0.8872 - val_loss: 0.2595 - val_acc: 0.8950\n","782000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2799 - acc: 0.9006 - val_loss: 0.2150 - val_acc: 0.9200\n","784000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2646 - acc: 0.8972 - val_loss: 0.1853 - val_acc: 0.9100\n","786000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2846 - acc: 0.9033 - val_loss: 0.3238 - val_acc: 0.8900\n","788000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2778 - acc: 0.8950 - val_loss: 0.1303 - val_acc: 0.9450\n","790000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2821 - acc: 0.8972 - val_loss: 0.3033 - val_acc: 0.8850\n","792000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2710 - acc: 0.9039 - val_loss: 0.3336 - val_acc: 0.8850\n","794000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2582 - acc: 0.9039 - val_loss: 0.2617 - val_acc: 0.9100\n","796000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2695 - acc: 0.9044 - val_loss: 0.2397 - val_acc: 0.9100\n","798000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2771 - acc: 0.8978 - val_loss: 0.2117 - val_acc: 0.9050\n","800000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2738 - acc: 0.8956 - val_loss: 0.2401 - val_acc: 0.9200\n","802000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2452 - acc: 0.9144 - val_loss: 0.2825 - val_acc: 0.8850\n","804000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2846 - acc: 0.8994 - val_loss: 0.3628 - val_acc: 0.8500\n","806000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2785 - acc: 0.8950 - val_loss: 0.2792 - val_acc: 0.8900\n","808000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2855 - acc: 0.8956 - val_loss: 0.3004 - val_acc: 0.8900\n","810000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2862 - acc: 0.9006 - val_loss: 0.2141 - val_acc: 0.9250\n","812000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2667 - acc: 0.9056 - val_loss: 0.2344 - val_acc: 0.9000\n","814000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2900 - acc: 0.8956 - val_loss: 0.2102 - val_acc: 0.9400\n","816000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2680 - acc: 0.9089 - val_loss: 0.1696 - val_acc: 0.9300\n","818000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2805 - acc: 0.9017 - val_loss: 0.2401 - val_acc: 0.9150\n","820000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2556 - acc: 0.9078 - val_loss: 0.2300 - val_acc: 0.9150\n","822000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2666 - acc: 0.9006 - val_loss: 0.1937 - val_acc: 0.9150\n","824000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2926 - acc: 0.8917 - val_loss: 0.2618 - val_acc: 0.9200\n","826000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2683 - acc: 0.8978 - val_loss: 0.2467 - val_acc: 0.9050\n","828000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2565 - acc: 0.9056 - val_loss: 0.1871 - val_acc: 0.9200\n","830000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2587 - acc: 0.9100 - val_loss: 0.1885 - val_acc: 0.9250\n","832000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2944 - acc: 0.9006 - val_loss: 0.2275 - val_acc: 0.9250\n","834000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2756 - acc: 0.9000 - val_loss: 0.2752 - val_acc: 0.9000\n","836000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2446 - acc: 0.9094 - val_loss: 0.2431 - val_acc: 0.9000\n","838000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2723 - acc: 0.9044 - val_loss: 0.2998 - val_acc: 0.8850\n","840000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2862 - acc: 0.8983 - val_loss: 0.2933 - val_acc: 0.8800\n","842000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2499 - acc: 0.9044 - val_loss: 0.2884 - val_acc: 0.9000\n","844000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2762 - acc: 0.9056 - val_loss: 0.1877 - val_acc: 0.9350\n","846000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2789 - acc: 0.9000 - val_loss: 0.1776 - val_acc: 0.9550\n","848000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2633 - acc: 0.9028 - val_loss: 0.2543 - val_acc: 0.9050\n","850000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2477 - acc: 0.9094 - val_loss: 0.1780 - val_acc: 0.9550\n","852000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2682 - acc: 0.9050 - val_loss: 0.2905 - val_acc: 0.9050\n","854000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2565 - acc: 0.9072 - val_loss: 0.2234 - val_acc: 0.9050\n","856000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2618 - acc: 0.9089 - val_loss: 0.2932 - val_acc: 0.8650\n","858000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3090 - acc: 0.8883 - val_loss: 0.1832 - val_acc: 0.9150\n","860000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2681 - acc: 0.9028 - val_loss: 0.2167 - val_acc: 0.9200\n","862000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2677 - acc: 0.9017 - val_loss: 0.3818 - val_acc: 0.8450\n","864000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2703 - acc: 0.8961 - val_loss: 0.3182 - val_acc: 0.8750\n","866000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2964 - acc: 0.8894 - val_loss: 0.2310 - val_acc: 0.9150\n","868000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2749 - acc: 0.9011 - val_loss: 0.3568 - val_acc: 0.8950\n","870000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2708 - acc: 0.9028 - val_loss: 0.1910 - val_acc: 0.9250\n","872000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2895 - acc: 0.8872 - val_loss: 0.2364 - val_acc: 0.9050\n","874000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2706 - acc: 0.9033 - val_loss: 0.1881 - val_acc: 0.9400\n","876000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2577 - acc: 0.9056 - val_loss: 0.1982 - val_acc: 0.9300\n","878000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2387 - acc: 0.9106 - val_loss: 0.1588 - val_acc: 0.9300\n","880000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2796 - acc: 0.8989 - val_loss: 0.3829 - val_acc: 0.8750\n","882000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2811 - acc: 0.8961 - val_loss: 0.1997 - val_acc: 0.9150\n","884000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2853 - acc: 0.8939 - val_loss: 0.2230 - val_acc: 0.9200\n","886000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2732 - acc: 0.9039 - val_loss: 0.1851 - val_acc: 0.9300\n","888000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2550 - acc: 0.9111 - val_loss: 0.2069 - val_acc: 0.9250\n","890000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2727 - acc: 0.9011 - val_loss: 0.1495 - val_acc: 0.9350\n","892000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2580 - acc: 0.9000 - val_loss: 0.2878 - val_acc: 0.8950\n","894000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2606 - acc: 0.9083 - val_loss: 0.2596 - val_acc: 0.9050\n","896000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2210 - acc: 0.9172 - val_loss: 0.3725 - val_acc: 0.8550\n","898000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2481 - acc: 0.9022 - val_loss: 0.1417 - val_acc: 0.9500\n","900000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2981 - acc: 0.8933 - val_loss: 0.2060 - val_acc: 0.9200\n","902000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2691 - acc: 0.9056 - val_loss: 0.2374 - val_acc: 0.9100\n","904000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2667 - acc: 0.9022 - val_loss: 0.2672 - val_acc: 0.8900\n","906000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2655 - acc: 0.9039 - val_loss: 0.2402 - val_acc: 0.9300\n","908000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2572 - acc: 0.9089 - val_loss: 0.2122 - val_acc: 0.9150\n","910000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2633 - acc: 0.9006 - val_loss: 0.2289 - val_acc: 0.9050\n","912000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2720 - acc: 0.9011 - val_loss: 0.2631 - val_acc: 0.9050\n","914000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2906 - acc: 0.8994 - val_loss: 0.2074 - val_acc: 0.9050\n","916000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2442 - acc: 0.9039 - val_loss: 0.2669 - val_acc: 0.9150\n","918000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2711 - acc: 0.9050 - val_loss: 0.3007 - val_acc: 0.9150\n","920000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2715 - acc: 0.9083 - val_loss: 0.2868 - val_acc: 0.8700\n","922000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2533 - acc: 0.9061 - val_loss: 0.2256 - val_acc: 0.9200\n","924000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2737 - acc: 0.8928 - val_loss: 0.2392 - val_acc: 0.9150\n","926000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2752 - acc: 0.8961 - val_loss: 0.3460 - val_acc: 0.8900\n","928000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2838 - acc: 0.9028 - val_loss: 0.2568 - val_acc: 0.9000\n","930000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2748 - acc: 0.9039 - val_loss: 0.2661 - val_acc: 0.9000\n","932000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2577 - acc: 0.8994 - val_loss: 0.2826 - val_acc: 0.9200\n","934000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2751 - acc: 0.9033 - val_loss: 0.2260 - val_acc: 0.9150\n","936000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2570 - acc: 0.9083 - val_loss: 0.2584 - val_acc: 0.9050\n","938000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2544 - acc: 0.9044 - val_loss: 0.2098 - val_acc: 0.9150\n","940000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2677 - acc: 0.9022 - val_loss: 0.2647 - val_acc: 0.9100\n","942000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2975 - acc: 0.9067 - val_loss: 0.2911 - val_acc: 0.8950\n","944000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2537 - acc: 0.9139 - val_loss: 0.3198 - val_acc: 0.8750\n","946000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2838 - acc: 0.8967 - val_loss: 0.3187 - val_acc: 0.8750\n","948000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2889 - acc: 0.8939 - val_loss: 0.2554 - val_acc: 0.9150\n","950000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2567 - acc: 0.9117 - val_loss: 0.2513 - val_acc: 0.9200\n","952000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2603 - acc: 0.9117 - val_loss: 0.2219 - val_acc: 0.9150\n","954000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2224 - acc: 0.9139 - val_loss: 0.1921 - val_acc: 0.9350\n","956000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2743 - acc: 0.9033 - val_loss: 0.1722 - val_acc: 0.9300\n","958000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2449 - acc: 0.9033 - val_loss: 0.1503 - val_acc: 0.9500\n","960000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2544 - acc: 0.9039 - val_loss: 0.1974 - val_acc: 0.9350\n","962000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2769 - acc: 0.8994 - val_loss: 0.1836 - val_acc: 0.9100\n","964000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2537 - acc: 0.9067 - val_loss: 0.2315 - val_acc: 0.9250\n","966000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2304 - acc: 0.9200 - val_loss: 0.2557 - val_acc: 0.9150\n","968000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2499 - acc: 0.9056 - val_loss: 0.3537 - val_acc: 0.8700\n","970000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2979 - acc: 0.8917 - val_loss: 0.2490 - val_acc: 0.9150\n","972000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2744 - acc: 0.8978 - val_loss: 0.1868 - val_acc: 0.9350\n","974000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2917 - acc: 0.8906 - val_loss: 0.2370 - val_acc: 0.9000\n","976000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2690 - acc: 0.8956 - val_loss: 0.2162 - val_acc: 0.9200\n","978000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2607 - acc: 0.9061 - val_loss: 0.2413 - val_acc: 0.9100\n","980000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2615 - acc: 0.9083 - val_loss: 0.2748 - val_acc: 0.8750\n","982000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2651 - acc: 0.9022 - val_loss: 0.3033 - val_acc: 0.8800\n","984000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2640 - acc: 0.9061 - val_loss: 0.2043 - val_acc: 0.9200\n","986000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2676 - acc: 0.8967 - val_loss: 0.2752 - val_acc: 0.8900\n","988000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2474 - acc: 0.9050 - val_loss: 0.2649 - val_acc: 0.8900\n","990000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2766 - acc: 0.9006 - val_loss: 0.3353 - val_acc: 0.8750\n","992000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2546 - acc: 0.9033 - val_loss: 0.2716 - val_acc: 0.8850\n","994000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2716 - acc: 0.8978 - val_loss: 0.3295 - val_acc: 0.9000\n","996000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2613 - acc: 0.9061 - val_loss: 0.2162 - val_acc: 0.9150\n","998000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2762 - acc: 0.9033 - val_loss: 0.3236 - val_acc: 0.8800\n","1000000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2920 - acc: 0.8928 - val_loss: 0.2503 - val_acc: 0.9150\n","\n"," Train Loop Number 4\n","2000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2519 - acc: 0.9117 - val_loss: 0.2888 - val_acc: 0.8950\n","4000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2781 - acc: 0.9000 - val_loss: 0.2109 - val_acc: 0.9250\n","6000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2722 - acc: 0.9056 - val_loss: 0.2556 - val_acc: 0.8950\n","8000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2482 - acc: 0.9072 - val_loss: 0.3369 - val_acc: 0.8750\n","10000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2810 - acc: 0.9028 - val_loss: 0.2977 - val_acc: 0.9000\n","12000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2744 - acc: 0.8922 - val_loss: 0.2249 - val_acc: 0.9000\n","14000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2657 - acc: 0.9128 - val_loss: 0.2254 - val_acc: 0.9300\n","16000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2864 - acc: 0.8967 - val_loss: 0.2672 - val_acc: 0.9250\n","18000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2513 - acc: 0.9089 - val_loss: 0.3876 - val_acc: 0.8550\n","20000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2586 - acc: 0.8989 - val_loss: 0.1749 - val_acc: 0.9500\n","22000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2613 - acc: 0.8989 - val_loss: 0.3079 - val_acc: 0.8900\n","24000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2603 - acc: 0.8950 - val_loss: 0.3244 - val_acc: 0.8700\n","26000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2727 - acc: 0.9017 - val_loss: 0.2727 - val_acc: 0.8950\n","28000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2488 - acc: 0.9039 - val_loss: 0.1849 - val_acc: 0.9300\n","30000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2466 - acc: 0.9139 - val_loss: 0.2647 - val_acc: 0.8850\n","32000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2746 - acc: 0.9056 - val_loss: 0.2472 - val_acc: 0.9100\n","34000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2617 - acc: 0.9033 - val_loss: 0.2158 - val_acc: 0.9200\n","36000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2451 - acc: 0.9006 - val_loss: 0.3506 - val_acc: 0.8900\n","38000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2769 - acc: 0.9011 - val_loss: 0.3171 - val_acc: 0.8500\n","40000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2864 - acc: 0.8911 - val_loss: 0.1298 - val_acc: 0.9550\n","42000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2579 - acc: 0.9072 - val_loss: 0.1956 - val_acc: 0.9100\n","44000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2573 - acc: 0.9083 - val_loss: 0.2146 - val_acc: 0.9250\n","46000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2558 - acc: 0.9061 - val_loss: 0.3276 - val_acc: 0.9000\n","48000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2710 - acc: 0.8989 - val_loss: 0.2719 - val_acc: 0.9000\n","50000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2427 - acc: 0.9100 - val_loss: 0.2134 - val_acc: 0.9150\n","52000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2668 - acc: 0.8933 - val_loss: 0.1714 - val_acc: 0.9500\n","54000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2847 - acc: 0.8989 - val_loss: 0.2137 - val_acc: 0.9150\n","56000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2614 - acc: 0.9028 - val_loss: 0.3281 - val_acc: 0.8600\n","58000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2803 - acc: 0.8900 - val_loss: 0.2402 - val_acc: 0.9100\n","60000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2732 - acc: 0.8944 - val_loss: 0.3634 - val_acc: 0.8850\n","62000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2978 - acc: 0.8978 - val_loss: 0.2376 - val_acc: 0.9000\n","64000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2628 - acc: 0.8972 - val_loss: 0.2356 - val_acc: 0.9100\n","66000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2447 - acc: 0.9100 - val_loss: 0.2092 - val_acc: 0.9100\n","68000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2640 - acc: 0.9017 - val_loss: 0.3016 - val_acc: 0.8900\n","70000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2682 - acc: 0.8978 - val_loss: 0.2533 - val_acc: 0.8900\n","72000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2519 - acc: 0.8989 - val_loss: 0.2387 - val_acc: 0.9350\n","74000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2570 - acc: 0.9083 - val_loss: 0.1686 - val_acc: 0.9250\n","76000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2891 - acc: 0.8889 - val_loss: 0.2960 - val_acc: 0.8700\n","78000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2490 - acc: 0.9144 - val_loss: 0.2046 - val_acc: 0.9100\n","80000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2677 - acc: 0.9111 - val_loss: 0.2821 - val_acc: 0.8750\n","82000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2874 - acc: 0.8922 - val_loss: 0.1852 - val_acc: 0.9200\n","84000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3089 - acc: 0.8956 - val_loss: 0.1654 - val_acc: 0.9350\n","86000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2849 - acc: 0.8939 - val_loss: 0.3351 - val_acc: 0.8550\n","88000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2348 - acc: 0.9161 - val_loss: 0.2072 - val_acc: 0.9100\n","90000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2912 - acc: 0.8900 - val_loss: 0.2346 - val_acc: 0.9200\n","92000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2319 - acc: 0.9117 - val_loss: 0.3003 - val_acc: 0.8900\n","94000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2754 - acc: 0.8961 - val_loss: 0.2003 - val_acc: 0.9100\n","96000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2754 - acc: 0.9033 - val_loss: 0.1476 - val_acc: 0.9350\n","98000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2581 - acc: 0.9056 - val_loss: 0.1981 - val_acc: 0.9250\n","100000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2540 - acc: 0.9078 - val_loss: 0.2412 - val_acc: 0.9050\n","102000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2762 - acc: 0.8928 - val_loss: 0.2417 - val_acc: 0.9050\n","104000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2466 - acc: 0.9117 - val_loss: 0.2415 - val_acc: 0.9300\n","106000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2387 - acc: 0.9189 - val_loss: 0.2569 - val_acc: 0.8950\n","108000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2688 - acc: 0.9106 - val_loss: 0.2376 - val_acc: 0.9250\n","110000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2645 - acc: 0.9028 - val_loss: 0.1936 - val_acc: 0.9200\n","112000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2961 - acc: 0.8900 - val_loss: 0.2841 - val_acc: 0.8950\n","114000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2705 - acc: 0.9006 - val_loss: 0.2702 - val_acc: 0.8900\n","116000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2749 - acc: 0.9044 - val_loss: 0.2152 - val_acc: 0.9300\n","118000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2624 - acc: 0.9028 - val_loss: 0.3645 - val_acc: 0.8700\n","120000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2507 - acc: 0.9161 - val_loss: 0.2857 - val_acc: 0.9050\n","122000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2650 - acc: 0.9072 - val_loss: 0.2623 - val_acc: 0.8900\n","124000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2542 - acc: 0.9128 - val_loss: 0.2144 - val_acc: 0.9100\n","126000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2764 - acc: 0.9017 - val_loss: 0.2810 - val_acc: 0.9050\n","128000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2425 - acc: 0.9006 - val_loss: 0.2560 - val_acc: 0.9050\n","130000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2254 - acc: 0.9144 - val_loss: 0.2597 - val_acc: 0.8850\n","132000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2819 - acc: 0.8978 - val_loss: 0.2053 - val_acc: 0.9300\n","134000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2670 - acc: 0.9061 - val_loss: 0.1897 - val_acc: 0.9400\n","136000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2708 - acc: 0.9039 - val_loss: 0.2876 - val_acc: 0.8950\n","138000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2668 - acc: 0.9133 - val_loss: 0.2494 - val_acc: 0.8900\n","140000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2916 - acc: 0.8894 - val_loss: 0.2681 - val_acc: 0.9100\n","142000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2619 - acc: 0.9000 - val_loss: 0.3107 - val_acc: 0.8750\n","144000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2571 - acc: 0.9011 - val_loss: 0.1549 - val_acc: 0.9200\n","146000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2612 - acc: 0.9100 - val_loss: 0.2319 - val_acc: 0.9100\n","148000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2605 - acc: 0.9028 - val_loss: 0.1891 - val_acc: 0.9300\n","150000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3063 - acc: 0.8933 - val_loss: 0.2641 - val_acc: 0.8850\n","152000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3149 - acc: 0.8850 - val_loss: 0.2538 - val_acc: 0.9200\n","154000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2689 - acc: 0.9056 - val_loss: 0.2246 - val_acc: 0.9050\n","156000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2366 - acc: 0.9150 - val_loss: 0.2566 - val_acc: 0.9050\n","158000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2616 - acc: 0.9044 - val_loss: 0.1706 - val_acc: 0.9400\n","160000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2818 - acc: 0.9022 - val_loss: 0.3204 - val_acc: 0.8650\n","162000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2611 - acc: 0.9072 - val_loss: 0.1896 - val_acc: 0.9200\n","164000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2822 - acc: 0.8928 - val_loss: 0.3069 - val_acc: 0.8800\n","166000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2789 - acc: 0.8994 - val_loss: 0.2243 - val_acc: 0.9000\n","168000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2733 - acc: 0.9033 - val_loss: 0.2474 - val_acc: 0.9150\n","170000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2886 - acc: 0.8922 - val_loss: 0.2487 - val_acc: 0.9000\n","172000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2887 - acc: 0.8933 - val_loss: 0.2731 - val_acc: 0.8900\n","174000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2570 - acc: 0.9089 - val_loss: 0.2357 - val_acc: 0.9100\n","176000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2828 - acc: 0.8983 - val_loss: 0.2535 - val_acc: 0.9000\n","178000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2953 - acc: 0.8956 - val_loss: 0.2344 - val_acc: 0.9100\n","180000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2373 - acc: 0.9100 - val_loss: 0.2050 - val_acc: 0.9250\n","182000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2504 - acc: 0.9011 - val_loss: 0.1677 - val_acc: 0.9300\n","184000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2663 - acc: 0.9028 - val_loss: 0.2175 - val_acc: 0.9300\n","186000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2789 - acc: 0.9006 - val_loss: 0.2760 - val_acc: 0.8750\n","188000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2541 - acc: 0.9039 - val_loss: 0.2312 - val_acc: 0.9150\n","190000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2693 - acc: 0.8933 - val_loss: 0.2747 - val_acc: 0.8950\n","192000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2556 - acc: 0.9056 - val_loss: 0.3164 - val_acc: 0.8700\n","194000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2733 - acc: 0.9000 - val_loss: 0.2133 - val_acc: 0.9150\n","196000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2556 - acc: 0.9039 - val_loss: 0.2031 - val_acc: 0.9300\n","198000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2629 - acc: 0.9044 - val_loss: 0.2074 - val_acc: 0.9300\n","200000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2625 - acc: 0.9011 - val_loss: 0.2893 - val_acc: 0.8750\n","202000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2626 - acc: 0.9044 - val_loss: 0.2930 - val_acc: 0.9000\n","204000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2765 - acc: 0.8994 - val_loss: 0.2122 - val_acc: 0.9250\n","206000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2605 - acc: 0.9028 - val_loss: 0.2133 - val_acc: 0.9050\n","208000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3108 - acc: 0.8911 - val_loss: 0.3569 - val_acc: 0.9000\n","210000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2604 - acc: 0.9061 - val_loss: 0.2663 - val_acc: 0.9250\n","212000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2998 - acc: 0.8856 - val_loss: 0.2414 - val_acc: 0.9150\n","214000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3009 - acc: 0.8978 - val_loss: 0.2218 - val_acc: 0.9200\n","216000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2667 - acc: 0.9022 - val_loss: 0.2251 - val_acc: 0.9100\n","218000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2757 - acc: 0.9028 - val_loss: 0.1699 - val_acc: 0.9450\n","220000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2657 - acc: 0.9011 - val_loss: 0.2684 - val_acc: 0.9150\n","222000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2362 - acc: 0.9050 - val_loss: 0.2216 - val_acc: 0.9200\n","224000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2393 - acc: 0.9139 - val_loss: 0.1728 - val_acc: 0.9250\n","226000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2541 - acc: 0.9039 - val_loss: 0.2954 - val_acc: 0.8800\n","228000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2761 - acc: 0.8967 - val_loss: 0.2724 - val_acc: 0.9000\n","230000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2601 - acc: 0.8989 - val_loss: 0.2266 - val_acc: 0.9150\n","232000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2496 - acc: 0.9078 - val_loss: 0.2706 - val_acc: 0.8900\n","234000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2542 - acc: 0.9072 - val_loss: 0.2240 - val_acc: 0.9300\n","236000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3002 - acc: 0.8967 - val_loss: 0.1766 - val_acc: 0.9350\n","238000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2582 - acc: 0.9011 - val_loss: 0.3107 - val_acc: 0.8850\n","240000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2647 - acc: 0.9011 - val_loss: 0.1946 - val_acc: 0.9300\n","242000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2762 - acc: 0.9044 - val_loss: 0.2982 - val_acc: 0.8950\n","244000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2699 - acc: 0.9039 - val_loss: 0.2570 - val_acc: 0.9100\n","246000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3002 - acc: 0.8889 - val_loss: 0.1843 - val_acc: 0.9250\n","248000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2627 - acc: 0.9072 - val_loss: 0.2267 - val_acc: 0.8950\n","250000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2416 - acc: 0.9067 - val_loss: 0.2535 - val_acc: 0.8850\n","252000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2891 - acc: 0.8967 - val_loss: 0.1851 - val_acc: 0.9300\n","254000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2611 - acc: 0.9072 - val_loss: 0.2232 - val_acc: 0.9250\n","256000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2834 - acc: 0.9067 - val_loss: 0.2600 - val_acc: 0.9150\n","258000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2426 - acc: 0.9122 - val_loss: 0.2220 - val_acc: 0.9250\n","260000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2851 - acc: 0.9006 - val_loss: 0.2186 - val_acc: 0.9100\n","262000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2857 - acc: 0.9083 - val_loss: 0.2884 - val_acc: 0.8900\n","264000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2415 - acc: 0.9089 - val_loss: 0.2427 - val_acc: 0.9250\n","266000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2839 - acc: 0.8917 - val_loss: 0.2499 - val_acc: 0.9350\n","268000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2428 - acc: 0.9144 - val_loss: 0.1980 - val_acc: 0.9100\n","270000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2510 - acc: 0.8983 - val_loss: 0.2240 - val_acc: 0.9050\n","272000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2683 - acc: 0.8983 - val_loss: 0.2196 - val_acc: 0.9200\n","274000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2985 - acc: 0.8967 - val_loss: 0.4151 - val_acc: 0.8550\n","276000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2613 - acc: 0.9050 - val_loss: 0.3443 - val_acc: 0.8750\n","278000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2518 - acc: 0.9089 - val_loss: 0.2496 - val_acc: 0.9300\n","280000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2527 - acc: 0.9067 - val_loss: 0.2070 - val_acc: 0.9100\n","282000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2390 - acc: 0.9144 - val_loss: 0.2200 - val_acc: 0.9200\n","284000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2427 - acc: 0.9067 - val_loss: 0.1853 - val_acc: 0.9200\n","286000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2854 - acc: 0.8972 - val_loss: 0.2062 - val_acc: 0.9200\n","288000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2779 - acc: 0.8972 - val_loss: 0.2471 - val_acc: 0.9000\n","290000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2944 - acc: 0.8956 - val_loss: 0.2551 - val_acc: 0.9250\n","292000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2635 - acc: 0.8972 - val_loss: 0.2796 - val_acc: 0.8850\n","294000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2718 - acc: 0.9117 - val_loss: 0.2917 - val_acc: 0.8900\n","296000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2604 - acc: 0.9056 - val_loss: 0.2430 - val_acc: 0.9000\n","298000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2425 - acc: 0.9178 - val_loss: 0.1754 - val_acc: 0.9450\n","300000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2737 - acc: 0.8967 - val_loss: 0.3148 - val_acc: 0.8950\n","302000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2475 - acc: 0.9083 - val_loss: 0.2868 - val_acc: 0.8950\n","304000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2721 - acc: 0.9022 - val_loss: 0.2981 - val_acc: 0.9000\n","306000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2918 - acc: 0.8883 - val_loss: 0.2893 - val_acc: 0.8850\n","308000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2596 - acc: 0.9072 - val_loss: 0.2399 - val_acc: 0.8850\n","310000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2683 - acc: 0.8989 - val_loss: 0.1740 - val_acc: 0.9200\n","312000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2605 - acc: 0.8989 - val_loss: 0.2111 - val_acc: 0.9200\n","314000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2771 - acc: 0.8906 - val_loss: 0.1752 - val_acc: 0.9300\n","316000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2481 - acc: 0.9183 - val_loss: 0.2130 - val_acc: 0.9100\n","318000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2521 - acc: 0.9022 - val_loss: 0.2383 - val_acc: 0.9050\n","320000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2686 - acc: 0.9072 - val_loss: 0.3065 - val_acc: 0.9100\n","322000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2669 - acc: 0.8972 - val_loss: 0.2038 - val_acc: 0.9150\n","324000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2589 - acc: 0.9039 - val_loss: 0.2363 - val_acc: 0.9150\n","326000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2816 - acc: 0.8956 - val_loss: 0.3329 - val_acc: 0.8650\n","328000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2448 - acc: 0.9167 - val_loss: 0.2499 - val_acc: 0.9050\n","330000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2596 - acc: 0.9028 - val_loss: 0.3418 - val_acc: 0.8900\n","332000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2385 - acc: 0.9089 - val_loss: 0.3562 - val_acc: 0.8550\n","334000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2466 - acc: 0.9100 - val_loss: 0.2834 - val_acc: 0.9150\n","336000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2742 - acc: 0.8922 - val_loss: 0.1749 - val_acc: 0.9400\n","338000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2695 - acc: 0.8967 - val_loss: 0.2038 - val_acc: 0.9350\n","340000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2436 - acc: 0.9111 - val_loss: 0.2746 - val_acc: 0.8950\n","342000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2403 - acc: 0.9139 - val_loss: 0.1556 - val_acc: 0.9450\n","344000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2308 - acc: 0.9161 - val_loss: 0.2912 - val_acc: 0.8950\n","346000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2796 - acc: 0.9006 - val_loss: 0.1711 - val_acc: 0.9350\n","348000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2506 - acc: 0.9122 - val_loss: 0.2548 - val_acc: 0.9100\n","350000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2869 - acc: 0.8950 - val_loss: 0.3537 - val_acc: 0.8550\n","352000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2466 - acc: 0.9117 - val_loss: 0.2547 - val_acc: 0.9100\n","354000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2661 - acc: 0.8939 - val_loss: 0.2338 - val_acc: 0.8800\n","356000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2651 - acc: 0.9061 - val_loss: 0.1815 - val_acc: 0.9400\n","358000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2644 - acc: 0.9100 - val_loss: 0.2540 - val_acc: 0.8950\n","360000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2371 - acc: 0.9144 - val_loss: 0.2548 - val_acc: 0.8900\n","362000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2721 - acc: 0.9033 - val_loss: 0.3016 - val_acc: 0.9000\n","364000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2541 - acc: 0.9050 - val_loss: 0.2117 - val_acc: 0.9200\n","366000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2451 - acc: 0.9044 - val_loss: 0.1877 - val_acc: 0.9300\n","368000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2669 - acc: 0.8933 - val_loss: 0.2958 - val_acc: 0.8900\n","370000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2311 - acc: 0.9139 - val_loss: 0.1659 - val_acc: 0.9350\n","372000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2610 - acc: 0.9067 - val_loss: 0.2462 - val_acc: 0.8800\n","374000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2889 - acc: 0.8933 - val_loss: 0.1597 - val_acc: 0.9500\n","376000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2713 - acc: 0.9044 - val_loss: 0.2323 - val_acc: 0.9050\n","378000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2680 - acc: 0.9022 - val_loss: 0.1571 - val_acc: 0.9350\n","380000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2377 - acc: 0.9089 - val_loss: 0.2239 - val_acc: 0.8950\n","382000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2320 - acc: 0.9156 - val_loss: 0.1476 - val_acc: 0.9400\n","384000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2508 - acc: 0.9067 - val_loss: 0.2211 - val_acc: 0.9200\n","386000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2941 - acc: 0.8978 - val_loss: 0.2104 - val_acc: 0.9300\n","388000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2639 - acc: 0.9067 - val_loss: 0.2388 - val_acc: 0.9150\n","390000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2666 - acc: 0.9061 - val_loss: 0.2990 - val_acc: 0.9050\n","392000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2937 - acc: 0.8906 - val_loss: 0.2407 - val_acc: 0.8850\n","394000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2552 - acc: 0.9050 - val_loss: 0.1835 - val_acc: 0.9400\n","396000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2555 - acc: 0.9100 - val_loss: 0.2293 - val_acc: 0.9200\n","398000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3065 - acc: 0.8894 - val_loss: 0.2114 - val_acc: 0.9150\n","400000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2703 - acc: 0.9028 - val_loss: 0.2325 - val_acc: 0.9100\n","402000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2520 - acc: 0.9050 - val_loss: 0.2881 - val_acc: 0.8950\n","404000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2708 - acc: 0.8989 - val_loss: 0.2307 - val_acc: 0.9200\n","406000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2277 - acc: 0.9178 - val_loss: 0.2423 - val_acc: 0.8950\n","408000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2505 - acc: 0.9117 - val_loss: 0.2574 - val_acc: 0.8900\n","410000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2869 - acc: 0.8933 - val_loss: 0.1894 - val_acc: 0.9300\n","412000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2743 - acc: 0.8956 - val_loss: 0.1810 - val_acc: 0.9350\n","414000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2588 - acc: 0.9044 - val_loss: 0.2424 - val_acc: 0.9000\n","416000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2391 - acc: 0.9117 - val_loss: 0.3242 - val_acc: 0.8850\n","418000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2646 - acc: 0.9033 - val_loss: 0.2486 - val_acc: 0.9100\n","420000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2712 - acc: 0.9028 - val_loss: 0.2671 - val_acc: 0.8800\n","422000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2704 - acc: 0.9033 - val_loss: 0.2473 - val_acc: 0.9050\n","424000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2435 - acc: 0.9072 - val_loss: 0.2723 - val_acc: 0.9100\n","426000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2589 - acc: 0.9006 - val_loss: 0.2675 - val_acc: 0.8800\n","428000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2822 - acc: 0.8956 - val_loss: 0.2512 - val_acc: 0.9050\n","430000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2742 - acc: 0.9083 - val_loss: 0.2399 - val_acc: 0.9250\n","432000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2404 - acc: 0.9178 - val_loss: 0.2258 - val_acc: 0.9000\n","434000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2517 - acc: 0.9050 - val_loss: 0.1569 - val_acc: 0.9450\n","436000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2766 - acc: 0.8983 - val_loss: 0.1307 - val_acc: 0.9400\n","438000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2759 - acc: 0.9017 - val_loss: 0.2999 - val_acc: 0.8800\n","440000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2591 - acc: 0.9056 - val_loss: 0.2817 - val_acc: 0.8800\n","442000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2534 - acc: 0.9050 - val_loss: 0.2935 - val_acc: 0.8800\n","444000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2354 - acc: 0.9083 - val_loss: 0.2244 - val_acc: 0.9050\n","446000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2389 - acc: 0.9183 - val_loss: 0.2238 - val_acc: 0.9100\n","448000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2684 - acc: 0.8989 - val_loss: 0.2613 - val_acc: 0.8900\n","450000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2422 - acc: 0.9056 - val_loss: 0.2415 - val_acc: 0.9050\n","452000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2598 - acc: 0.9050 - val_loss: 0.1526 - val_acc: 0.9550\n","454000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2755 - acc: 0.9022 - val_loss: 0.3453 - val_acc: 0.8800\n","456000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2345 - acc: 0.9100 - val_loss: 0.2176 - val_acc: 0.9150\n","458000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2691 - acc: 0.8939 - val_loss: 0.2372 - val_acc: 0.8950\n","460000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2650 - acc: 0.9022 - val_loss: 0.2368 - val_acc: 0.9100\n","462000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2581 - acc: 0.9039 - val_loss: 0.2108 - val_acc: 0.9150\n","464000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2516 - acc: 0.9033 - val_loss: 0.2707 - val_acc: 0.8800\n","466000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2493 - acc: 0.9067 - val_loss: 0.2883 - val_acc: 0.9100\n","468000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2838 - acc: 0.8972 - val_loss: 0.2487 - val_acc: 0.9150\n","470000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2566 - acc: 0.9100 - val_loss: 0.2005 - val_acc: 0.9100\n","472000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2237 - acc: 0.9161 - val_loss: 0.2046 - val_acc: 0.9250\n","474000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2829 - acc: 0.8956 - val_loss: 0.1942 - val_acc: 0.9300\n","476000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2806 - acc: 0.8894 - val_loss: 0.1783 - val_acc: 0.9250\n","478000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2559 - acc: 0.9078 - val_loss: 0.2513 - val_acc: 0.8950\n","480000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2713 - acc: 0.8978 - val_loss: 0.2166 - val_acc: 0.9100\n","482000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2684 - acc: 0.9033 - val_loss: 0.1545 - val_acc: 0.9450\n","484000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2636 - acc: 0.9078 - val_loss: 0.2048 - val_acc: 0.9100\n","486000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2400 - acc: 0.9189 - val_loss: 0.3420 - val_acc: 0.8600\n","488000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2590 - acc: 0.9117 - val_loss: 0.1973 - val_acc: 0.9400\n","490000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2380 - acc: 0.9106 - val_loss: 0.1830 - val_acc: 0.9300\n","492000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2673 - acc: 0.9000 - val_loss: 0.1931 - val_acc: 0.9250\n","494000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2483 - acc: 0.9078 - val_loss: 0.1758 - val_acc: 0.9450\n","496000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2384 - acc: 0.9089 - val_loss: 0.1848 - val_acc: 0.9150\n","498000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2509 - acc: 0.9094 - val_loss: 0.1460 - val_acc: 0.9350\n","500000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2607 - acc: 0.9111 - val_loss: 0.2015 - val_acc: 0.9200\n","502000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2505 - acc: 0.9133 - val_loss: 0.1725 - val_acc: 0.9200\n","504000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2628 - acc: 0.9056 - val_loss: 0.2094 - val_acc: 0.9150\n","506000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2570 - acc: 0.9083 - val_loss: 0.2282 - val_acc: 0.8950\n","508000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2661 - acc: 0.9044 - val_loss: 0.1458 - val_acc: 0.9450\n","510000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2793 - acc: 0.8961 - val_loss: 0.2046 - val_acc: 0.9150\n","512000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2496 - acc: 0.9089 - val_loss: 0.2158 - val_acc: 0.9250\n","514000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2621 - acc: 0.9100 - val_loss: 0.1822 - val_acc: 0.9250\n","516000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2576 - acc: 0.9033 - val_loss: 0.2779 - val_acc: 0.9150\n","518000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2235 - acc: 0.9117 - val_loss: 0.1859 - val_acc: 0.9300\n","520000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2296 - acc: 0.9156 - val_loss: 0.2291 - val_acc: 0.9250\n","522000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2738 - acc: 0.9017 - val_loss: 0.2100 - val_acc: 0.9150\n","524000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2794 - acc: 0.9056 - val_loss: 0.2344 - val_acc: 0.9000\n","526000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2670 - acc: 0.9111 - val_loss: 0.1803 - val_acc: 0.9450\n","528000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2577 - acc: 0.9061 - val_loss: 0.3103 - val_acc: 0.8900\n","530000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2767 - acc: 0.9039 - val_loss: 0.2574 - val_acc: 0.9150\n","532000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2596 - acc: 0.9039 - val_loss: 0.2478 - val_acc: 0.9200\n","534000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2545 - acc: 0.9067 - val_loss: 0.2846 - val_acc: 0.8900\n","536000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2410 - acc: 0.9139 - val_loss: 0.2115 - val_acc: 0.9200\n","538000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2438 - acc: 0.9078 - val_loss: 0.2619 - val_acc: 0.9000\n","540000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2232 - acc: 0.9217 - val_loss: 0.2051 - val_acc: 0.9200\n","542000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2593 - acc: 0.9078 - val_loss: 0.2339 - val_acc: 0.9300\n","544000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2497 - acc: 0.8994 - val_loss: 0.2016 - val_acc: 0.9300\n","546000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2535 - acc: 0.9089 - val_loss: 0.2506 - val_acc: 0.9000\n","548000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2755 - acc: 0.9006 - val_loss: 0.1995 - val_acc: 0.9300\n","550000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2517 - acc: 0.9083 - val_loss: 0.1872 - val_acc: 0.9300\n","552000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2627 - acc: 0.9022 - val_loss: 0.1965 - val_acc: 0.9250\n","554000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2197 - acc: 0.9200 - val_loss: 0.2345 - val_acc: 0.9100\n","556000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2613 - acc: 0.8933 - val_loss: 0.2208 - val_acc: 0.9100\n","558000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2358 - acc: 0.9022 - val_loss: 0.2340 - val_acc: 0.9300\n","560000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2284 - acc: 0.9106 - val_loss: 0.2581 - val_acc: 0.9200\n","562000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2578 - acc: 0.9106 - val_loss: 0.1889 - val_acc: 0.9250\n","564000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2536 - acc: 0.9033 - val_loss: 0.2315 - val_acc: 0.9050\n","566000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2703 - acc: 0.9039 - val_loss: 0.2014 - val_acc: 0.9450\n","568000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2422 - acc: 0.9111 - val_loss: 0.2200 - val_acc: 0.9300\n","570000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2081 - acc: 0.9222 - val_loss: 0.1715 - val_acc: 0.9300\n","572000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2660 - acc: 0.8961 - val_loss: 0.2240 - val_acc: 0.9000\n","574000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2875 - acc: 0.8939 - val_loss: 0.1997 - val_acc: 0.9200\n","576000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2671 - acc: 0.9011 - val_loss: 0.2087 - val_acc: 0.9150\n","578000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2728 - acc: 0.9083 - val_loss: 0.2536 - val_acc: 0.8950\n","580000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2442 - acc: 0.9139 - val_loss: 0.3716 - val_acc: 0.8450\n","582000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2349 - acc: 0.9156 - val_loss: 0.2718 - val_acc: 0.8850\n","584000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2669 - acc: 0.9044 - val_loss: 0.2362 - val_acc: 0.9200\n","586000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2698 - acc: 0.9044 - val_loss: 0.1903 - val_acc: 0.9250\n","588000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2499 - acc: 0.9106 - val_loss: 0.2442 - val_acc: 0.9200\n","590000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2896 - acc: 0.8933 - val_loss: 0.1874 - val_acc: 0.9150\n","592000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2303 - acc: 0.9117 - val_loss: 0.1911 - val_acc: 0.9300\n","594000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2637 - acc: 0.9094 - val_loss: 0.1416 - val_acc: 0.9450\n","596000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2620 - acc: 0.8994 - val_loss: 0.2699 - val_acc: 0.9000\n","598000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2393 - acc: 0.9089 - val_loss: 0.2139 - val_acc: 0.9150\n","600000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2297 - acc: 0.9117 - val_loss: 0.2456 - val_acc: 0.9000\n","602000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2380 - acc: 0.9117 - val_loss: 0.1526 - val_acc: 0.9450\n","604000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2613 - acc: 0.9106 - val_loss: 0.3233 - val_acc: 0.8900\n","606000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2599 - acc: 0.9033 - val_loss: 0.2080 - val_acc: 0.9200\n","608000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2576 - acc: 0.9106 - val_loss: 0.1689 - val_acc: 0.9400\n","610000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.3040 - acc: 0.8933 - val_loss: 0.2934 - val_acc: 0.9000\n","612000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2625 - acc: 0.9044 - val_loss: 0.2540 - val_acc: 0.9100\n","614000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2913 - acc: 0.9028 - val_loss: 0.2268 - val_acc: 0.8950\n","616000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2475 - acc: 0.9028 - val_loss: 0.1883 - val_acc: 0.9300\n","618000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2909 - acc: 0.8933 - val_loss: 0.2349 - val_acc: 0.9150\n","620000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2516 - acc: 0.9061 - val_loss: 0.2435 - val_acc: 0.9100\n","622000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2325 - acc: 0.9133 - val_loss: 0.2463 - val_acc: 0.9000\n","624000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2664 - acc: 0.9011 - val_loss: 0.1623 - val_acc: 0.9300\n","626000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2516 - acc: 0.9111 - val_loss: 0.2457 - val_acc: 0.8950\n","628000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2850 - acc: 0.8928 - val_loss: 0.2867 - val_acc: 0.8850\n","630000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2191 - acc: 0.9250 - val_loss: 0.2825 - val_acc: 0.8750\n","632000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2806 - acc: 0.8939 - val_loss: 0.2772 - val_acc: 0.9000\n","634000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2429 - acc: 0.9106 - val_loss: 0.3034 - val_acc: 0.9100\n","636000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2590 - acc: 0.9083 - val_loss: 0.2271 - val_acc: 0.9150\n","638000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2651 - acc: 0.8972 - val_loss: 0.1664 - val_acc: 0.9300\n","640000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2697 - acc: 0.9050 - val_loss: 0.2392 - val_acc: 0.9250\n","642000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2511 - acc: 0.9156 - val_loss: 0.2328 - val_acc: 0.9000\n","644000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2581 - acc: 0.9106 - val_loss: 0.2240 - val_acc: 0.9100\n","646000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2527 - acc: 0.9039 - val_loss: 0.1912 - val_acc: 0.9350\n","648000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2217 - acc: 0.9217 - val_loss: 0.2942 - val_acc: 0.8900\n","650000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2586 - acc: 0.8994 - val_loss: 0.1491 - val_acc: 0.9500\n","652000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2687 - acc: 0.9083 - val_loss: 0.1799 - val_acc: 0.9350\n","654000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2507 - acc: 0.9111 - val_loss: 0.3184 - val_acc: 0.9100\n","656000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2701 - acc: 0.9056 - val_loss: 0.2128 - val_acc: 0.9200\n","658000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2695 - acc: 0.9011 - val_loss: 0.3187 - val_acc: 0.8700\n","660000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2276 - acc: 0.9167 - val_loss: 0.2634 - val_acc: 0.8900\n","662000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2335 - acc: 0.9089 - val_loss: 0.2142 - val_acc: 0.9200\n","664000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2502 - acc: 0.9128 - val_loss: 0.3340 - val_acc: 0.8850\n","666000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2475 - acc: 0.9067 - val_loss: 0.1959 - val_acc: 0.9400\n","668000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2937 - acc: 0.8872 - val_loss: 0.2095 - val_acc: 0.9200\n","670000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2811 - acc: 0.8967 - val_loss: 0.2440 - val_acc: 0.9100\n","672000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2601 - acc: 0.9100 - val_loss: 0.2711 - val_acc: 0.9050\n","674000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2708 - acc: 0.8983 - val_loss: 0.3323 - val_acc: 0.8900\n","676000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2386 - acc: 0.9144 - val_loss: 0.1437 - val_acc: 0.9500\n","678000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2265 - acc: 0.9111 - val_loss: 0.2252 - val_acc: 0.9100\n","680000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2623 - acc: 0.9056 - val_loss: 0.2369 - val_acc: 0.9100\n","682000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2331 - acc: 0.9106 - val_loss: 0.2353 - val_acc: 0.9250\n","684000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2467 - acc: 0.9133 - val_loss: 0.3392 - val_acc: 0.8850\n","686000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2503 - acc: 0.9067 - val_loss: 0.2418 - val_acc: 0.8800\n","688000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2634 - acc: 0.9022 - val_loss: 0.2859 - val_acc: 0.8850\n","690000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2337 - acc: 0.9089 - val_loss: 0.2355 - val_acc: 0.9150\n","692000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2390 - acc: 0.9144 - val_loss: 0.3139 - val_acc: 0.8650\n","694000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2573 - acc: 0.9022 - val_loss: 0.2403 - val_acc: 0.9150\n","696000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2631 - acc: 0.8994 - val_loss: 0.2192 - val_acc: 0.9300\n","698000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2435 - acc: 0.9044 - val_loss: 0.2923 - val_acc: 0.9300\n","700000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2633 - acc: 0.9050 - val_loss: 0.3079 - val_acc: 0.8900\n","702000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2757 - acc: 0.8978 - val_loss: 0.2804 - val_acc: 0.9100\n","704000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2738 - acc: 0.8961 - val_loss: 0.2556 - val_acc: 0.9250\n","706000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2685 - acc: 0.8967 - val_loss: 0.2020 - val_acc: 0.9050\n","708000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2911 - acc: 0.8922 - val_loss: 0.2843 - val_acc: 0.8950\n","710000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2364 - acc: 0.9133 - val_loss: 0.2434 - val_acc: 0.9150\n","712000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2585 - acc: 0.8989 - val_loss: 0.1810 - val_acc: 0.9200\n","714000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2403 - acc: 0.9139 - val_loss: 0.1114 - val_acc: 0.9550\n","716000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2306 - acc: 0.9106 - val_loss: 0.1890 - val_acc: 0.9250\n","718000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2594 - acc: 0.9094 - val_loss: 0.1803 - val_acc: 0.9350\n","720000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2560 - acc: 0.9050 - val_loss: 0.1713 - val_acc: 0.9350\n","722000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2637 - acc: 0.9061 - val_loss: 0.2002 - val_acc: 0.9300\n","724000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2814 - acc: 0.8917 - val_loss: 0.1644 - val_acc: 0.9350\n","726000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2598 - acc: 0.9089 - val_loss: 0.2003 - val_acc: 0.9100\n","728000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2619 - acc: 0.9039 - val_loss: 0.2054 - val_acc: 0.9100\n","730000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2668 - acc: 0.8972 - val_loss: 0.2369 - val_acc: 0.9150\n","732000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2416 - acc: 0.9139 - val_loss: 0.2220 - val_acc: 0.9150\n","734000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2450 - acc: 0.9094 - val_loss: 0.2035 - val_acc: 0.9200\n","736000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2824 - acc: 0.9017 - val_loss: 0.2478 - val_acc: 0.9000\n","738000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2781 - acc: 0.9017 - val_loss: 0.2274 - val_acc: 0.9000\n","740000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2579 - acc: 0.9061 - val_loss: 0.2253 - val_acc: 0.9300\n","742000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2530 - acc: 0.9011 - val_loss: 0.2077 - val_acc: 0.9450\n","744000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2570 - acc: 0.9083 - val_loss: 0.2524 - val_acc: 0.9000\n","746000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2652 - acc: 0.9028 - val_loss: 0.1345 - val_acc: 0.9450\n","748000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2738 - acc: 0.9067 - val_loss: 0.1896 - val_acc: 0.9300\n","750000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2314 - acc: 0.9189 - val_loss: 0.1581 - val_acc: 0.9300\n","752000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2721 - acc: 0.9017 - val_loss: 0.2335 - val_acc: 0.9050\n","754000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2416 - acc: 0.9083 - val_loss: 0.1904 - val_acc: 0.9250\n","756000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2385 - acc: 0.9122 - val_loss: 0.2442 - val_acc: 0.9100\n","758000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2565 - acc: 0.9022 - val_loss: 0.2141 - val_acc: 0.8950\n","760000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2734 - acc: 0.9000 - val_loss: 0.1895 - val_acc: 0.9250\n","762000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2681 - acc: 0.9017 - val_loss: 0.2952 - val_acc: 0.8950\n","764000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2572 - acc: 0.9067 - val_loss: 0.1943 - val_acc: 0.9200\n","766000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2806 - acc: 0.8972 - val_loss: 0.1728 - val_acc: 0.9400\n","768000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2628 - acc: 0.9050 - val_loss: 0.2916 - val_acc: 0.9000\n","770000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2458 - acc: 0.9117 - val_loss: 0.2055 - val_acc: 0.9150\n","772000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2495 - acc: 0.9144 - val_loss: 0.2601 - val_acc: 0.8750\n","774000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2534 - acc: 0.9078 - val_loss: 0.2000 - val_acc: 0.9250\n","776000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2507 - acc: 0.9017 - val_loss: 0.2659 - val_acc: 0.9200\n","778000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2687 - acc: 0.9017 - val_loss: 0.1799 - val_acc: 0.9350\n","780000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2623 - acc: 0.9022 - val_loss: 0.1863 - val_acc: 0.9400\n","782000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2417 - acc: 0.9144 - val_loss: 0.1441 - val_acc: 0.9550\n","784000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2345 - acc: 0.9072 - val_loss: 0.1794 - val_acc: 0.9400\n","786000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2674 - acc: 0.8989 - val_loss: 0.3015 - val_acc: 0.8850\n","788000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2266 - acc: 0.9156 - val_loss: 0.1880 - val_acc: 0.9150\n","790000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2424 - acc: 0.9117 - val_loss: 0.2447 - val_acc: 0.8900\n","792000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2452 - acc: 0.9094 - val_loss: 0.2043 - val_acc: 0.9350\n","794000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2439 - acc: 0.9189 - val_loss: 0.1852 - val_acc: 0.9400\n","796000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2182 - acc: 0.9133 - val_loss: 0.1967 - val_acc: 0.9200\n","798000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2429 - acc: 0.9083 - val_loss: 0.3038 - val_acc: 0.8950\n","800000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2521 - acc: 0.9050 - val_loss: 0.2727 - val_acc: 0.9150\n","802000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2680 - acc: 0.9000 - val_loss: 0.2975 - val_acc: 0.9150\n","804000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2740 - acc: 0.9039 - val_loss: 0.1709 - val_acc: 0.9300\n","806000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2219 - acc: 0.9183 - val_loss: 0.1653 - val_acc: 0.9250\n","808000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2570 - acc: 0.9106 - val_loss: 0.3243 - val_acc: 0.8750\n","810000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2813 - acc: 0.8989 - val_loss: 0.2264 - val_acc: 0.8950\n","812000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2300 - acc: 0.9089 - val_loss: 0.1980 - val_acc: 0.9150\n","814000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2285 - acc: 0.9144 - val_loss: 0.3081 - val_acc: 0.8800\n","816000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2495 - acc: 0.9061 - val_loss: 0.1613 - val_acc: 0.9300\n","818000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2739 - acc: 0.8978 - val_loss: 0.2434 - val_acc: 0.9150\n","820000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2847 - acc: 0.8950 - val_loss: 0.2856 - val_acc: 0.9150\n","822000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2359 - acc: 0.9078 - val_loss: 0.1696 - val_acc: 0.9350\n","824000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2545 - acc: 0.9106 - val_loss: 0.1481 - val_acc: 0.9350\n","826000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2555 - acc: 0.9122 - val_loss: 0.2835 - val_acc: 0.8900\n","828000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2078 - acc: 0.9222 - val_loss: 0.2656 - val_acc: 0.9100\n","830000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2721 - acc: 0.9078 - val_loss: 0.1359 - val_acc: 0.9600\n","832000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2592 - acc: 0.8994 - val_loss: 0.2418 - val_acc: 0.9100\n","834000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2490 - acc: 0.8989 - val_loss: 0.2692 - val_acc: 0.8900\n","836000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2301 - acc: 0.9167 - val_loss: 0.2074 - val_acc: 0.9250\n","838000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2327 - acc: 0.9067 - val_loss: 0.1928 - val_acc: 0.9250\n","840000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2397 - acc: 0.9117 - val_loss: 0.1446 - val_acc: 0.9550\n","842000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2153 - acc: 0.9267 - val_loss: 0.2729 - val_acc: 0.8850\n","844000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2427 - acc: 0.9122 - val_loss: 0.2204 - val_acc: 0.9200\n","846000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2599 - acc: 0.9117 - val_loss: 0.2367 - val_acc: 0.9000\n","848000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2534 - acc: 0.9050 - val_loss: 0.2785 - val_acc: 0.9050\n","850000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2429 - acc: 0.9117 - val_loss: 0.2152 - val_acc: 0.9150\n","852000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2414 - acc: 0.9106 - val_loss: 0.3205 - val_acc: 0.9200\n","854000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2661 - acc: 0.8994 - val_loss: 0.2224 - val_acc: 0.9300\n","856000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2539 - acc: 0.9094 - val_loss: 0.2084 - val_acc: 0.9150\n","858000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2583 - acc: 0.8994 - val_loss: 0.1853 - val_acc: 0.9150\n","860000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2585 - acc: 0.9089 - val_loss: 0.2459 - val_acc: 0.9150\n","862000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2461 - acc: 0.9078 - val_loss: 0.2747 - val_acc: 0.9100\n","864000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2521 - acc: 0.9067 - val_loss: 0.1710 - val_acc: 0.9250\n","866000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2367 - acc: 0.9139 - val_loss: 0.2036 - val_acc: 0.9250\n","868000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2389 - acc: 0.9033 - val_loss: 0.3062 - val_acc: 0.9000\n","870000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2503 - acc: 0.9083 - val_loss: 0.1972 - val_acc: 0.9250\n","872000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2333 - acc: 0.9094 - val_loss: 0.1932 - val_acc: 0.9250\n","874000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2506 - acc: 0.9061 - val_loss: 0.2555 - val_acc: 0.9000\n","876000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2413 - acc: 0.9039 - val_loss: 0.2275 - val_acc: 0.9050\n","878000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2539 - acc: 0.9161 - val_loss: 0.2844 - val_acc: 0.8950\n","880000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2505 - acc: 0.9017 - val_loss: 0.2491 - val_acc: 0.9000\n","882000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2362 - acc: 0.9111 - val_loss: 0.2189 - val_acc: 0.9250\n","884000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2528 - acc: 0.9094 - val_loss: 0.2736 - val_acc: 0.9150\n","886000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2657 - acc: 0.9033 - val_loss: 0.2264 - val_acc: 0.9000\n","888000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2633 - acc: 0.9011 - val_loss: 0.2526 - val_acc: 0.8850\n","890000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2376 - acc: 0.9111 - val_loss: 0.1796 - val_acc: 0.9200\n","892000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2496 - acc: 0.9072 - val_loss: 0.2611 - val_acc: 0.9050\n","894000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2793 - acc: 0.9067 - val_loss: 0.1708 - val_acc: 0.9400\n","896000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2515 - acc: 0.9100 - val_loss: 0.2039 - val_acc: 0.9250\n","898000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2685 - acc: 0.9011 - val_loss: 0.2376 - val_acc: 0.9050\n","900000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2322 - acc: 0.9139 - val_loss: 0.2466 - val_acc: 0.9000\n","902000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2495 - acc: 0.9039 - val_loss: 0.2539 - val_acc: 0.9100\n","904000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2292 - acc: 0.9122 - val_loss: 0.2638 - val_acc: 0.9050\n","906000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2465 - acc: 0.9128 - val_loss: 0.3628 - val_acc: 0.8600\n","908000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2306 - acc: 0.9144 - val_loss: 0.3336 - val_acc: 0.8750\n","910000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2602 - acc: 0.9017 - val_loss: 0.1960 - val_acc: 0.9150\n","912000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2457 - acc: 0.9078 - val_loss: 0.2024 - val_acc: 0.9250\n","914000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2665 - acc: 0.9061 - val_loss: 0.2799 - val_acc: 0.9000\n","916000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2437 - acc: 0.9094 - val_loss: 0.2385 - val_acc: 0.9000\n","918000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2529 - acc: 0.9133 - val_loss: 0.3012 - val_acc: 0.8600\n","920000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2219 - acc: 0.9194 - val_loss: 0.1268 - val_acc: 0.9550\n","922000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2517 - acc: 0.9150 - val_loss: 0.1779 - val_acc: 0.9400\n","924000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2515 - acc: 0.9089 - val_loss: 0.2439 - val_acc: 0.8950\n","926000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2828 - acc: 0.8967 - val_loss: 0.2621 - val_acc: 0.9100\n","928000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2501 - acc: 0.9117 - val_loss: 0.2553 - val_acc: 0.9100\n","930000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2729 - acc: 0.9000 - val_loss: 0.1226 - val_acc: 0.9600\n","932000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2551 - acc: 0.9028 - val_loss: 0.2339 - val_acc: 0.9250\n","934000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2395 - acc: 0.9111 - val_loss: 0.1979 - val_acc: 0.9250\n","936000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2851 - acc: 0.8989 - val_loss: 0.2116 - val_acc: 0.9050\n","938000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2231 - acc: 0.9167 - val_loss: 0.3004 - val_acc: 0.9050\n","940000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2777 - acc: 0.8917 - val_loss: 0.1731 - val_acc: 0.9450\n","942000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2543 - acc: 0.9100 - val_loss: 0.2046 - val_acc: 0.9200\n","944000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2715 - acc: 0.9061 - val_loss: 0.2788 - val_acc: 0.8800\n","946000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2538 - acc: 0.9094 - val_loss: 0.2176 - val_acc: 0.9250\n","948000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2542 - acc: 0.9022 - val_loss: 0.2781 - val_acc: 0.9050\n","950000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2504 - acc: 0.9078 - val_loss: 0.2635 - val_acc: 0.9000\n","952000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2329 - acc: 0.9117 - val_loss: 0.2919 - val_acc: 0.9000\n","954000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2600 - acc: 0.9044 - val_loss: 0.1481 - val_acc: 0.9400\n","956000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2472 - acc: 0.9122 - val_loss: 0.2121 - val_acc: 0.9250\n","958000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2471 - acc: 0.9167 - val_loss: 0.2875 - val_acc: 0.8800\n","960000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2738 - acc: 0.8994 - val_loss: 0.2362 - val_acc: 0.8800\n","962000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2339 - acc: 0.9144 - val_loss: 0.2129 - val_acc: 0.9300\n","964000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2516 - acc: 0.9033 - val_loss: 0.2313 - val_acc: 0.8750\n","966000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2547 - acc: 0.9050 - val_loss: 0.2113 - val_acc: 0.9400\n","968000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2371 - acc: 0.9144 - val_loss: 0.1720 - val_acc: 0.9200\n","970000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2167 - acc: 0.9183 - val_loss: 0.2957 - val_acc: 0.8950\n","972000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2296 - acc: 0.9250 - val_loss: 0.3273 - val_acc: 0.8800\n","974000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2556 - acc: 0.9150 - val_loss: 0.2280 - val_acc: 0.9150\n","976000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2353 - acc: 0.9072 - val_loss: 0.1781 - val_acc: 0.9350\n","978000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2412 - acc: 0.9167 - val_loss: 0.2619 - val_acc: 0.9000\n","980000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2694 - acc: 0.8978 - val_loss: 0.1400 - val_acc: 0.9450\n","982000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2747 - acc: 0.9067 - val_loss: 0.2253 - val_acc: 0.9100\n","984000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2774 - acc: 0.8978 - val_loss: 0.2036 - val_acc: 0.9100\n","986000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2677 - acc: 0.9006 - val_loss: 0.2229 - val_acc: 0.9050\n","988000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2590 - acc: 0.9078 - val_loss: 0.2908 - val_acc: 0.9050\n","990000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2673 - acc: 0.9050 - val_loss: 0.2031 - val_acc: 0.9400\n","992000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2304 - acc: 0.9194 - val_loss: 0.1579 - val_acc: 0.9400\n","994000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2587 - acc: 0.9094 - val_loss: 0.2911 - val_acc: 0.8900\n","996000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2735 - acc: 0.8978 - val_loss: 0.2883 - val_acc: 0.9050\n","998000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2562 - acc: 0.9094 - val_loss: 0.1644 - val_acc: 0.9350\n","1000000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2670 - acc: 0.9022 - val_loss: 0.2138 - val_acc: 0.9350\n","\n"," Train Loop Number 5\n","2000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2423 - acc: 0.9094 - val_loss: 0.2573 - val_acc: 0.9000\n","4000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2638 - acc: 0.9033 - val_loss: 0.3053 - val_acc: 0.9000\n","6000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2465 - acc: 0.9106 - val_loss: 0.2747 - val_acc: 0.9200\n","8000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2610 - acc: 0.9083 - val_loss: 0.2453 - val_acc: 0.9100\n","10000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2674 - acc: 0.9006 - val_loss: 0.2886 - val_acc: 0.9000\n","12000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2394 - acc: 0.9117 - val_loss: 0.2499 - val_acc: 0.9300\n","14000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2635 - acc: 0.9000 - val_loss: 0.2836 - val_acc: 0.9050\n","16000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2379 - acc: 0.9122 - val_loss: 0.2311 - val_acc: 0.9050\n","18000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2377 - acc: 0.9133 - val_loss: 0.1859 - val_acc: 0.9000\n","20000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2439 - acc: 0.9117 - val_loss: 0.1928 - val_acc: 0.9300\n","22000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2479 - acc: 0.9094 - val_loss: 0.1699 - val_acc: 0.9250\n","24000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2693 - acc: 0.9017 - val_loss: 0.2487 - val_acc: 0.9150\n","26000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2338 - acc: 0.9122 - val_loss: 0.2515 - val_acc: 0.8800\n","28000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2415 - acc: 0.9083 - val_loss: 0.2701 - val_acc: 0.9250\n","30000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2515 - acc: 0.9039 - val_loss: 0.2952 - val_acc: 0.8900\n","32000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2553 - acc: 0.9139 - val_loss: 0.2990 - val_acc: 0.9000\n","34000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2796 - acc: 0.9061 - val_loss: 0.2086 - val_acc: 0.9200\n","36000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2607 - acc: 0.9117 - val_loss: 0.2752 - val_acc: 0.8850\n","38000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2090 - acc: 0.9178 - val_loss: 0.2407 - val_acc: 0.9200\n","40000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2348 - acc: 0.9133 - val_loss: 0.2776 - val_acc: 0.8900\n","42000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2521 - acc: 0.8994 - val_loss: 0.1750 - val_acc: 0.9150\n","44000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2257 - acc: 0.9150 - val_loss: 0.2051 - val_acc: 0.9300\n","46000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2507 - acc: 0.9078 - val_loss: 0.2280 - val_acc: 0.9200\n","48000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2543 - acc: 0.9194 - val_loss: 0.2167 - val_acc: 0.9150\n","50000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2343 - acc: 0.9133 - val_loss: 0.2208 - val_acc: 0.9000\n","52000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2569 - acc: 0.9011 - val_loss: 0.2825 - val_acc: 0.8700\n","54000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2656 - acc: 0.8939 - val_loss: 0.3062 - val_acc: 0.8950\n","56000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2485 - acc: 0.9078 - val_loss: 0.2281 - val_acc: 0.9100\n","58000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2532 - acc: 0.9100 - val_loss: 0.2434 - val_acc: 0.9250\n","60000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2384 - acc: 0.9161 - val_loss: 0.2320 - val_acc: 0.9150\n","62000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2501 - acc: 0.9033 - val_loss: 0.2591 - val_acc: 0.9150\n","64000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2545 - acc: 0.9067 - val_loss: 0.3117 - val_acc: 0.8850\n","66000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2462 - acc: 0.9067 - val_loss: 0.2214 - val_acc: 0.9200\n","68000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2668 - acc: 0.8967 - val_loss: 0.2765 - val_acc: 0.8700\n","70000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2428 - acc: 0.9072 - val_loss: 0.2114 - val_acc: 0.9350\n","72000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2435 - acc: 0.9139 - val_loss: 0.2544 - val_acc: 0.8950\n","74000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2487 - acc: 0.9106 - val_loss: 0.2838 - val_acc: 0.8850\n","76000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2540 - acc: 0.9083 - val_loss: 0.2116 - val_acc: 0.9200\n","78000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2437 - acc: 0.9078 - val_loss: 0.2857 - val_acc: 0.9000\n","80000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2398 - acc: 0.9122 - val_loss: 0.2031 - val_acc: 0.9400\n","82000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2533 - acc: 0.9083 - val_loss: 0.3507 - val_acc: 0.8750\n","84000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2418 - acc: 0.9117 - val_loss: 0.3207 - val_acc: 0.8750\n","86000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2326 - acc: 0.9156 - val_loss: 0.1949 - val_acc: 0.9200\n","88000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2390 - acc: 0.9061 - val_loss: 0.2392 - val_acc: 0.9150\n","90000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2370 - acc: 0.9100 - val_loss: 0.2202 - val_acc: 0.9100\n","92000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2540 - acc: 0.9033 - val_loss: 0.2012 - val_acc: 0.9350\n","94000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2660 - acc: 0.8994 - val_loss: 0.1992 - val_acc: 0.9250\n","96000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2503 - acc: 0.9167 - val_loss: 0.1388 - val_acc: 0.9550\n","98000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2421 - acc: 0.9139 - val_loss: 0.2019 - val_acc: 0.9200\n","100000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2640 - acc: 0.9044 - val_loss: 0.2051 - val_acc: 0.9150\n","102000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2496 - acc: 0.9089 - val_loss: 0.2726 - val_acc: 0.8950\n","104000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2496 - acc: 0.9117 - val_loss: 0.1941 - val_acc: 0.9250\n","106000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2602 - acc: 0.9011 - val_loss: 0.2361 - val_acc: 0.9250\n","108000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2067 - acc: 0.9183 - val_loss: 0.3057 - val_acc: 0.8950\n","110000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2454 - acc: 0.9106 - val_loss: 0.3192 - val_acc: 0.8750\n","112000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2574 - acc: 0.9067 - val_loss: 0.1222 - val_acc: 0.9700\n","114000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2325 - acc: 0.9183 - val_loss: 0.2246 - val_acc: 0.8950\n","116000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2527 - acc: 0.9106 - val_loss: 0.1779 - val_acc: 0.9450\n","118000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2400 - acc: 0.9117 - val_loss: 0.2609 - val_acc: 0.9200\n","120000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2261 - acc: 0.9167 - val_loss: 0.1078 - val_acc: 0.9550\n","122000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2462 - acc: 0.9078 - val_loss: 0.1900 - val_acc: 0.9100\n","124000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2684 - acc: 0.9111 - val_loss: 0.3336 - val_acc: 0.8800\n","126000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2207 - acc: 0.9206 - val_loss: 0.1657 - val_acc: 0.9300\n","128000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2464 - acc: 0.9167 - val_loss: 0.1867 - val_acc: 0.9200\n","130000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2493 - acc: 0.9078 - val_loss: 0.2634 - val_acc: 0.8950\n","132000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2608 - acc: 0.9022 - val_loss: 0.2063 - val_acc: 0.9150\n","134000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2426 - acc: 0.9111 - val_loss: 0.1830 - val_acc: 0.9350\n","136000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2206 - acc: 0.9228 - val_loss: 0.1912 - val_acc: 0.9300\n","138000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2581 - acc: 0.9033 - val_loss: 0.1635 - val_acc: 0.9500\n","140000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2054 - acc: 0.9300 - val_loss: 0.1885 - val_acc: 0.9400\n","142000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2307 - acc: 0.9128 - val_loss: 0.2071 - val_acc: 0.9350\n","144000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2653 - acc: 0.8967 - val_loss: 0.2332 - val_acc: 0.9000\n","146000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2050 - acc: 0.9272 - val_loss: 0.1919 - val_acc: 0.9150\n","148000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2291 - acc: 0.9094 - val_loss: 0.2151 - val_acc: 0.9250\n","150000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2349 - acc: 0.9044 - val_loss: 0.1918 - val_acc: 0.9100\n","152000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2434 - acc: 0.9189 - val_loss: 0.1753 - val_acc: 0.9350\n","154000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2775 - acc: 0.9011 - val_loss: 0.2543 - val_acc: 0.9150\n","156000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2238 - acc: 0.9194 - val_loss: 0.2777 - val_acc: 0.9050\n","158000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2644 - acc: 0.9000 - val_loss: 0.2941 - val_acc: 0.8700\n","160000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2524 - acc: 0.8994 - val_loss: 0.2127 - val_acc: 0.9250\n","162000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2764 - acc: 0.8956 - val_loss: 0.2310 - val_acc: 0.9100\n","164000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2298 - acc: 0.9050 - val_loss: 0.1726 - val_acc: 0.9250\n","166000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2115 - acc: 0.9300 - val_loss: 0.1662 - val_acc: 0.9500\n","168000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2628 - acc: 0.8978 - val_loss: 0.2324 - val_acc: 0.9300\n","170000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2349 - acc: 0.9106 - val_loss: 0.1509 - val_acc: 0.9400\n","172000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2180 - acc: 0.9200 - val_loss: 0.1707 - val_acc: 0.9450\n","174000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2107 - acc: 0.9256 - val_loss: 0.2972 - val_acc: 0.8650\n","176000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2597 - acc: 0.9089 - val_loss: 0.2658 - val_acc: 0.9050\n","178000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2560 - acc: 0.9061 - val_loss: 0.2922 - val_acc: 0.8950\n","180000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2227 - acc: 0.9156 - val_loss: 0.1478 - val_acc: 0.9300\n","182000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2235 - acc: 0.9222 - val_loss: 0.3187 - val_acc: 0.8650\n","184000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2565 - acc: 0.9022 - val_loss: 0.2644 - val_acc: 0.9100\n","186000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2151 - acc: 0.9217 - val_loss: 0.1715 - val_acc: 0.9350\n","188000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2263 - acc: 0.9156 - val_loss: 0.1692 - val_acc: 0.9400\n","190000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2238 - acc: 0.9256 - val_loss: 0.2566 - val_acc: 0.9100\n","192000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2555 - acc: 0.9094 - val_loss: 0.2729 - val_acc: 0.9200\n","194000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2545 - acc: 0.9094 - val_loss: 0.2458 - val_acc: 0.9050\n","196000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2585 - acc: 0.9100 - val_loss: 0.2549 - val_acc: 0.9150\n","198000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2530 - acc: 0.9072 - val_loss: 0.2010 - val_acc: 0.9150\n","200000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1800/1800 [==============================] - 3s 1ms/step - loss: 0.2621 - acc: 0.9000 - val_loss: 0.2257 - val_acc: 0.9150\n","202000 Train on 1800 samples, validate on 200 samples\n","Epoch 1/1\n","1536/1800 [========================>.....] - ETA: 0s - loss: 0.2438 - acc: 0.9141Training Interrupted Saving Model with 202000 Iterations/patient\n"],"name":"stdout"}]},{"metadata":{"id":"ifLnhK7zMmnp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e9499e93-a788-48cf-dd19-860911c2e82b","executionInfo":{"status":"ok","timestamp":1555939881366,"user_tz":-480,"elapsed":725,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}}},"cell_type":"code","source":["len(train_hist_acc)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2600"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"upeI4h3KPrx_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"534ee6ed-5df3-4048-cd46-fd5ffa3a780d","executionInfo":{"status":"ok","timestamp":1555940566882,"user_tz":-480,"elapsed":2215,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}}},"cell_type":"code","source":["plt.plot(train_hist_acc[:2500])\n","plt.title(\"Training Accuracy\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Accuracy\")\n","plt.savefig('drive/My Drive/MRI_Brain_Segmentation/train_acc.png', dpi=200)"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"dC3ZQMsuQOou","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"8c46a555-b21e-4532-b1ce-16b5b9b7abca","executionInfo":{"status":"ok","timestamp":1555940587197,"user_tz":-480,"elapsed":1650,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}}},"cell_type":"code","source":["plt.plot(train_hist_val[:2500], 'r')\n","plt.title(\"Validation Accuracy\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Accuracy\")\n","plt.savefig('drive/My Drive/MRI_Brain_Segmentation/train_val_acc.png', dpi=200)"],"execution_count":29,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4UZSNuH6AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"eZx3K3-Ro0EH","colab_type":"text"},"cell_type":"markdown","source":["Testing Part\n"]},{"metadata":{"id":"EEWVk0e67S6J","colab_type":"code","outputId":"345361ae-c6b5-4a23-b8e5-9bf98e35c95e","executionInfo":{"status":"ok","timestamp":1555566622941,"user_tz":-480,"elapsed":3023,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/My Drive/lib')\n","\n","import keras\n","from keras import layers\n","from keras import models\n","from keras import optimizers\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras import backend\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import math\n","\n","from build_resnet import buildResBlock"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"66kmWUXF7isK","colab_type":"code","outputId":"8f16274a-a3b4-42c6-8659-cf70331f02e3","executionInfo":{"status":"ok","timestamp":1555566725262,"user_tz":-480,"elapsed":25125,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","import time\n","from numpy import array\n","import keras\n","import sys\n","import copy\n","import matplotlib.pyplot as plt\n","\n","\n","def whitening(image):\n","    \"\"\"Whitening. Normalises image to zero mean and unit variance.\"\"\"\n","\n","    image = image.astype(np.float32)\n","\n","    mean = np.mean(image)\n","    std = np.std(image)\n","\n","    if std > 0:\n","        ret = (image - mean) / std\n","    else:\n","        ret = image * 0.\n","    return ret\n","\n","def histeq(im,nbr_bins=256):\n","  \"\"\"This is for image equalization\"\"\"\n","  #get image histogram\n","  imhist,bins = np.histogram(im.flatten(),nbr_bins,normed=True)\n","   \n","  cdf = imhist.cumsum() #cumulative distribution function\n","  \n","  cdf_m = np.ma.masked_equal(cdf,0)#mask the background voxels \n","    \n","  # the main step of histogram equalization\n","  cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min()) \n","   \n","  cdf = np.ma.filled(cdf_m,0).astype('uint8') # set the removed background pixels back to 0\n","\n","  #use linear interpolation of cdf to find new pixel values\n","    \n","  # im2 = np.interp(im.flatten(),bins[:-1],cdf)/255 # this line can cause the program to fail\n","  # ############################################## because it returns a float64 and run out of RAM\n","  im2 = (np.interp(im.flatten(),bins[:-1],cdf)/255).astype(np.float32)\n","\n","  return im2.reshape(im.shape), cdf\n","  \n","  \n","\n","#-------------------------------\n","# TESTING the trained network\n","#-------------------------------\n","\n","#define the test file\n","# To carry out the testing one file at a time (with 5 patients)\n","path = \"drive/My Drive/MRI_Brain_Segmentation/Dataset_final_test_1.npy\"\n","#Load the Dataset \n","dataset = np.load(path)\n","\n","# READ in the test dataset, do the padding, and the two normalizations\n","# histeq returns two output items, and only the first output is needed,ignore the 2nd one.\n","# Normalize and Pad the Data (Preprocessing)\n","npad = ((43,43), (43,43), (43,43))\n","#\n","pred_labelimage = []  # Making a copy of the image for segmentation, \n","            # at the end, it will be the label for each voxel\n","#\n","for i in range (0, len(dataset)):\n","  dataset[i][0] = np.pad(dataset[i][0], pad_width=npad, mode='constant', constant_values=0)\n","  dataset[i][1] = np.pad(dataset[i][1], pad_width=npad, mode='constant', constant_values=0)\n","  # Histogram equalization\n","  dataset[i][0], _ = histeq(dataset[i][0])\n","  \n","  #dataset[i][0] = whitening(dataset[i][0])\n","  \n","  #image.append(dataset[i][1])\n","  #print(image[i][180][180][145:160])\n","  print(\"resetting pred_labelimage[\" + str(i) + \"] to zero:\")\n","  \n","  pred_labelimage.append(0*dataset[i][1])\n","  #print(pred_labelimage[i][180][180][145:160])\n","    \n","#Load the Ouput File\n","# image[0] is the first patient\n","print(\"pred_labelimage Shape is: \", pred_labelimage[0].shape)\n","print(\"pred_labelimage len is: \", len(pred_labelimage))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"],"name":"stderr"},{"output_type":"stream","text":["resetting pred_labelimage[0] to zero:\n","resetting pred_labelimage[1] to zero:\n","resetting pred_labelimage[2] to zero:\n","resetting pred_labelimage[3] to zero:\n","resetting pred_labelimage[4] to zero:\n","pred_labelimage Shape is:  (402, 342, 342)\n","pred_labelimage len is:  5\n"],"name":"stdout"}]},{"metadata":{"id":"I-FNYPoPtzbO","colab_type":"code","outputId":"dba4c6f8-c096-47f4-ec5c-e44b39f7f404","executionInfo":{"status":"ok","timestamp":1555567491405,"user_tz":-480,"elapsed":695983,"user":{"displayName":"Chi Leong Benjamin WAN","photoUrl":"","userId":"05191805912806198221"}},"colab":{"base_uri":"https://localhost:8080/","height":11854}},"cell_type":"code","source":["import gc\n","import copy\n","import matplotlib.pyplot as plt\n","\n","# To reload the model parameters\n","\n","t1=[]\n","t2=[]\n","t3=[]\n","t4=[]\n","t5=[]\n","t6=[]\n","t7=[]\n","t8=[]\n","t9=[]\n","t10=[]\n","t11=[]\n","\n","pred_samples_pool = 2000\n","pred_batch_size = 32\n","\n","patient_start = 0\n","#patient_end = len(dataset)\n","patient_end = 1\n","\n","print(\"---------------------Start the Prediction: ------------------------\")\n","for i in range(patient_start,patient_end):   \n","  print(\"Test Patient Number \" + str(i))\n","  total = np.size(dataset[i][0])\n","  print(\"total number of voxels in the Patient = \",total)\n","  # image[i] contains the labels of the dataset\n","  #true_labels = copy.deepcopy(image[i])  #this command is needed for making a clean copy\n","  true_labels = copy.deepcopy(dataset[i][1])\n","  #\n","  iter = 0\n","  correct = 0\n","  start = 0   #342*342*160            #0\n","  end = total #342*342*160 + 116964*5 #total\n","  raw = dataset[i][0]\n","  \n","  print(\"To display one slice of the raw and label image first:\")\n","  # To show the raw image, slice 160\n","  plt.imshow(dataset[i][0][160])\n","  plt.show()\n","  # To show the label image, slice 160\n","  plt.imshow(dataset[i][1][160],cmap='nipy_spectral',interpolation='nearest')\n","  plt.show()\n","  \n","  start_time = time.time()\n","  # print(\"Starting recording test time for the patient\")\n","  for j in range (start,end):\n","    stime = time.time()\n","    voxel = j\n","    # PREDICTION for every FIVE layers:\n","    # The IF statement to predict is carried out when\n","    # ( end of 5 layers OR coming to the last voxel )\n","    # \n","    # Only predict if the voxel is non-zero within those 5 layers\n","    # \n","    #if(((j+1)%(342*342*5)==0 or (j+1) == end) and len(t1)>0):\n","    if(len(t1)==pred_samples_pool or (j+1)==end):\n","      #\n","      # TO PREDICT/ CARRY OUT SEGMENTATION\n","      #pred = test.predict([t1,t4,t7])\n","      pred = resnet.predict([t7], batch_size=pred_batch_size)\n","      #pred = resnet.predict([t1, t2, t3], batch_size=pred_batch_size)\n","      \n","      \n","      # After prediction:\n","      # assign the prediction result to image, with the x,y,z locations\n","      #\n","      pred_labelimage[i][t9,t10,t11] = np.argmax(pred,axis=1)\n","      \n","      # compare between the pred result and the groundtruth (t8); #correct\n","      c = np.sum(np.equal(np.argmax(pred,axis=1),t8))\n","      correct += c\n","      \n","      print(\"Iteration \" + str(j) + \" Prediction: \" + str(c) + \"/\" + str(len(t1)) + \" samples correct (\" + str(float(c)/len(t1)) + \")\")\n","      \n","      # total number of voxels predicted is iter\n","      iter += len(np.equal(np.argmax(pred,axis=1),t8))\n","      \n","      # Clean up the array for the next round\n","      t1=[]\n","      t2=[]\n","      t3=[]\n","      t4=[]\n","      t5=[]\n","      t6=[]\n","      t7=[]\n","      t8=[]\n","      t9=[]\n","      t10=[]\n","      t11=[] \n","      gc.collect()\n","\n","    # \n","    # TO APPEND data array for the next round of prediction\n","    # find the voxel x,y,z values\n","    z_layer = (voxel//116964)\n","    row = (voxel%116964)//342\n","    col = (voxel%116964)%342\n","    value = dataset[i][1][z_layer][row][col]\n","    # Here, we ignore and not append to data array if value = 0 (background voxel)\n","    if(value==0):\n","      continue\n","    else:\n","      # If the value is non-zero. we will extract the slice raw values for testing\n","      small_x = raw[z_layer][row-8:row+8,col-8:col+8]\n","      small_y = raw[z_layer-8:z_layer+8, row, col-8:col+8]\n","      small_z = raw[z_layer-8:z_layer+8, row-8:row+8, col]\n","      \n","      large_x = raw[z_layer, row-43:row+44, col-43:col+44]\n","      large_y = raw[z_layer-43:z_layer+44, row, col-43:col+44]\n","      large_z = raw[z_layer-43:z_layer+44, row-43:row+44, col]\n","      \n","      vol = raw[z_layer-13:z_layer+13,row-13:row+13,col-13:col+13]\n","\n","      small_x = np.reshape(small_x, (16,16,1))\n","      small_y = np.reshape(small_y, (16,16,1))\n","      small_z = np.reshape(small_z, (16,16,1))\n","          \n","      large_x = np.reshape(large_x, (87,87,1))\n","      large_y = np.reshape(large_x, (87,87,1))\n","      large_z = np.reshape(large_x, (87,87,1))\n","      \n","      vol = np.reshape(vol, (26,26,26,1))\n","\n","      t1.append(small_x)\n","      t2.append(small_y)\n","      t3.append(small_z)\n","        \n","      t4.append(large_x)\n","      t5.append(large_y)\n","      t6.append(large_z)\n","      \n","      t7.append(vol)\n","      \n","      t8.append(value)\n","      t9.append(z_layer)\n","      t10.append(row)\n","      t11.append(col)\n","    \n","  # When coming to here, the entire set of voxels has been completed\n","  print(\"Accuracy is: \",(correct/iter)*100,\" ; correct and iter are \",correct, iter ) \n","  print(\"Time Taken for testing Patient: \", time.time()-start_time)\n","  \n","  # FINISHED testing and showing the accuracy of one patient \n","  #\n","  # To show the predicated result of one slice (slice 160) from the CNN\n","  print(\"To display one slice of the predicted result and difference image :\")\n","  plt.imshow(pred_labelimage[i][160],cmap='nipy_spectral',interpolation='nearest')\n","  plt.show()\n","  \n","  difference_mask = pred_labelimage[i][160] != true_labels[160]\n","  difference = difference_mask*true_labels[160]\n","  difference[difference>0] = 255 # make this area dark\n","   # To show the difference image\n","  plt.imshow(difference)\n","  plt.show()\n","  \n","  # Saving the result of just one slice to the folder\n","  np.save(\"drive/My Drive/MRI_Brain_Segmentation/Test_output_\" + str(i) + \"_\"+str(path[len(path)-5:-4])+ \".npy\",pred_labelimage[i][160])\n","  np.save(\"drive/My Drive/MRI_Brain_Segmentation/Test_raw_\" + str(i) + \"_\"+str(path[len(path)-5:-4])+ \".npy\",dataset[i][0][160][160])\n","  np.save(\"drive/My Drive/MRI_Brain_Segmentation/Test_label_\" + str(i) + \"_\"+str(path[len(path)-5:-4])+ \".npy\",dataset[i][1][160])\n","  np.save(\"drive/My Drive/MRI_Brain_Segmentation/Test_diff_\" + str(i) + \"_\"+str(path[len(path)-5:-4])+ \".npy\",difference)\n","  \n","  # Saving the result patient i to the folder\n","  np.save(\"drive/My Drive/MRI_Brain_Segmentation/Patient_output_\" + str(i) + \"_\"+str(path[len(path)-5:-4])+ \".npy\",pred_labelimage[i])\n","  # np.save(\"output_\" + str(i) + \"_\"+str(path[len(path)-5:-4])+\".npy\",image[i])\n","print(\"All the patients are completed !\")\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["---------------------Start the Prediction: ------------------------\n","Test Patient Number 0\n","total number of voxels in the Patient =  47019528\n","To display one slice of the raw and label image first:\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMf8PvunvYonljN4AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Iteration 12335394 Prediction: 1991/2000 samples correct (0.9955)\n","Iteration 12569020 Prediction: 1928/2000 samples correct (0.964)\n","Iteration 12799830 Prediction: 1484/2000 samples correct (0.742)\n","Iteration 12918871 Prediction: 622/2000 samples correct (0.311)\n","Iteration 13036178 Prediction: 1084/2000 samples correct (0.542)\n","Iteration 13151395 Prediction: 1449/2000 samples correct (0.7245)\n","Iteration 13264277 Prediction: 1456/2000 samples correct (0.728)\n","Iteration 13274898 Prediction: 1426/2000 samples correct (0.713)\n","Iteration 13385342 Prediction: 1628/2000 samples correct (0.814)\n","Iteration 13493763 Prediction: 1588/2000 samples correct (0.794)\n","Iteration 13504366 Prediction: 1562/2000 samples correct (0.781)\n","Iteration 13612121 Prediction: 1571/2000 samples correct (0.7855)\n","Iteration 13621990 Prediction: 1569/2000 samples correct (0.7845)\n","Iteration 13728740 Prediction: 1588/2000 samples correct (0.794)\n","Iteration 13737982 Prediction: 1616/2000 samples correct (0.808)\n","Iteration 13751312 Prediction: 1611/2000 samples correct (0.8055)\n","Iteration 13852877 Prediction: 1692/2000 samples correct (0.846)\n","Iteration 13864464 Prediction: 1550/2000 samples correct (0.775)\n","Iteration 13967109 Prediction: 1844/2000 samples correct (0.922)\n","Iteration 13975625 Prediction: 1606/2000 samples correct (0.803)\n","Iteration 14079966 Prediction: 1782/2000 samples correct (0.891)\n","Iteration 14088519 Prediction: 1734/2000 samples correct (0.867)\n","Iteration 14099816 Prediction: 1684/2000 samples correct (0.842)\n","Iteration 14200302 Prediction: 1768/2000 samples correct (0.884)\n","Iteration 14208194 Prediction: 1747/2000 samples correct (0.8735)\n","Iteration 14220497 Prediction: 1750/2000 samples correct (0.875)\n","Iteration 14318650 Prediction: 1727/2000 samples correct (0.8635)\n","Iteration 14326844 Prediction: 1663/2000 samples correct (0.8315)\n","Iteration 14426389 Prediction: 1814/2000 samples correct (0.907)\n","Iteration 14435645 Prediction: 1646/2000 samples correct (0.823)\n","Iteration 14443517 Prediction: 1652/2000 samples correct (0.826)\n","Iteration 14454428 Prediction: 1801/2000 samples correct (0.9005)\n","Iteration 14551238 Prediction: 1637/2000 samples correct (0.8185)\n","Iteration 14558775 Prediction: 1743/2000 samples correct (0.8715)\n","Iteration 14568656 Prediction: 1620/2000 samples correct (0.81)\n","Iteration 14665823 Prediction: 1731/2000 samples correct (0.8655)\n","Iteration 14673349 Prediction: 1835/2000 samples correct (0.9175)\n","Iteration 14682211 Prediction: 1510/2000 samples correct (0.755)\n","Iteration 14779378 Prediction: 1747/2000 samples correct (0.8735)\n","Iteration 14787531 Prediction: 1770/2000 samples correct (0.885)\n","Iteration 14795114 Prediction: 1538/2000 samples correct (0.769)\n","Iteration 14804969 Prediction: 1832/2000 samples correct (0.916)\n","Iteration 14900723 Prediction: 1653/2000 samples correct (0.8265)\n","Iteration 14907926 Prediction: 1823/2000 samples correct (0.9115)\n","Iteration 14916504 Prediction: 1504/2000 samples correct (0.752)\n","Iteration 15012261 Prediction: 1729/2000 samples correct (0.8645)\n","Iteration 15020120 Prediction: 1691/2000 samples correct (0.8455)\n","Iteration 15027615 Prediction: 1649/2000 samples correct (0.8245)\n","Iteration 15036859 Prediction: 1685/2000 samples correct (0.8425)\n","Iteration 15131920 Prediction: 1525/2000 samples correct (0.7625)\n","Iteration 15139118 Prediction: 1778/2000 samples correct (0.889)\n","Iteration 15147648 Prediction: 1479/2000 samples correct (0.7395)\n","Iteration 15156590 Prediction: 1906/2000 samples correct (0.953)\n","Iteration 15250280 Prediction: 1468/2000 samples correct (0.734)\n","Iteration 15257159 Prediction: 1790/2000 samples correct (0.895)\n","Iteration 15265660 Prediction: 1526/2000 samples correct (0.763)\n","Iteration 15275275 Prediction: 1896/2000 samples correct (0.948)\n","Iteration 15367609 Prediction: 1361/2000 samples correct (0.6805)\n","Iteration 15374459 Prediction: 1802/2000 samples correct (0.901)\n","Iteration 15382641 Prediction: 1501/2000 samples correct (0.7505)\n","Iteration 15391846 Prediction: 1880/2000 samples correct (0.94)\n","Iteration 15483896 Prediction: 1304/2000 samples correct (0.652)\n","Iteration 15490729 Prediction: 1794/2000 samples correct (0.897)\n","Iteration 15498590 Prediction: 1480/2000 samples correct (0.74)\n","Iteration 15507098 Prediction: 1892/2000 samples correct (0.946)\n","Iteration 15599477 Prediction: 1406/2000 samples correct (0.703)\n","Iteration 15606311 Prediction: 1682/2000 samples correct (0.841)\n","Iteration 15613837 Prediction: 1556/2000 samples correct (0.778)\n","Iteration 15622333 Prediction: 1833/2000 samples correct (0.9165)\n","Iteration 15714685 Prediction: 1630/2000 samples correct (0.815)\n","Iteration 15721525 Prediction: 1450/2000 samples correct (0.725)\n","Iteration 15728364 Prediction: 1701/2000 samples correct (0.8505)\n","Iteration 15736252 Prediction: 1616/2000 samples correct (0.808)\n","Iteration 15827258 Prediction: 1915/2000 samples correct (0.9575)\n","Iteration 15835419 Prediction: 1329/2000 samples correct (0.6645)\n","Iteration 15841941 Prediction: 1651/2000 samples correct (0.8255)\n","Iteration 15849805 Prediction: 1461/2000 samples correct (0.7305)\n","Iteration 15857694 Prediction: 1887/2000 samples correct (0.9435)\n","Iteration 15948679 Prediction: 1686/2000 samples correct (0.843)\n","Iteration 15955444 Prediction: 1553/2000 samples correct (0.7765)\n","Iteration 15961998 Prediction: 1600/2000 samples correct (0.8)\n","Iteration 15969841 Prediction: 1588/2000 samples correct (0.794)\n","Iteration 16059468 Prediction: 1892/2000 samples correct (0.946)\n","Iteration 16067996 Prediction: 1607/2000 samples correct (0.8035)\n","Iteration 16074459 Prediction: 1528/2000 samples correct (0.764)\n","Iteration 16081312 Prediction: 1572/2000 samples correct (0.786)\n","Iteration 16088882 Prediction: 1781/2000 samples correct (0.8905)\n","Iteration 16179818 Prediction: 1842/2000 samples correct (0.921)\n","Iteration 16186642 Prediction: 1660/2000 samples correct (0.83)\n","Iteration 16192866 Prediction: 1579/2000 samples correct (0.7895)\n","Iteration 16200030 Prediction: 1493/2000 samples correct (0.7465)\n","Iteration 16207867 Prediction: 1862/2000 samples correct (0.931)\n","Iteration 16298207 Prediction: 1821/2000 samples correct (0.9105)\n","Iteration 16304675 Prediction: 1731/2000 samples correct (0.8655)\n","Iteration 16311129 Prediction: 1553/2000 samples correct (0.7765)\n","Iteration 16318039 Prediction: 1454/2000 samples correct (0.727)\n","Iteration 16326197 Prediction: 1896/2000 samples correct (0.948)\n","Iteration 16416111 Prediction: 1771/2000 samples correct (0.8855)\n","Iteration 16422313 Prediction: 1795/2000 samples correct (0.8975)\n","Iteration 16428764 Prediction: 1522/2000 samples correct (0.761)\n","Iteration 16435667 Prediction: 1549/2000 samples correct (0.7745)\n","Iteration 16443543 Prediction: 1934/2000 samples correct (0.967)\n","Iteration 16532826 Prediction: 1835/2000 samples correct (0.9175)\n","Iteration 16539252 Prediction: 1821/2000 samples correct (0.9105)\n","Iteration 16545457 Prediction: 1515/2000 samples correct (0.7575)\n","Iteration 16552591 Prediction: 1508/2000 samples correct (0.754)\n","Iteration 16560147 Prediction: 1921/2000 samples correct (0.9605)\n","Iteration 16649072 Prediction: 1870/2000 samples correct (0.935)\n","Iteration 16655513 Prediction: 1802/2000 samples correct (0.901)\n","Iteration 16661681 Prediction: 1408/2000 samples correct (0.704)\n","Iteration 16668249 Prediction: 1581/2000 samples correct (0.7905)\n","Iteration 16675745 Prediction: 1905/2000 samples correct (0.9525)\n","Iteration 16764281 Prediction: 1899/2000 samples correct (0.9495)\n","Iteration 16770793 Prediction: 1827/2000 samples correct (0.9135)\n","Iteration 16776922 Prediction: 1449/2000 samples correct (0.7245)\n","Iteration 16783134 Prediction: 1574/2000 samples correct (0.787)\n","Iteration 16790320 Prediction: 1763/2000 samples correct (0.8815)\n","Iteration 16877527 Prediction: 1930/2000 samples correct (0.965)\n","Iteration 16885374 Prediction: 1825/2000 samples correct (0.9125)\n","Iteration 16891517 Prediction: 1723/2000 samples correct (0.8615)\n","Iteration 16897674 Prediction: 1340/2000 samples correct (0.67)\n","Iteration 16904234 Prediction: 1605/2000 samples correct (0.8025)\n","Iteration 16912065 Prediction: 1894/2000 samples correct (0.947)\n","Iteration 16999246 Prediction: 1842/2000 samples correct (0.921)\n","Iteration 17005391 Prediction: 1878/2000 samples correct (0.939)\n","Iteration 17011280 Prediction: 1341/2000 samples correct (0.6705)\n","Iteration 17017699 Prediction: 1686/2000 samples correct (0.843)\n","Iteration 17024956 Prediction: 1851/2000 samples correct (0.9255)\n","Iteration 17111809 Prediction: 1928/2000 samples correct (0.964)\n","Iteration 17119009 Prediction: 1826/2000 samples correct (0.913)\n","Iteration 17125103 Prediction: 1828/2000 samples correct (0.914)\n","Iteration 17131221 Prediction: 1578/2000 samples correct (0.789)\n","Iteration 17137794 Prediction: 1682/2000 samples correct (0.841)\n","Iteration 17145610 Prediction: 1903/2000 samples correct (0.9515)\n","Iteration 17232467 Prediction: 1850/2000 samples correct (0.925)\n","Iteration 17238367 Prediction: 1865/2000 samples correct (0.9325)\n","Iteration 17244194 Prediction: 1711/2000 samples correct (0.8555)\n","Iteration 17250339 Prediction: 1725/2000 samples correct (0.8625)\n","Iteration 17257520 Prediction: 1771/2000 samples correct (0.8855)\n","Iteration 17342968 Prediction: 1950/2000 samples correct (0.975)\n","Iteration 17351581 Prediction: 1781/2000 samples correct (0.8905)\n","Iteration 17357626 Prediction: 1878/2000 samples correct (0.939)\n","Iteration 17363449 Prediction: 1631/2000 samples correct (0.8155)\n","Iteration 17369602 Prediction: 1693/2000 samples correct (0.8465)\n","Iteration 17377181 Prediction: 1884/2000 samples correct (0.942)\n","Iteration 17464023 Prediction: 1887/2000 samples correct (0.9435)\n","Iteration 17470526 Prediction: 1767/2000 samples correct (0.8835)\n","Iteration 17476303 Prediction: 1788/2000 samples correct (0.894)\n","Iteration 17482177 Prediction: 1693/2000 samples correct (0.8465)\n","Iteration 17489020 Prediction: 1734/2000 samples correct (0.867)\n","Iteration 17496872 Prediction: 1905/2000 samples correct (0.9525)\n","Iteration 17583398 Prediction: 1802/2000 samples correct (0.901)\n","Iteration 17589244 Prediction: 1812/2000 samples correct (0.906)\n","Iteration 17594985 Prediction: 1685/2000 samples correct (0.8425)\n","Iteration 17600842 Prediction: 1702/2000 samples correct (0.851)\n","Iteration 17607691 Prediction: 1773/2000 samples correct (0.8865)\n","Iteration 17616606 Prediction: 1912/2000 samples correct (0.956)\n","Iteration 17701725 Prediction: 1736/2000 samples correct (0.868)\n","Iteration 17707508 Prediction: 1847/2000 samples correct (0.9235)\n","Iteration 17713045 Prediction: 1527/2000 samples correct (0.7635)\n","Iteration 17719121 Prediction: 1742/2000 samples correct (0.871)\n","Iteration 17726320 Prediction: 1872/2000 samples correct (0.936)\n","Iteration 17812917 Prediction: 1830/2000 samples correct (0.915)\n","Iteration 17820072 Prediction: 1747/2000 samples correct (0.8735)\n","Iteration 17825835 Prediction: 1802/2000 samples correct (0.901)\n","Iteration 17831647 Prediction: 1522/2000 samples correct (0.761)\n","Iteration 17837521 Prediction: 1719/2000 samples correct (0.8595)\n","Iteration 17845072 Prediction: 1872/2000 samples correct (0.936)\n","Iteration 17932213 Prediction: 1880/2000 samples correct (0.94)\n","Iteration 17938383 Prediction: 1726/2000 samples correct (0.863)\n","Iteration 17943922 Prediction: 1757/2000 samples correct (0.8785)\n","Iteration 17949988 Prediction: 1628/2000 samples correct (0.814)\n","Iteration 17956167 Prediction: 1739/2000 samples correct (0.8695)\n","Iteration 17964358 Prediction: 1823/2000 samples correct (0.9115)\n","Iteration 18050623 Prediction: 1862/2000 samples correct (0.931)\n","Iteration 18056699 Prediction: 1751/2000 samples correct (0.8755)\n","Iteration 18062492 Prediction: 1689/2000 samples correct (0.8445)\n","Iteration 18068642 Prediction: 1612/2000 samples correct (0.806)\n","Iteration 18075226 Prediction: 1808/2000 samples correct (0.904)\n","Iteration 18160695 Prediction: 1896/2000 samples correct (0.948)\n","Iteration 18169238 Prediction: 1757/2000 samples correct (0.8785)\n","Iteration 18175060 Prediction: 1771/2000 samples correct (0.8855)\n","Iteration 18180884 Prediction: 1483/2000 samples correct (0.7415)\n","Iteration 18187037 Prediction: 1688/2000 samples correct (0.844)\n","Iteration 18194563 Prediction: 1811/2000 samples correct (0.9055)\n","Iteration 18280723 Prediction: 1894/2000 samples correct (0.947)\n","Iteration 18287588 Prediction: 1703/2000 samples correct (0.8515)\n","Iteration 18293381 Prediction: 1754/2000 samples correct (0.877)\n","Iteration 18299163 Prediction: 1388/2000 samples correct (0.694)\n","Iteration 18305354 Prediction: 1730/2000 samples correct (0.865)\n","Iteration 18313548 Prediction: 1852/2000 samples correct (0.926)\n","Iteration 18399754 Prediction: 1801/2000 samples correct (0.9005)\n","Iteration 18406199 Prediction: 1729/2000 samples correct (0.8645)\n","Iteration 18411724 Prediction: 1738/2000 samples correct (0.869)\n","Iteration 18417557 Prediction: 1540/2000 samples correct (0.77)\n","Iteration 18424028 Prediction: 1725/2000 samples correct (0.8625)\n","Iteration 18433292 Prediction: 1870/2000 samples correct (0.935)\n","Iteration 18518726 Prediction: 1718/2000 samples correct (0.859)\n","Iteration 18524528 Prediction: 1786/2000 samples correct (0.893)\n","Iteration 18530047 Prediction: 1736/2000 samples correct (0.868)\n","Iteration 18536162 Prediction: 1696/2000 samples correct (0.848)\n","Iteration 18543038 Prediction: 1763/2000 samples correct (0.8815)\n","Iteration 18629944 Prediction: 1894/2000 samples correct (0.947)\n","Iteration 18637413 Prediction: 1670/2000 samples correct (0.835)\n","Iteration 18642934 Prediction: 1813/2000 samples correct (0.9065)\n","Iteration 18648428 Prediction: 1794/2000 samples correct (0.897)\n","Iteration 18654889 Prediction: 1817/2000 samples correct (0.9085)\n","Iteration 18663128 Prediction: 1809/2000 samples correct (0.9045)\n","Iteration 18750323 Prediction: 1738/2000 samples correct (0.869)\n","Iteration 18756440 Prediction: 1776/2000 samples correct (0.888)\n","Iteration 18761927 Prediction: 1788/2000 samples correct (0.894)\n","Iteration 18767685 Prediction: 1774/2000 samples correct (0.887)\n","Iteration 18774196 Prediction: 1761/2000 samples correct (0.8805)\n","Iteration 18785879 Prediction: 1921/2000 samples correct (0.9605)\n","Iteration 18870329 Prediction: 1706/2000 samples correct (0.853)\n","Iteration 18875832 Prediction: 1662/2000 samples correct (0.831)\n","Iteration 18881318 Prediction: 1783/2000 samples correct (0.8915)\n","Iteration 18887482 Prediction: 1796/2000 samples correct (0.898)\n","Iteration 18895318 Prediction: 1792/2000 samples correct (0.896)\n","Iteration 18984597 Prediction: 1768/2000 samples correct (0.884)\n","Iteration 18990435 Prediction: 1629/2000 samples correct (0.8145)\n","Iteration 18995905 Prediction: 1825/2000 samples correct (0.9125)\n","Iteration 19001694 Prediction: 1804/2000 samples correct (0.902)\n","Iteration 19008510 Prediction: 1749/2000 samples correct (0.8745)\n","Iteration 19099461 Prediction: 1838/2000 samples correct (0.919)\n","Iteration 19105974 Prediction: 1715/2000 samples correct (0.8575)\n","Iteration 19111432 Prediction: 1736/2000 samples correct (0.868)\n","Iteration 19116915 Prediction: 1802/2000 samples correct (0.901)\n","Iteration 19123127 Prediction: 1745/2000 samples correct (0.8725)\n","Iteration 19210309 Prediction: 1852/2000 samples correct (0.926)\n","Iteration 19221213 Prediction: 1680/2000 samples correct (0.84)\n","Iteration 19226695 Prediction: 1756/2000 samples correct (0.878)\n","Iteration 19232144 Prediction: 1853/2000 samples correct (0.9265)\n","Iteration 19237981 Prediction: 1732/2000 samples correct (0.866)\n","Iteration 19245880 Prediction: 1761/2000 samples correct (0.8805)\n","Iteration 19336518 Prediction: 1767/2000 samples correct (0.8835)\n","Iteration 19342276 Prediction: 1650/2000 samples correct (0.825)\n","Iteration 19347725 Prediction: 1787/2000 samples correct (0.8935)\n","Iteration 19353306 Prediction: 1699/2000 samples correct (0.8495)\n","Iteration 19359805 Prediction: 1723/2000 samples correct (0.8615)\n","Iteration 19451710 Prediction: 1826/2000 samples correct (0.913)\n","Iteration 19457869 Prediction: 1652/2000 samples correct (0.826)\n","Iteration 19463087 Prediction: 1598/2000 samples correct (0.799)\n","Iteration 19468872 Prediction: 1752/2000 samples correct (0.876)\n","Iteration 19475019 Prediction: 1665/2000 samples correct (0.8325)\n","Iteration 19566979 Prediction: 1851/2000 samples correct (0.9255)\n","Iteration 19573821 Prediction: 1701/2000 samples correct (0.8505)\n","Iteration 19579276 Prediction: 1561/2000 samples correct (0.7805)\n","Iteration 19584501 Prediction: 1819/2000 samples correct (0.9095)\n","Iteration 19590596 Prediction: 1640/2000 samples correct (0.82)\n","Iteration 19682603 Prediction: 1762/2000 samples correct (0.881)\n","Iteration 19690068 Prediction: 1658/2000 samples correct (0.829)\n","Iteration 19695542 Prediction: 1619/2000 samples correct (0.8095)\n","Iteration 19700750 Prediction: 1790/2000 samples correct (0.895)\n","Iteration 19706560 Prediction: 1799/2000 samples correct (0.8995)\n","Iteration 19798171 Prediction: 1755/2000 samples correct (0.8775)\n","Iteration 19806047 Prediction: 1638/2000 samples correct (0.819)\n","Iteration 19811544 Prediction: 1608/2000 samples correct (0.804)\n","Iteration 19816966 Prediction: 1739/2000 samples correct (0.8695)\n","Iteration 19822756 Prediction: 1819/2000 samples correct (0.9095)\n","Iteration 19831719 Prediction: 1774/2000 samples correct (0.887)\n","Iteration 19921699 Prediction: 1779/2000 samples correct (0.8895)\n","Iteration 19927463 Prediction: 1632/2000 samples correct (0.816)\n","Iteration 19932894 Prediction: 1598/2000 samples correct (0.799)\n","Iteration 19938445 Prediction: 1827/2000 samples correct (0.9135)\n","Iteration 19944918 Prediction: 1782/2000 samples correct (0.891)\n","Iteration 20037296 Prediction: 1801/2000 samples correct (0.9005)\n","Iteration 20043349 Prediction: 1657/2000 samples correct (0.8285)\n","Iteration 20048575 Prediction: 1625/2000 samples correct (0.8125)\n","Iteration 20054069 Prediction: 1817/2000 samples correct (0.9085)\n","Iteration 20060154 Prediction: 1819/2000 samples correct (0.9095)\n","Iteration 20152844 Prediction: 1804/2000 samples correct (0.902)\n","Iteration 20158991 Prediction: 1813/2000 samples correct (0.9065)\n","Iteration 20164457 Prediction: 1651/2000 samples correct (0.8255)\n","Iteration 20169906 Prediction: 1730/2000 samples correct (0.865)\n","Iteration 20175445 Prediction: 1869/2000 samples correct (0.9345)\n","Iteration 20267436 Prediction: 1888/2000 samples correct (0.944)\n","Iteration 20274274 Prediction: 1826/2000 samples correct (0.913)\n","Iteration 20279767 Prediction: 1742/2000 samples correct (0.871)\n","Iteration 20285234 Prediction: 1735/2000 samples correct (0.8675)\n","Iteration 20290718 Prediction: 1851/2000 samples correct (0.9255)\n","Iteration 20298199 Prediction: 1873/2000 samples correct (0.9365)\n","Iteration 20389188 Prediction: 1842/2000 samples correct (0.921)\n","Iteration 20394947 Prediction: 1772/2000 samples correct (0.886)\n","Iteration 20400428 Prediction: 1781/2000 samples correct (0.8905)\n","Iteration 20405642 Prediction: 1799/2000 samples correct (0.8995)\n","Iteration 20411424 Prediction: 1838/2000 samples correct (0.919)\n","Iteration 20503759 Prediction: 1784/2000 samples correct (0.892)\n","Iteration 20509864 Prediction: 1765/2000 samples correct (0.8825)\n","Iteration 20515362 Prediction: 1802/2000 samples correct (0.901)\n","Iteration 20520816 Prediction: 1807/2000 samples correct (0.9035)\n","Iteration 20526323 Prediction: 1830/2000 samples correct (0.915)\n","Iteration 20617927 Prediction: 1835/2000 samples correct (0.9175)\n","Iteration 20624841 Prediction: 1822/2000 samples correct (0.911)\n","Iteration 20630573 Prediction: 1812/2000 samples correct (0.906)\n","Iteration 20636048 Prediction: 1763/2000 samples correct (0.8815)\n","Iteration 20641515 Prediction: 1769/2000 samples correct (0.8845)\n","Iteration 20647431 Prediction: 1838/2000 samples correct (0.919)\n","Iteration 20739681 Prediction: 1847/2000 samples correct (0.9235)\n","Iteration 20745475 Prediction: 1719/2000 samples correct (0.8595)\n","Iteration 20750987 Prediction: 1659/2000 samples correct (0.8295)\n","Iteration 20756424 Prediction: 1785/2000 samples correct (0.8925)\n","Iteration 20761967 Prediction: 1789/2000 samples correct (0.8945)\n","Iteration 20853309 Prediction: 1860/2000 samples correct (0.93)\n","Iteration 20859819 Prediction: 1745/2000 samples correct (0.8725)\n","Iteration 20865609 Prediction: 1611/2000 samples correct (0.8055)\n","Iteration 20871070 Prediction: 1776/2000 samples correct (0.888)\n","Iteration 20876557 Prediction: 1846/2000 samples correct (0.923)\n","Iteration 20965109 Prediction: 1829/2000 samples correct (0.9145)\n","Iteration 20974651 Prediction: 1888/2000 samples correct (0.944)\n","Iteration 20980485 Prediction: 1558/2000 samples correct (0.779)\n","Iteration 20985997 Prediction: 1705/2000 samples correct (0.8525)\n","Iteration 20991457 Prediction: 1772/2000 samples correct (0.886)\n","Iteration 20997296 Prediction: 1812/2000 samples correct (0.906)\n","Iteration 21088555 Prediction: 1877/2000 samples correct (0.9385)\n","Iteration 21095054 Prediction: 1803/2000 samples correct (0.9015)\n","Iteration 21100562 Prediction: 1746/2000 samples correct (0.873)\n","Iteration 21106039 Prediction: 1815/2000 samples correct (0.9075)\n","Iteration 21111786 Prediction: 1849/2000 samples correct (0.9245)\n","Iteration 21202116 Prediction: 1771/2000 samples correct (0.8855)\n","Iteration 21209585 Prediction: 1768/2000 samples correct (0.884)\n","Iteration 21215159 Prediction: 1785/2000 samples correct (0.8925)\n","Iteration 21220899 Prediction: 1673/2000 samples correct (0.8365)\n","Iteration 21226367 Prediction: 1792/2000 samples correct (0.896)\n","Iteration 21232239 Prediction: 1748/2000 samples correct (0.874)\n","Iteration 21323215 Prediction: 1840/2000 samples correct (0.92)\n","Iteration 21329680 Prediction: 1767/2000 samples correct (0.8835)\n","Iteration 21335424 Prediction: 1574/2000 samples correct (0.787)\n","Iteration 21340895 Prediction: 1811/2000 samples correct (0.9055)\n","Iteration 21346420 Prediction: 1838/2000 samples correct (0.919)\n","Iteration 21437050 Prediction: 1748/2000 samples correct (0.874)\n","Iteration 21444211 Prediction: 1902/2000 samples correct (0.951)\n","Iteration 21449739 Prediction: 1745/2000 samples correct (0.8725)\n","Iteration 21455516 Prediction: 1612/2000 samples correct (0.806)\n","Iteration 21461005 Prediction: 1752/2000 samples correct (0.876)\n","Iteration 21467465 Prediction: 1849/2000 samples correct (0.9245)\n","Iteration 21558794 Prediction: 1875/2000 samples correct (0.9375)\n","Iteration 21564670 Prediction: 1817/2000 samples correct (0.9085)\n","Iteration 21570424 Prediction: 1407/2000 samples correct (0.7035)\n","Iteration 21575912 Prediction: 1762/2000 samples correct (0.881)\n","Iteration 21581691 Prediction: 1833/2000 samples correct (0.9165)\n","Iteration 21672693 Prediction: 1782/2000 samples correct (0.891)\n","Iteration 21679201 Prediction: 1887/2000 samples correct (0.9435)\n","Iteration 21684949 Prediction: 1647/2000 samples correct (0.8235)\n","Iteration 21690479 Prediction: 1542/2000 samples correct (0.771)\n","Iteration 21695983 Prediction: 1755/2000 samples correct (0.8775)\n","Iteration 21784537 Prediction: 1800/2000 samples correct (0.9)\n","Iteration 21793432 Prediction: 1823/2000 samples correct (0.9115)\n","Iteration 21799268 Prediction: 1781/2000 samples correct (0.8905)\n","Iteration 21804998 Prediction: 1390/2000 samples correct (0.695)\n","Iteration 21810487 Prediction: 1740/2000 samples correct (0.87)\n","Iteration 21816284 Prediction: 1794/2000 samples correct (0.897)\n","Iteration 21907293 Prediction: 1703/2000 samples correct (0.8515)\n","Iteration 21913495 Prediction: 1871/2000 samples correct (0.9355)\n","Iteration 21919214 Prediction: 1831/2000 samples correct (0.9155)\n","Iteration 21924731 Prediction: 1677/2000 samples correct (0.8385)\n","Iteration 21930249 Prediction: 1762/2000 samples correct (0.881)\n","Iteration 22019150 Prediction: 1791/2000 samples correct (0.8955)\n","Iteration 22027732 Prediction: 1615/2000 samples correct (0.8075)\n","Iteration 22033789 Prediction: 1906/2000 samples correct (0.953)\n","Iteration 22039311 Prediction: 1715/2000 samples correct (0.8575)\n","Iteration 22044810 Prediction: 1726/2000 samples correct (0.863)\n","Iteration 22051584 Prediction: 1822/2000 samples correct (0.911)\n","Iteration 22142258 Prediction: 1663/2000 samples correct (0.8315)\n","Iteration 22148437 Prediction: 1778/2000 samples correct (0.889)\n","Iteration 22154176 Prediction: 1862/2000 samples correct (0.931)\n","Iteration 22159694 Prediction: 1709/2000 samples correct (0.8545)\n","Iteration 22165551 Prediction: 1758/2000 samples correct (0.879)\n","Iteration 22255793 Prediction: 1646/2000 samples correct (0.823)\n","Iteration 22262964 Prediction: 1712/2000 samples correct (0.856)\n","Iteration 22268776 Prediction: 1898/2000 samples correct (0.949)\n","Iteration 22274300 Prediction: 1715/2000 samples correct (0.8575)\n","Iteration 22280044 Prediction: 1706/2000 samples correct (0.853)\n","Iteration 22287634 Prediction: 1726/2000 samples correct (0.863)\n","Iteration 22377255 Prediction: 1559/2000 samples correct (0.7795)\n","Iteration 22383390 Prediction: 1838/2000 samples correct (0.919)\n","Iteration 22388889 Prediction: 1835/2000 samples correct (0.9175)\n","Iteration 22394642 Prediction: 1638/2000 samples correct (0.819)\n","Iteration 22400839 Prediction: 1719/2000 samples correct (0.8595)\n","Iteration 22491114 Prediction: 1645/2000 samples correct (0.8225)\n","Iteration 22497958 Prediction: 1756/2000 samples correct (0.878)\n","Iteration 22503708 Prediction: 1861/2000 samples correct (0.9305)\n","Iteration 22509244 Prediction: 1648/2000 samples correct (0.824)\n","Iteration 22515072 Prediction: 1674/2000 samples correct (0.837)\n","Iteration 22523627 Prediction: 1675/2000 samples correct (0.8375)\n","Iteration 22612816 Prediction: 1651/2000 samples correct (0.8255)\n","Iteration 22618631 Prediction: 1875/2000 samples correct (0.9375)\n","Iteration 22624172 Prediction: 1812/2000 samples correct (0.906)\n","Iteration 22629968 Prediction: 1626/2000 samples correct (0.813)\n","Iteration 22637087 Prediction: 1715/2000 samples correct (0.8575)\n","Iteration 22727730 Prediction: 1601/2000 samples correct (0.8005)\n","Iteration 22733949 Prediction: 1873/2000 samples correct (0.9365)\n","Iteration 22739692 Prediction: 1839/2000 samples correct (0.9195)\n","Iteration 22745253 Prediction: 1672/2000 samples correct (0.836)\n","Iteration 22752006 Prediction: 1686/2000 samples correct (0.843)\n","Iteration 22842997 Prediction: 1649/2000 samples correct (0.8245)\n","Iteration 22849826 Prediction: 1782/2000 samples correct (0.891)\n","Iteration 22855357 Prediction: 1892/2000 samples correct (0.946)\n","Iteration 22861432 Prediction: 1674/2000 samples correct (0.837)\n","Iteration 22867632 Prediction: 1674/2000 samples correct (0.837)\n","Iteration 22958264 Prediction: 1659/2000 samples correct (0.8295)\n","Iteration 22965773 Prediction: 1724/2000 samples correct (0.862)\n","Iteration 22971556 Prediction: 1893/2000 samples correct (0.9465)\n","Iteration 22977126 Prediction: 1645/2000 samples correct (0.8225)\n","Iteration 22983248 Prediction: 1727/2000 samples correct (0.8635)\n","Iteration 23072821 Prediction: 1576/2000 samples correct (0.788)\n","Iteration 23081430 Prediction: 1809/2000 samples correct (0.9045)\n","Iteration 23087512 Prediction: 1914/2000 samples correct (0.957)\n","Iteration 23093351 Prediction: 1728/2000 samples correct (0.864)\n","Iteration 23099229 Prediction: 1659/2000 samples correct (0.8295)\n","Iteration 23108727 Prediction: 1457/2000 samples correct (0.7285)\n","Iteration 23197702 Prediction: 1771/2000 samples correct (0.8855)\n","Iteration 23203824 Prediction: 1911/2000 samples correct (0.9555)\n","Iteration 23209630 Prediction: 1769/2000 samples correct (0.8845)\n","Iteration 23215509 Prediction: 1714/2000 samples correct (0.857)\n","Iteration 23223646 Prediction: 1620/2000 samples correct (0.81)\n","Iteration 23313926 Prediction: 1739/2000 samples correct (0.8695)\n","Iteration 23320081 Prediction: 1865/2000 samples correct (0.9325)\n","Iteration 23325874 Prediction: 1776/2000 samples correct (0.888)\n","Iteration 23331758 Prediction: 1671/2000 samples correct (0.8355)\n","Iteration 23339292 Prediction: 1697/2000 samples correct (0.8485)\n","Iteration 23430188 Prediction: 1688/2000 samples correct (0.844)\n","Iteration 23436371 Prediction: 1848/2000 samples correct (0.924)\n","Iteration 23442166 Prediction: 1798/2000 samples correct (0.899)\n","Iteration 23448308 Prediction: 1636/2000 samples correct (0.818)\n","Iteration 23455593 Prediction: 1716/2000 samples correct (0.858)\n","Iteration 23546827 Prediction: 1706/2000 samples correct (0.853)\n","Iteration 23553016 Prediction: 1839/2000 samples correct (0.9195)\n","Iteration 23558817 Prediction: 1819/2000 samples correct (0.9095)\n","Iteration 23565000 Prediction: 1603/2000 samples correct (0.8015)\n","Iteration 23572811 Prediction: 1576/2000 samples correct (0.788)\n","Iteration 23664146 Prediction: 1714/2000 samples correct (0.857)\n","Iteration 23670332 Prediction: 1791/2000 samples correct (0.8955)\n","Iteration 23676138 Prediction: 1775/2000 samples correct (0.8875)\n","Iteration 23682293 Prediction: 1680/2000 samples correct (0.84)\n","Iteration 23690868 Prediction: 1662/2000 samples correct (0.831)\n","Iteration 23781852 Prediction: 1743/2000 samples correct (0.8715)\n","Iteration 23788271 Prediction: 1802/2000 samples correct (0.901)\n","Iteration 23794103 Prediction: 1758/2000 samples correct (0.879)\n","Iteration 23800564 Prediction: 1676/2000 samples correct (0.838)\n","Iteration 23809831 Prediction: 1589/2000 samples correct (0.7945)\n","Iteration 23900453 Prediction: 1762/2000 samples correct (0.881)\n","Iteration 23906346 Prediction: 1895/2000 samples correct (0.9475)\n","Iteration 23912453 Prediction: 1732/2000 samples correct (0.866)\n","Iteration 23919945 Prediction: 1718/2000 samples correct (0.859)\n","Iteration 24011659 Prediction: 1643/2000 samples correct (0.8215)\n","Iteration 24019514 Prediction: 1790/2000 samples correct (0.895)\n","Iteration 24025626 Prediction: 1892/2000 samples correct (0.946)\n","Iteration 24031516 Prediction: 1715/2000 samples correct (0.8575)\n","Iteration 24039729 Prediction: 1770/2000 samples correct (0.885)\n","Iteration 24131708 Prediction: 1594/2000 samples correct (0.797)\n","Iteration 24138509 Prediction: 1845/2000 samples correct (0.9225)\n","Iteration 24144634 Prediction: 1768/2000 samples correct (0.884)\n","Iteration 24151123 Prediction: 1709/2000 samples correct (0.8545)\n","Iteration 24241795 Prediction: 1379/2000 samples correct (0.6895)\n","Iteration 24251703 Prediction: 1797/2000 samples correct (0.8985)\n","Iteration 24257903 Prediction: 1888/2000 samples correct (0.944)\n","Iteration 24264047 Prediction: 1666/2000 samples correct (0.833)\n","Iteration 24271863 Prediction: 1550/2000 samples correct (0.775)\n","Iteration 24364901 Prediction: 1394/2000 samples correct (0.697)\n","Iteration 24371787 Prediction: 1788/2000 samples correct (0.894)\n","Iteration 24377903 Prediction: 1833/2000 samples correct (0.9165)\n","Iteration 24384394 Prediction: 1563/2000 samples correct (0.7815)\n","Iteration 24476760 Prediction: 1393/2000 samples correct (0.6965)\n","Iteration 24486007 Prediction: 1811/2000 samples correct (0.9055)\n","Iteration 24492431 Prediction: 1880/2000 samples correct (0.94)\n","Iteration 24498606 Prediction: 1786/2000 samples correct (0.893)\n","Iteration 24507175 Prediction: 1545/2000 samples correct (0.7725)\n","Iteration 24600559 Prediction: 1619/2000 samples correct (0.8095)\n","Iteration 24607364 Prediction: 1807/2000 samples correct (0.9035)\n","Iteration 24613243 Prediction: 1870/2000 samples correct (0.935)\n","Iteration 24620409 Prediction: 1483/2000 samples correct (0.7415)\n","Iteration 24714475 Prediction: 1718/2000 samples correct (0.859)\n","Iteration 24721980 Prediction: 1795/2000 samples correct (0.8975)\n","Iteration 24728133 Prediction: 1896/2000 samples correct (0.948)\n","Iteration 24734598 Prediction: 1697/2000 samples correct (0.8485)\n","Iteration 24744215 Prediction: 1536/2000 samples correct (0.768)\n","Iteration 24836506 Prediction: 1854/2000 samples correct (0.927)\n","Iteration 24842995 Prediction: 1852/2000 samples correct (0.926)\n","Iteration 24849201 Prediction: 1799/2000 samples correct (0.8995)\n","Iteration 24857038 Prediction: 1483/2000 samples correct (0.7415)\n","Iteration 24951415 Prediction: 1879/2000 samples correct (0.9395)\n","Iteration 24958299 Prediction: 1838/2000 samples correct (0.919)\n","Iteration 24964499 Prediction: 1824/2000 samples correct (0.912)\n","Iteration 24971949 Prediction: 1614/2000 samples correct (0.807)\n","Iteration 25066740 Prediction: 1871/2000 samples correct (0.9355)\n","Iteration 25074236 Prediction: 1835/2000 samples correct (0.9175)\n","Iteration 25080699 Prediction: 1859/2000 samples correct (0.9295)\n","Iteration 25087612 Prediction: 1694/2000 samples correct (0.847)\n","Iteration 25182672 Prediction: 1865/2000 samples correct (0.9325)\n","Iteration 25190808 Prediction: 1817/2000 samples correct (0.9085)\n","Iteration 25197014 Prediction: 1841/2000 samples correct (0.9205)\n","Iteration 25204228 Prediction: 1786/2000 samples correct (0.893)\n","Iteration 25299954 Prediction: 1865/2000 samples correct (0.9325)\n","Iteration 25307816 Prediction: 1782/2000 samples correct (0.891)\n","Iteration 25314019 Prediction: 1852/2000 samples correct (0.926)\n","Iteration 25321824 Prediction: 1712/2000 samples correct (0.856)\n","Iteration 25417626 Prediction: 1896/2000 samples correct (0.948)\n","Iteration 25425150 Prediction: 1796/2000 samples correct (0.898)\n","Iteration 25431609 Prediction: 1840/2000 samples correct (0.92)\n","Iteration 25439182 Prediction: 1687/2000 samples correct (0.8435)\n","Iteration 25535543 Prediction: 1852/2000 samples correct (0.926)\n","Iteration 25542770 Prediction: 1818/2000 samples correct (0.909)\n","Iteration 25549242 Prediction: 1811/2000 samples correct (0.9055)\n","Iteration 25557144 Prediction: 1699/2000 samples correct (0.8495)\n","Iteration 25653592 Prediction: 1855/2000 samples correct (0.9275)\n","Iteration 25660780 Prediction: 1814/2000 samples correct (0.907)\n","Iteration 25667292 Prediction: 1703/2000 samples correct (0.8515)\n","Iteration 25676170 Prediction: 1662/2000 samples correct (0.831)\n","Iteration 25772559 Prediction: 1877/2000 samples correct (0.9385)\n","Iteration 25779473 Prediction: 1874/2000 samples correct (0.937)\n","Iteration 25786631 Prediction: 1659/2000 samples correct (0.8295)\n","Iteration 25883749 Prediction: 1684/2000 samples correct (0.842)\n","Iteration 25892327 Prediction: 1712/2000 samples correct (0.856)\n","Iteration 25899114 Prediction: 1726/2000 samples correct (0.863)\n","Iteration 25907339 Prediction: 1579/2000 samples correct (0.7895)\n","Iteration 26005173 Prediction: 1857/2000 samples correct (0.9285)\n","Iteration 26012326 Prediction: 1676/2000 samples correct (0.838)\n","Iteration 26019157 Prediction: 1718/2000 samples correct (0.859)\n","Iteration 26116990 Prediction: 1643/2000 samples correct (0.8215)\n","Iteration 26125911 Prediction: 1527/2000 samples correct (0.7635)\n","Iteration 26132700 Prediction: 1704/2000 samples correct (0.852)\n","Iteration 26140966 Prediction: 1607/2000 samples correct (0.8035)\n","Iteration 26239798 Prediction: 1627/2000 samples correct (0.8135)\n","Iteration 26246916 Prediction: 1684/2000 samples correct (0.842)\n","Iteration 26253800 Prediction: 1643/2000 samples correct (0.8215)\n","Iteration 26353638 Prediction: 1661/2000 samples correct (0.8305)\n","Iteration 26361503 Prediction: 1574/2000 samples correct (0.787)\n","Iteration 26368039 Prediction: 1677/2000 samples correct (0.8385)\n","Iteration 26378270 Prediction: 1605/2000 samples correct (0.8025)\n","Iteration 26476048 Prediction: 1594/2000 samples correct (0.797)\n","Iteration 26482895 Prediction: 1752/2000 samples correct (0.876)\n","Iteration 26490471 Prediction: 1642/2000 samples correct (0.821)\n","Iteration 26590694 Prediction: 1645/2000 samples correct (0.8225)\n","Iteration 26597871 Prediction: 1697/2000 samples correct (0.8485)\n","Iteration 26604997 Prediction: 1759/2000 samples correct (0.8795)\n","Iteration 26705547 Prediction: 1603/2000 samples correct (0.8015)\n","Iteration 26713130 Prediction: 1634/2000 samples correct (0.817)\n","Iteration 26719946 Prediction: 1739/2000 samples correct (0.8695)\n","Iteration 26819443 Prediction: 1677/2000 samples correct (0.8385)\n","Iteration 26828642 Prediction: 1600/2000 samples correct (0.8)\n","Iteration 26835222 Prediction: 1708/2000 samples correct (0.854)\n","Iteration 26843413 Prediction: 1581/2000 samples correct (0.7905)\n","Iteration 26943901 Prediction: 1627/2000 samples correct (0.8135)\n","Iteration 26950762 Prediction: 1687/2000 samples correct (0.8435)\n","Iteration 26957964 Prediction: 1572/2000 samples correct (0.786)\n","Iteration 27059197 Prediction: 1623/2000 samples correct (0.8115)\n","Iteration 27066411 Prediction: 1701/2000 samples correct (0.8505)\n","Iteration 27073537 Prediction: 1555/2000 samples correct (0.7775)\n","Iteration 27174827 Prediction: 1697/2000 samples correct (0.8485)\n","Iteration 27182651 Prediction: 1674/2000 samples correct (0.837)\n","Iteration 27189540 Prediction: 1587/2000 samples correct (0.7935)\n","Iteration 27291389 Prediction: 1685/2000 samples correct (0.8425)\n","Iteration 27299572 Prediction: 1731/2000 samples correct (0.8655)\n","Iteration 27306447 Prediction: 1807/2000 samples correct (0.9035)\n","Iteration 27408394 Prediction: 1690/2000 samples correct (0.845)\n","Iteration 27416594 Prediction: 1778/2000 samples correct (0.889)\n","Iteration 27423472 Prediction: 1787/2000 samples correct (0.8935)\n","Iteration 27525688 Prediction: 1684/2000 samples correct (0.842)\n","Iteration 27533853 Prediction: 1805/2000 samples correct (0.9025)\n","Iteration 27540729 Prediction: 1778/2000 samples correct (0.889)\n","Iteration 27643321 Prediction: 1692/2000 samples correct (0.846)\n","Iteration 27651217 Prediction: 1774/2000 samples correct (0.887)\n","Iteration 27658363 Prediction: 1752/2000 samples correct (0.876)\n","Iteration 27761294 Prediction: 1659/2000 samples correct (0.8295)\n","Iteration 27768885 Prediction: 1798/2000 samples correct (0.899)\n","Iteration 27776054 Prediction: 1753/2000 samples correct (0.8765)\n","Iteration 27879287 Prediction: 1649/2000 samples correct (0.8245)\n","Iteration 27886843 Prediction: 1815/2000 samples correct (0.9075)\n","Iteration 27894319 Prediction: 1757/2000 samples correct (0.8785)\n","Iteration 27997605 Prediction: 1733/2000 samples correct (0.8665)\n","Iteration 28005150 Prediction: 1861/2000 samples correct (0.9305)\n","Iteration 28013712 Prediction: 1683/2000 samples correct (0.8415)\n","Iteration 28116329 Prediction: 1770/2000 samples correct (0.885)\n","Iteration 28123540 Prediction: 1818/2000 samples correct (0.909)\n","Iteration 28225773 Prediction: 1654/2000 samples correct (0.827)\n","Iteration 28235317 Prediction: 1798/2000 samples correct (0.899)\n","Iteration 28242508 Prediction: 1739/2000 samples correct (0.8695)\n","Iteration 28346456 Prediction: 1652/2000 samples correct (0.826)\n","Iteration 28354677 Prediction: 1768/2000 samples correct (0.884)\n","Iteration 28361883 Prediction: 1693/2000 samples correct (0.8465)\n","Iteration 28466560 Prediction: 1776/2000 samples correct (0.888)\n","Iteration 28474091 Prediction: 1805/2000 samples correct (0.9025)\n","Iteration 28576982 Prediction: 1630/2000 samples correct (0.815)\n","Iteration 28586907 Prediction: 1802/2000 samples correct (0.901)\n","Iteration 28594129 Prediction: 1712/2000 samples correct (0.856)\n","Iteration 28699419 Prediction: 1716/2000 samples correct (0.858)\n","Iteration 28707291 Prediction: 1769/2000 samples correct (0.8845)\n","Iteration 28809542 Prediction: 1617/2000 samples correct (0.8085)\n","Iteration 28820497 Prediction: 1755/2000 samples correct (0.8775)\n","Iteration 28828029 Prediction: 1615/2000 samples correct (0.8075)\n","Iteration 28934028 Prediction: 1733/2000 samples correct (0.8665)\n","Iteration 28941925 Prediction: 1674/2000 samples correct (0.837)\n","Iteration 29046553 Prediction: 1598/2000 samples correct (0.799)\n","Iteration 29055828 Prediction: 1796/2000 samples correct (0.898)\n","Iteration 29063688 Prediction: 1499/2000 samples correct (0.7495)\n","Iteration 29170351 Prediction: 1850/2000 samples correct (0.925)\n","Iteration 29178260 Prediction: 1571/2000 samples correct (0.7855)\n","Iteration 29285283 Prediction: 1751/2000 samples correct (0.8755)\n","Iteration 29293783 Prediction: 1621/2000 samples correct (0.8105)\n","Iteration 29400554 Prediction: 1703/2000 samples correct (0.8515)\n","Iteration 29409453 Prediction: 1721/2000 samples correct (0.8605)\n","Iteration 29516787 Prediction: 1683/2000 samples correct (0.8415)\n","Iteration 29526062 Prediction: 1813/2000 samples correct (0.9065)\n","Iteration 29633107 Prediction: 1742/2000 samples correct (0.871)\n","Iteration 29643018 Prediction: 1744/2000 samples correct (0.872)\n","Iteration 29750727 Prediction: 1697/2000 samples correct (0.8485)\n","Iteration 29760612 Prediction: 1724/2000 samples correct (0.862)\n","Iteration 29869380 Prediction: 1760/2000 samples correct (0.88)\n","Iteration 29878611 Prediction: 1732/2000 samples correct (0.866)\n","Iteration 29988053 Prediction: 1781/2000 samples correct (0.8905)\n","Iteration 29996980 Prediction: 1715/2000 samples correct (0.8575)\n","Iteration 30107102 Prediction: 1785/2000 samples correct (0.8925)\n","Iteration 30116323 Prediction: 1668/2000 samples correct (0.834)\n","Iteration 30226776 Prediction: 1781/2000 samples correct (0.8905)\n","Iteration 30336602 Prediction: 1634/2000 samples correct (0.817)\n","Iteration 30347144 Prediction: 1707/2000 samples correct (0.8535)\n","Iteration 30458678 Prediction: 1640/2000 samples correct (0.82)\n","Iteration 30470303 Prediction: 1569/2000 samples correct (0.7845)\n","Iteration 30580800 Prediction: 1589/2000 samples correct (0.7945)\n","Iteration 30694332 Prediction: 1447/2000 samples correct (0.7235)\n","Iteration 30807838 Prediction: 1319/2000 samples correct (0.6595)\n","Iteration 30819486 Prediction: 1145/2000 samples correct (0.5725)\n","Iteration 30933730 Prediction: 1327/2000 samples correct (0.6635)\n","Iteration 31049638 Prediction: 1413/2000 samples correct (0.7065)\n","Iteration 31166969 Prediction: 1283/2000 samples correct (0.6415)\n","Iteration 31392323 Prediction: 1861/2000 samples correct (0.9305)\n","Iteration 31517864 Prediction: 1948/2000 samples correct (0.974)\n","Iteration 47019527 Prediction: 1526/1533 samples correct (0.9954337899543378)\n","Accuracy is:  86.0485569762384  ; correct and iter are  1082089 1257533\n","Time Taken for testing Patient:  687.1203880310059\n","To display one slice of the predicted result and difference image :\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh4KZZRwKZpZxKJhZxqFgZpn/BywqIBcLCdWIAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["All the patients are completed !\n"],"name":"stdout"}]}]}